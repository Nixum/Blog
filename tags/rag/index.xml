<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>RAG on Nixum Blog</title>
    <link>http://nixum.cc/tags/rag/</link>
    <description>Recent content in RAG on Nixum Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 22 Jun 2024 00:00:00 +0000</lastBuildDate><atom:link href="http://nixum.cc/tags/rag/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>向量数据库与RAG</title>
      <link>http://nixum.cc/p/%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%8Erag/</link>
      <pubDate>Sat, 22 Jun 2024 00:00:00 +0000</pubDate>
      
      <guid>http://nixum.cc/p/%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%8Erag/</guid>
      <description>[TOC]
一、从场景出发 虽然目前ChatGPT等大语言模型已经十分好用了，无论是响应速度和回答的质量，基本上能解决我们日常一些问题和简单的工作，但不可否认，目前的大语言模型仍然有很多缺陷，比如：
 回答幻觉：大语言模型回答问题的本质上是基于其已有的训练数据，预测(推理)出哪些可能的文字作为答案，所以难免会出现张冠李戴、胡说八道的情况，最典型的比如你在 ChatGPT-3.5问他”西红柿炒钢丝球要怎么做“，它会十分正经的回答，又或者问一些代码问题，它有时也会回答出一些不存在的语法或者方法的调用，产生不正确的答案； 上下文限制：比如Chat GPT-3.5 Turbo的上下文限制是4K tokens（大概3000字），GPT-4 Turbo的上下文限制是128K tokens（大概9.6万字），这意味着其最多只能处理（记忆）这么多字的内容，且随着处理的上下文越多，响应速度也会越来越慢，成本越来越高； 训练的语料更新不够及时：比如Chat GPT-3.5 Turbo训练的语料库只记录了2021年9月之前的数据，GPT-4 Turbo则是2023年4月，这意味着在此之后产生的数据模型是不知道的； 在某些领域还不够专业：比如某些垂直领域的训练语料往往比较封闭，不对外公开，GPT训练的语料不够丰富准确，进而回答的质量就会大打折扣；  为了优化上述问题，提升大语言模型回答的质量，其中一种解决方案就是在提问时向大语言模型提供更加准确且核心的资料供其参考，这个时候向量数据库就派上用场了。
二、向量数据库的作用 向量数据库并不是什么特别新的技术，早在机器学习场景中就有广泛应用，比如人脸识别、以图搜图、音乐软件的听音识曲等都有应用到，只是最近被大模型带火了一把。
 向量数据库用专门的数据结构和算法来处理向量之间的相似性计算和查询。 通过构建索引结构，向量数据库可以快速找到最相似的向量，以满足各种应用场景中的查询需求。
 上面是AWS上找到的对向量数据库的描述，以此代入到提升大模型回答质量的场景下，向量数据库的核心作用，就是将用户准备好的强相关性的文本转成向量，存储到数据库中，当用户输入问题时，也将问题转成向量，然后在数据库中进行相似性搜索，找到相关联的向量和上下文，进而找到对应的文本，最后跟着问题一起发送给大语言模型，从而达到减少模型的计算量，提升理解能力和响应速度，降低成本，绕过tokens限制，提高回答质量的目的，这种方式也被称为RAG（Retrieval Augmented Generation）检索增强生成。
 
三、与传统数据库功能上的差异 或许你可能会疑惑，如果用传统数据库或者es等搜索出关联的信息，再跟着问题一起发送给大语言模型，也能实现类似的效果，这样行不行？答案当然是可以，但它不是最优的，出来的效果也并不好，原因在于传统数据库的搜索功能都是基于关键字搜索，只能匹配出对应的文本，语义上的联系其实非常弱。
传统数据库都是基于B+树或者分词+倒排索引的方式进行关键字匹配和排序，得到最终结果，例如，通过传统数据库搜索”布偶猫“，只能匹配得到带有”布偶猫“这个关键字相关的结果，无法得到”银渐层“、”蓝猫“等结果，因为他们是不同的词，传统数据库无法识别他们的语义关系。
 
而向量数据库是基于向量搜索的，需要我们事先将”蓝猫“，”银渐层“，”布偶“，根据他们的特征比如大小、毛发长短、颜色、习性、脸型等维度，计算出一组数字后作为他们的代表进行存储（这组数字也被称为向量），只要分解的维度足够多，就能将所有猫区分出来，然后通过计算向量间的距离来判断他们的相似度，产生语义上的联系；
 
四、向量数据库的核心要点  将事物根据特征转换为不同维度的向量的过程，就叫做vector embedding向量嵌入： 通过计算多个向量之间的距离来判断它们的相似度，就叫做similarity search相似性搜索；  这两个步骤，都决定着搜索质量的好坏。
4.1 向量嵌入 以大语言模型对话的场景来说，涉及的语料就是大量的文本了，而文本包含的特征可以是词汇、语法、语义、情感、情绪、主题、上下文等等，这些特征太多了，可能需要几百上千个特征才能区分出一段文本表达的含义，很难进行人为的标注，因此需要有一种自动化的方式来提取这些特征，这就可以通过 vector embedding来实现。
这一步其实并不属于向量数据库的功能，更像是数据入库前的前置操作，向量数据库本事只提供存储向量和搜索的功能。
现在常用的大语言模型，基本都提供了embedding接口，供用户把文本转换为向量，比如 OpenAI的text-embedding-ada-002模型可以把文本分解成1536维的向量，网易的bce-embedding-base_v1模型，可以把文本分解为768维的向量等等，具体排名可以参考HuggingFace的大文本嵌入排行，多少维其实就是一个长度多少的浮点类型数组，数组内的每个元素，则代表被分解的文本的特征，共同组成一个信息密集的表示。
那对于给定的文本，要如何分割，以及分解出多少个向量合适呢？
如果文本分割的粒度把控不好，可能会导致分割出来的无用信息太多，或者语义丢失，语义关联性不大等问题。对于文本分割，这里找到了一篇写得很好的文章：文本分割的五个层次：
 第 1 层：字符分割，比如按一定的字符数、块大小分割文本，不考虑其内容和形式。 第 2 层：基于分隔符分割，比如按句号、换行符、空格等进行文本切割。 第 3 层：文档类型分割，比如PDF、Markdown都有特定的语法表示语义分割，使得分割出来的文本关联性更强。 第 4 层：语义分割，比如每三句话转成向量，然后去掉第一句，加上下一句，再转成向量，比较两个向量的距离，如果距离超过一定的阈值，说明找到分割点。 第 5 层：使用大语言模型分割，使用适合的prompt指导大语言模型推理分割。  简单来说，我们更倾向于把上下文关联性强的文本合一起分割，得到的整体效果最好，下面的demo，就是按第4层的分割方式，可以参考一下。</description>
    </item>
    
  </channel>
</rss>
