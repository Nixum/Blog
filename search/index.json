[{"content":"[TOC]\n内核 PC机启动流程 以Ubuntu Linux + GRUB引导程序为例：\nPC机加电后，加载BIOS固件， 触发PC机BIOS固件发送指令 检测和初始化CPU、内存及主板平台，加载引导设备(比如硬盘)中的第一个扇区数据到0x7c00地址开始的内存空间，再接着跳转到0x7c00处执行指令，加载GRUB引导程序，加载硬盘分区中的OS文件，启动操作系统。\n内核结构分类 内核：计算机资源的管理者。计算资源分为硬件资源和软件资源\n  硬件资源：CPU、内存、硬盘、网卡、显卡、IO设备、总线，他们之间通过总线进行联系；\n  软件资源：对上述各种硬件资源的管理程序、设备的驱动程序\n  宏内核结构 - 类似单体服务 将上述所有软件资源进行整合，链接在一起，形成一个大的可执行程序，控制所有硬件资源。这个大程序会在处理器的特权模式下运行，对外提供系统调用函数，供其他程序、进程调用。\n当应用层有程序要调用进行内存分配时，就会调用宏内核进行内存分配\n 应用程序调用内核内存分配API函数 处理器切换到特权模式，开始运行内核代码 内核的内存管理代码按照特定的算法，分配内存 内存分配函数把分配的内存块的首地址返回给应用程序 应用程序接收到返回后，处理器开始运行用户模式下的应用程序，应用程序使用得到的内存首地址，开始使用这块内存  优点：由于所有硬件管理程序都整合在一起，相互调用时性能极高\n缺点：所有硬件管理程序都整合在一起，没有模块化，扩展性差，移植性差，牵一发而动全身，每次修改都需要重新安装，其中一个模块有问题就会影响其他模块。\nLinux就属于宏内核\n Linux 的基本思想是一切都是文件：每个文件都有确定的用途，包括用户数据、命令、配置参数、硬件设备等对于操作系统内核而言，都被视为各种类型的文件。Linux 支持多用户，各个用户对于自己的文件有自己特殊的权利，保证了各用户之间互不影响。多任务则是现代操作系统最重要的一个特点，Linux 可以使多个程序同时并独立地运行。\n 第一次访问 \nLinux使用宏内核架构，主要分为上述五大模块，模块间的通信通过函数调用，函数间的调用没有层次关系，所以函数的调用路径纵横交错，如果有函数出问题，那就会影响到调用它的模块，存在安全隐患，但优点是存内核调用，性能极高\nLinux高清全结构图：https://makelinux.github.io/kernel/map/\n 微内核结构 - 类似微服务 内核仅处理进程调度、中断、内存空间映射、进程间通信等，控制硬件资源的软件资源，转成一个个服务进程，比如进程管理、内存管理、设备管理、文件管理等，和用户进程一样，只是它们提供了宏内核的那些功能。\n微内核内，进程间通过消息进行通信，应用程序每次要申请资源都需要发送消息到微内核，微内核再把这条消息转发给相关服务进程，直到完成这次调用。\n当应用层有程序要调用进行内存分配时\n 应用程序发送内存分配的消息（发送消息这个函数由微内核提供） 处理器切换到特权模式，执行内核代码 微内核让当前进程停止运行，并根据消息包中的数据，推送给对应的消息接收者，比如这里是内存管理服务进程。 内存管理服务进程收到消息，分配一块内存 内存管理服务进程，处理完成后，通过消息的形式返回分配内存块的地址给内核，然后继续等待下一条消息。 微内核把包含内存地址的消息返回发送给内存分配消息的应用程序 处理器开始运行用户模式下的应用程序，应用程序收到这条消息，得到内存首地址，开始使用这块内存。  优点：系统结构清晰，移植性、伸缩性、扩展性强，微内核代码少，容易替换，各种系统服务进程可随时替换\n缺点：进程间消息依赖微内核进行消息传递，会频繁进行服务进程切换，模式切换，导致性能较差。\n分离硬件的相关性 分离硬件的相关性其实就是屏蔽底层硬件操作细节，形成一个独立的软件抽象层，对外提供接口，方便应用层接入。\n是内核设计的一种指导思想，所以在设计操作系统内核的时候，就可以分为\n 内核接口层：向应用层提供系统接口函数； 内核功能层：系统函数的主要实现，实现进程管理、内存管理、中断管理、设备管理、驱动、文件系统等； 内核硬件层：对硬件资源的操作，比如初始化CPU、内存、中断控制、其他IO设备  混合内核 混合内核在微内核的基础上进行改进，层次分明，动态加载模块到内核，兼具宏内核和微内核的优点。\nWindows就属于混合内核。\nCPU 中断：中断即中止执行当前程序，转而跳转到另一个特定的地址上，去运行特定的代码，响应外部事件。\n硬件中断：中断控制器给CPU发送了一个电子信号，CPU对这个信号作出应答，随后中断控制器将中断号发给CPU。\n软件中断：CPU执行INT指令，指令后带一个常数，该常数为软中断号。\n工作模式 实模式 又称实地址模式，运行真正的指令，对指令的动作不作区分，直接执行指令的真实功能，另外，发往内存的地址是真实的，对任何地址不加限制地发往内存。\n访问内存 - 分段内存管理模式：内存地址由段寄存器左移4位，再加上一个通用寄存器中的值或者常数形成地址，然后由这个地址去访问内存。仅支持16位地址空间，对指令不加限制地运行，对内存没有保护隔离作用。\n中断实现，内存中存放一个中断向量表，该表的地址和长度由CPU的特定寄存器IDTR指向，一个条目由代码段地址和段内偏移组成，当出现中断号后，CPU就根据IDTR寄存器中的信息，计算出中断向量中的条目，进而装载CS（代码段基地址）、IP（代码段内偏移）寄存器，响应中断。\n保护模式 保护模式是为了应对更高的计算量和内存量，CPU寄存器扩展为32位宽，使其能够寻址32位内存地址空间和32位的数据，还实现指令区分和访问内存地址限制。\n为了区分哪些指令(如in、out、cli)和哪些资源(如寄存器、IO端口、内存地址)可以被访问，实现了特权级。特权级分为4级，R0~R3，R0最大，可以指向所有指令，之后依次递减，下一层是上一层的子集。内存访问则靠段描述符和特权级相互配合实现。\n访问内存时使用平坦模型：为了应对分段模型的缺陷，在分段模型的基础上套多一层，简化分段模型，对内存段与段之间的访问严格检查，没有权限不会放行。\n中断时由于要进行权限检测和特权级切换，需要扩展中断向量表的信息，每个中断用一个中断门描述符来表示，存于中断向量表中\n长模式 又名AMD64模式，在保护模式的基础上，增加一些通用寄存器，扩展通用寄存器的位宽，所有的通用寄存器都是64位，还可单独使用低32位。低32位可以拆分成一个低16位寄存器，低16位又可以拆分为两个8位寄存器。\n访问内存时需要开启分页模式，弱化段模式管理，仅保留权限级别的检查，忽略段基址和段长度，地址检查交给MMU。\n中断时的中断向量表中的条目，中断描述符需要扩展，将其32位段内偏移扩展成支持64位\n 1、x86 CPU的位数越来越高，从16到32到64，每次进步都尽量的去兼容了之前的CPU架构，所以： A、16位时寻址能力不足，所以要借助额外的寄存器进行1M空间的寻址；32位时，每个程序都有自己独立的4G寻址空间，操作系统用低位的1G-2G，其余留给用户程序；64位后，暂时就遇不到寻址能力不足的事情了； B、前一代的寄存器尽量保留，不够用就扩展新的 C、寄存器的长度升级后，其低位可以兼容上一代的寄存器\n2、CPU同时在安全性上也要提升，从只有实模式【可以随意执行全部CPU指令，内存可以直接通过物理地址访问，随意访问随意读写】，到了32的保护模式【将指令划分为ring0到ring3，CPU指令不是你想调用就能调用；内存不是你想访问就能访问，首先CPU要允许，而且操作系统允许】，而64的长模式在安全方面与32并没有本至区别；\n3、从实模式到保护模式，访问内存时，需要访问的地址变大了，需要控制的内容变多了，于是引入了段描述符，所有的段描述符组成了描述符表，包括唯一的全局描述符GDT和多个局部描述符号LDT。GDT是操作系统特供，要重点关注。CPU寻址的时候，要通过段寄存器+GDTR寄存器定位到的内存中的描述符，判断是否允许访问。然后，再根据段描述符中地址进行访问。\n4、同时内存中内存管理有段、页、段页三种常用模式。一般在应用层，程序员感受不太到，操作系统全给咱们做完了。\n5、中断，其实是通过硬件或软件方式告诉CPU，来执行一段特殊的代码。比如咱们键盘输入，就是通过硬件中断的方式，告知操作系统的。在实模式下，中断是直接执行的。但到了保护模式和长模式下，就要特权级别校验通过才能执行，所以引入了中断门进行控制。在ring3调用中断一般是要通过操作系统切换到内核态ring0进行的，与内存类似，要通过中断向量表，确认中断门中权限是否允许，然后定位到指定代码执行。\n6、BIOS引导后，系统直接进入最简单、特权最大的实模式；而后告知CPU，切换到保护模式，并运行在ring0。后续的用户进程，一般就在ring3，想执行特权指令要通过操作系统来执行\n 内存 虚拟地址 由于各个应用程序由不同的公司开发，且每台计算机的内存容量都不一样，如果应用程序在运行时使用的是真实地址，将会导致各个应用程序在使用内存地址时产生冲突。为了解耦内存真实地址和应用使用的内存地址，对不同的应用程序使用的内存进行隔离和保护，引出虚拟地址概念。\n虚拟地址：让每个应用程序都各自享有一个从0开始到最大地址的空间，在每个应用程序独立且私有，其他应用程序无法观察到，再由操作系统将虚拟地址映射成真实的物理地址，实现内存的使用。\n虚拟地址由链接器产生，应用程序经过编译后通过链接器的处理变成可执行文件，链接器会处理程序代码间的地址引用，形成程序运行的静态内存空间视图。\n实模式下，多个进程共享所有地址空间太过危险，因此有了保护模式，保护模式下为了让各个应用程序能正确使用内存，使用了虚拟地址映射，让各个应用程序都有自己的地址空间，因此访问内存时不用考虑其他问题，访问时再经过MMU翻译得到对应的页表，不同的应用程序由于不同的页表，产生了进程地址空间隔离，但多个应用程序也可以共享某个页表，此时就涉及进程间的通信了。\n关于内存对齐：因为cpu总是以多个字节的块为单位访问内存的，不与它的访问块地址边界对齐cpu就要多次访问\nMMU 虚拟地址转换为物理地址通过MMU(内存管理单元)实现，只能在保护模式或者长模式下开启。\nMMU使用分页模型，即把虚拟地址空间和物理地址空间都分成同等大小的页，按照虚拟页和物理页进行转换，一个虚拟地址对应一个物理地址，一个虚拟页对应一个物理页，页大小一经配置就是固定，通过内存中存储的一个地址关系转换表（页表）实现转换。\n 为了增加灵活性和节约物理内存空间（因为页表是放在物理内存中的），所以页表中并不存放虚拟地址和物理地址的对应关系，只存放物理页面的地址，MMU 以虚拟地址为索引去查表返回物理页面地址，而且页表是分级的，总体分为三个部分：一个顶级页目录，多个中级页目录，最后才是页表。\n 分层的结构是根据页大小决定。\nCache 程序局部性原理：程序一旦编译完装载进内存中，它的地址就确定了，CPU大多数时间在执行相同的指令或者相邻的指令。\n内存，从逻辑上看，是一个巨大的字节数组，内存地址就是这个数组的下标。相比与CPU，内存的数据量吞吐是很慢的，当CPU需要内存数据时，内存一时间给不了，CPU就只能等，让总线插入等待时钟周期，直到内存准备好。因此内存才是决定系统整体性能的关键。为了不让CPU每次都要等，因此利用程序局部性原理，引入了Cache。\nCache主要由高速的静态存储器、地址转换模块和Cache行替换模块组成。\nCache会把自己的高速静态存储器和内存分成大小相同的行，一行大小通常为32字节或64字节。Cache和内存交换数据的最小单位是一行，多个行又会形成一组。除了存储数据，还有标志位，如脏位、回写位、访问位等。\n 1.CPU 发出的地址由 Cache 的地址转换模块分成 3 段：组号，行号，行内偏移。\n2.Cache 会根据组号、行号查找高速静态储存器中对应的行。如果找到即命中，用行内偏移读取并返回数据给 CPU，否则就分配一个新行并访问内存，把内存中对应的数据加载到 Cache 行并返回给 CPU。写入操作则比较直接，分为回写和直通写，回写是写入对应的 Cache 行就结束了，直通写则是在写入 Cache 行的同时写入内存。\n3.如果没有新行了，就要进入行替换逻辑，即找出一个 Cache 行写回内存，腾出空间，替换行有相关的算法，替换算法是为了让替换的代价最小化。例如，找出一个没有修改的 Cache 行，这样就不用把它其中的数据回写到内存中了，还有找出存在时间最久远的那个 Cache 行，因为它大概率不会再访问了。\n以上这些逻辑都由 Cache 硬件独立实现，软件不用做任何工作，对软件是透明的。\n Cache虽提升了性能，但会产生数据一致性问题。Cache可以是一个独立硬件，也可以集成在CPU中，Cache通常有多级，每一级间都有可能产生数据一致性问题。解决一致性问题就需要数据同步协议，如MESI和MOESI。\n MESI：M（Modified修改）、E（Exclusive独占）、S（Shared共享）、I（Invalida无效）\n最开始只有一个核读取了A数据，此时状态为E独占，数据是干净的， 后来另一个核又读取了A数据，此时状态为S共享，数据还是干净的， 接着其中一个核修改了数据A，此时会向其他核广播数据已被修改，让其他核的数据状态变为I失效，而本核的数据还没回写内存，状态则变为M已修改，等待后续刷新缓存后，数据变回E独占，其他核由于数据已失效，读数据A时需要重新从内存读到高速缓存，此时数据又共享了\n 同步原语 常见的有：原子操作、控制中断、自旋锁、信号量。底层原子操作都是依靠支持原子操作的机器指令的硬件实现。\n  原子操作是对单一变量。\n  控制中断：可以作用复杂变量，一般用在单核CPU，同一时刻只有一个代码执行流，响应中断会导致代码执行流切换，此时如果两个代码执行流操作到同一个全局变量就会导致数据不一致，所以需要在操作全局变量时关闭中断，处理完开启进行控制。\n  自旋锁还有一个作用就是可以在多核CPU下处理全局变量，因为控制中断只能控制本地CPU的中断，无法控制其他CPU核心的中断，此时其他CPU是可以操作到该全局变量的。自旋需要保证读取锁变量和判断并加锁操作是原子的，且会导致CPU空转一段时间。\n  信号量的一个作用就是解决自旋锁空转造成CPU资源浪费的缺点。\n  Linux初始化 引导 boot 也是机器加电，BIOS自检，BIOS加载引导扇区，加载GRUB引导程序，最后由GRUB加载Linux内核镜像vmlinuz。\nCPU被设计成只能运行内存中的程序，无法直接运行存储在硬盘或U盘中的操作系统程序，因为硬盘U盘等外部存储并不和CPU直接相连，访问机制和寻址方式与内存不一样。如果要运行硬盘或U盘中的程序，就必须先加载到内存。\n由于内存RAM断电会丢失数据，BIOS不是存储在普通的内存中的，而是存储在主板上特定的一块ROM内存中，在硬件设计上规定加电瞬间，强制将CS寄存器的值设置位0xF000，IP寄存器的值设置为0xFFF0，该地址就是BIOS的启动地址了。\nBIOS在初始化CPU和内存后进行检查，完了之后将自己的一部分复制到内存，最后跳转到内存中运行。之后枚举本地设备进行检查和初始化，调用设备的固件程序。当设备完成检查和初始化后，BIOS就会在内存中划定一个地址范围，存放和建立中断表和中断服务程序。\n一般我们会把Linux镜像放在硬盘的第一个扇区中（MBR主启动记录，包含基本的GRUB启动程序和分区表），然后让硬盘作为BIOS启动后搜索到的第一个设备，当MBR被BIOS装载到0x7c00地址开始的内存空间中，BIOS会把控制权交给MBR，即GRUB。\nGRUB所在的第一个扇区，只有512个字节，不足以装下GRUB这种大型的通用引导器，因此GRUB的加载被分成了多个步骤和多个文件，在启动时动态加载，先加载diskboot.img，然后读取剩下的core.img，识别出文件系统，使其能够访问/boot/grub目录，最后加载vmlinuz内核文件。\n启动 startup  1.GRUB 加载 vmlinuz 文件之后，会把控制权交给 vmlinuz 文件的 setup.bin 的部分中 _start，它会设置好栈，清空 bss，设置好 setup_header 结构，调用 16 位 main ，调用BIOS中断，进行初始化设置，包括console、堆、CPU模式、内存、键盘、APM、显卡模式等，然后切换到保护模式，最后跳转到 1MB 处的 vmlinux.bin 文件中。\n2.从 vmlinux.bin 文件中 startup32、startup64 函数开始建立新的全局段描述符表和 MMU 页表，切换到长模式下解压 vmlinux.bin.gz。释放出 vmlinux 文件之后，由解析 elf 格式的函数进行解析，释放 vmlinux 中的代码段和数据段到指定的内存。然后调用其中的 startup_64 函数，在这个函数的最后调用 Linux 内核的第一个 C 函数。 3.Linux 内核第一个 C 函数重新设置 MMU 页表，随后便调用 start_kernel 函数， start_kernel 函数中调用了大多数 Linux 内核功能性初始化函数，在最后调用 rest_init 函数建立了两个内核线程，在其中的 kernel_init 线程建立了第一个用户态进程\n  Linux初始化要点 \n内存管理 内存划分与组织 分段  段的长度大小不一，不便于用一个统一的数据结构表示 段的长度大小不一，容易产生内存碎片 内存不足时，内存数据会写回硬盘来释放内存，由于段的长度大小不一，每段的读写时间就会不一样，导致性能抖动  分页  页的大小固定，可以用位图表示页的分配和释放 页的大小固定，分配的最小单位是页，页也会产生内存碎片，但是可以通过修改页表的方式，让连续的虚拟页面映射到非连续的物理页面，比如有1，2，3是空闲，4已被分配，5是空闲，可以通过映射让5接上3 内存不足时，内存数据会写回硬盘来释放内存，由于每页大小一致，每页的读写时间也会一致  所以现代操作系统基本都用分页模式管理内存，比如x86 CPU长模式下MMU 4KB的分页。\n操作系统在对内存管理时，使用一个数据结构描述一个内存页，每个数据结构包含相邻页的指针，多个内存页形成一个链表。\n分页再分区 此外，可以将多个页划分为多个区，形成内存区，内存区只是一个逻辑概念，将内存划分方便管理，比如地址范围为多少多少的内存区域就给硬件用，多少多少的给内核区，多少多少给应用区等。同理，使用一个数据结构描述一个内存区，表示内存区的开始地址和结束地址，物理页的数量、已分配的物理页的数量、剩余数量等。\n组织内存页 为了能高效的分配内存，需要把内存区数据结构和内存页面数据结构关联起来，组织成一个内存分割合并的数据结构。妈的，真难。\n 以2的N次方为页面数组织页面的原因：\n1、内存对齐，提升CPU寻址速度 2、内存分配时，根据需求大小快速定位至少从哪一部分开始 3、内存分配时，并发加锁，分组可以提升效率 4、内存分配回收时，很多计算也更简单\n Linux内存管理 伙伴系统 Linux使用分页机制管理物理内存系统，把物理内存分成4KB大小的页面进行管理，通过一个Page结构体表示一个物理内存页面。\n因为硬件的限制，Linux把属性相同的物理内存页面，归结到一个区，不同的硬件，划分不同的区。在32位的x86平台中，将0~16MB划分位DMA区，高内存区适用于要访问的物理地址空间大于虚拟地址空间，Linux内核不能建立直接映射的情况。物理内存中剩余的页面就划分位常规内存区，64位的x86平台则没有高内存区。\n 在很多服务器和大型计算机上，如果物理内存是分布式的，由多个计算节点组成，那么每个 CPU 核都会有自己的本地内存，CPU 在访问它的本地内存的时候就比较快，访问其他 CPU 核内存的时候就比较慢，这种体系结构被称为 Non-Uniform Memory Access（NUMA）\n 在内存区的上层，通过定义节点pglist_data，来包含多个区\n连续且相同大小的Page组成伙伴，Linux在分配物理内存页面时，首先找到内存节点，接着找到内存区，然后是合适的空闲链表，最后在其中找到页的Page结构，完成物理内存页面的分配。\n 内存压缩不是指压缩内存中的数据，而是指移动内存页面，进行内存碎片整理，腾出更大的连续的内存空间。如果内存碎片整理了，还是不能成功分配内存，就要杀死进程以便释放更多内存页面\n 分配算法：关键字：快速分配路径、慢速分配路径，只有在快速分配路径失败之后，才会进入慢速分配路径，慢速分配路径中会进行内存回收相关的工作。最后，通过 expand 函数分割伙伴，完成页面分配。\nSLAB Linux使用SLAB来分配比页更小的内存对象。\nSLAB分配器会把一个内存页面或者一组连续的内存页面，划分成大小相同的块，每一小块内存就是SLAB对象，在这一组连续的内存页面中除了SLAB对象，还有SLAB管理头和着色区。\n着色区是一块动态的内存块，建立SLAB时才会设置它的大小，目的是为了错开不同的SLAB对象地址，降低硬件Cache行中地址的争用，避免抖动产生的性能下降。\nLinux使用kmen_cache结构管理page对应内存页面上的小块内存对象，然后让该 page 指向 kmem_cache，由 kmem_cache_node 结构管理多个 page。\n进程 进程是一个应用程序运行时刻的实例，是应用程序运行时所需资源的容器，是一堆数据结构。操作系统记录这个应用程序使用了多少内存，打开什么文件，控制资源不可用时进行睡眠，进程运行到哪了等等这些统计信息，放到内存中，抽象成进程。\n进程为了让操作系统管理，需要有一个地址空间，该地址空间至少包含两部分内容：内核和用户的应用程序。\n 当 CPU 在 R0 特权级运行时，就运行在内核的地址空间中，当 CPU 在 R3 特权级时，就运行在应用程序地址空间中。各进程的虚拟地址空间是相同的，它们之间物理地址不同，是由 MMU 页表进行隔离的，所以每个进程的应用程序的代码绝对不能随意访问内核的代码和数据。\n每个进程都有一个内核栈，指向同一个块内核内存区域，共享一份内核代码和内核数据。内核进程使用一份页表，用户进程两份页表，用户进程多了一份用户空间页表，与其它用户进程互不干扰。\n 由于应用程序需要内核提供资源，而内核需要控制应用程序的运行，让其能随时中断或恢复执行，因此需要保存应用程序的机器上下文和它运行时刻的栈。使用内核的功能时，会先停止应用程序代码的运行，进入内核地址空间运行内核代码，然后返回结果。\n 进程的机器上下文分为几个部分，一部分是 CPU 寄存器，一部分是内核函数调用路径。CPU 的通用寄存器，是中断发生进入内核时，压入内核栈中的，从中断入口处开始调用的函数，都是属于内核的函数。函数的调用路径就在内核栈中，整个过程是这样的：进程调度器函数会调用进程切换函数，完成切换进程这个操作，而在进程切换函数中会保存栈寄存器的值。\n 建立进程，其实就是创建进程结构体，分配进程的内核栈于应用程序栈，并对进程的内核栈进行初始化，最后将进程加入调度系统，以便投入运行。\n调度 CPU同一时刻只能运行一个进程，当一个进程不能获取某种资源导致它不能继续运行时，就应该让出CPU，因此面对多进程就需要一个合理的调度。\n进程的等待和唤醒可以通过信号量实现。\n每一个CPU都有一个空转进程，作为进程调度器最后的选择。空转进程会调用进程调度函数，在系统没有其他进程可以运行时又会调用空转进程，形成闭环。\nLinux进程调度 Linux进程调度支持多种调度算法：基于优先级的调度算法、实时调度算法、完全公平调度算法CFQ；也支持多种不同的进程调度器：RT调度器、Deadline调度器、CFS调度器、Idle调度器。\n进程优先级越高，占有CPU时间越长；调度优先级越高，调度系统选中它的概率就越大。\nLinux的CFS调度器没有时间片概念，它是直接分配CPU使用时间的比例。CFS会按权重比例分配CPU时间，权重表示进程优先级，进程时间 = CPU总时间 * 进程权重 / 就绪队列所有进程权重之和。对外接口提供一个nice值，范围为-20~19，数值越小，权重越大，每降低一个nice值，就能多获得10%的CPU时间。\n设备I/O 操作系统本身会对设备进行分类，定义一套对应设备的操作接口，由设备的驱动程序实现，而驱动程序实现对应设备的操作，从而达到操作系统与设备解耦的目的。\n设备和设备驱动的信息会抽象成对应的数据结构，再通过设备表结构组织驱动和设备的数据结构。\n总线是设备的基础，表示CPU与设备的连接，总线本身也是一个数据结构。\n一个驱动程序开始是由内核加载运行，然后调用由内核提供的接口建立设备，最后向内核注册设备和驱动，完成驱动和内核的握手动作。\n内核要求设备执行某项动作，就会调用设备的驱动程序函数，把相关参数传递给设备的驱动程序，由于参数很多，各种操作所需的参数又不相同，就会把这些需要的参数封装在一个数据结构中，称为I/O包。\n在Linux中，各种设备时一个个文件，只是并不对应磁盘上的数据文件，而是对应着存在内存当中的设备文件，对这些文件的操作等同于对设备的操作。在目录 /sys/bus目录下能看到所有总线下的所有设备，总线目录下有设备目录，设备目录下是设备文件。\n文件系统 文件系统是操作系统为了兼容不同的物理存储设备而存在。文件系统会把文件数据定义成一个动态的线性字节数组，将一段范围的字节数组整合成一个文件数据逻辑块，再映射成对应存储设备的物理存储块，从而解决不同物理存储设备有不同的物理存储块的问题。即 线性字节数组 -\u0026gt; 文件数据逻辑存储块 -\u0026gt; 物理存储块\n可以通过位图来标识哪些逻辑存储块是空闲，哪些是已被分配占用。\n通过一个包含文件系统标识、版本、状态、存储介质大小、文件系统逻辑存储块大小、位图所在存储块、根目录等信息的数据结构，作为文件系统的超级块或文件系统描述块。\n文件系统的格式化，指的是在存储设备上重建文件系统的数据结构，一般是先设置文件系统的超级快、位图、根目录，三者是强顺序的。\nLinux使用VFS虚拟文件系统作为中间层，抽象了文件系统共有的数据结构和操作函数集合，一个物理存储设备的驱动只要实现了这些函数就可以插入VFS中，从而可以同时支持各种不同的文件系统。\nVFS包含四大数据结构：超级块结构super_block、目录结构dentry、文件索引节点结构inode、进程打开文件实例结构file。Liunx 挂载文件系统时，会读取文件系统超级块，从超级块出发读取并构造全部目录结构，目录结构指向存储设备文件时，是一个个文件索引节点结构。\n网络 在Linux中，替代传输层以上协议实体的标准接口，称为套接字，负责实现传输层以上的所有功能，即套接字是一套通用的网络API，下层由多个协议栈实现\nUnix网络中的5种 I/O 模型\n参考：https://zhuanlan.zhihu.com/p/121826927、https://segmentfault.com/a/1190000022653305\n参考 极客时间 - 操作系统实战45讲\n","date":"2020-06-09T00:00:00Z","permalink":"http://nixum.cc/p/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/","title":"操作系统"},{"content":"[TOC]\n特点  分布式数据库，Json数据模型，面向对象数据模型，不强制表的scheme 当应用场景不清晰时，可以直接以对象模型直接存储，无需关心字段，表结构灵活，动态增加新字段 不用太过关注表间的关系，可直接嵌套存储，将多种关系存储在同一张表上，同时也加快查表，因为它可以减少磁盘定位次数，如果是关系型数据库，同时查多张表就需要定位多次 原生支持高可用，一般的部署方式是部署三个节点replica set，最多50个；多replica set可以实现自恢复（当主节点挂点后会选出从节点），异地容灾，数据库滚动更新 原生支持横向扩展，通过水平扩展分片实现，外部并不感知有多少个分片，只会当成一个分片使用 支持字段级加密，针对隐私数据，比如身份证、电话等，在入库时可以进行加密，查询时解密 支持地理位置经纬度查询 强大的聚合查询，适合报表、时序数据  NoSQL语句 客户端使用驱动时连接的执行流程\n 客户端执行流程 \n数据库端执行流程\n 数据库端执行流程 \n要获取ticket是因为MongoDB默认存储引擎wiredtiger的机制，ticket代表着系统资源的数量，ticket数量有限，读写操作都需要先获得ticket才可以进行下一步操作，机制类似信号量。\n连接 连接mongoDB语句，当有多节点或多分片时，连接也要写上，mongodb://节点1的host:port, 节点2的host:port,.../databaseName?[options: maxPoolSize(java默认是100), maxWaitTime(查询的最大等待事件), writeConcern, readConcern]\nmongoDB驱动里已提供负载均衡，多节点探测\n聚合 作用相当与group by，可作用与多个collection，可进行查询和计算。Mongo的聚合操作发生在pipeline中，由多个stage组成，有点像责任链，通过多个state来过滤，聚合数据，每一个{}代表一个state\ndemo\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  MySQL中的SELECTdepartment,count(null)asemp_QTYFROMUserWHEREgender=\u0026#39;female\u0026#39;GROUPBYdepartmentHAVINGcount(*)\u0026lt;10等价于mongo中的db.user.aggregate([{$match:{gender:\u0026#39;female\u0026#39;}},{$group:{_id:\u0026#39;$DEPARTMENT\u0026#39;,emp_qty:{$sum:1}}},{$match:{emp_qty:{$lt:10}}}])  几个比较特别的运算符\n$unwind：将查询到的数组展开\n$grouphLookup：图搜索\n$facet/$bucket: 分面搜索，根据不同范围条件，多个维度一次性进行分组输出\n优化  查询时，尽量使用索引，为经常做查询的条件添加索引 查询时，只查询需要的字段，而不是查询全部，减少网络资源的浪费 更新时，只更新必要的字段，而不是每次更新都把整个json文档发送过去，减少网络资源的浪费 插入时，尽可能直接批量插入，而不是一条一条插 通过mongodb提供的TTL索引，可以实现过期自删数据 建表时，文档嵌套不超过3层 尽量少用count()来计算总页数，而是使用limit 尽量少用skip/limit形式分页，而是通过id来定位起始的位置，这点跟aws dynamoDB很像，不过至少有提供这种功能 尽量少用事务，跨分片事务，避免过大事务，控制更新的文档(行)数量 使用aggregate时，前一个stage计算得到的数据会传递到下个stage，如果前一个stage没有该数据，则下一个stage无法获取到（尽管表中有该字段） 使用aggregate时，pipeline最开始时的match sort可以使用到索引，一旦发生过project投射，group分组，lookup表关联，unwind打散等操作后，则无法使用索引。  分析  在查询语句中使用explain()方法分析查询语句，有三种分析模式，通过传参的方式使用，比如：db.getCollection(\u0026quot;order\u0026quot;).explain('executionStats').find({条件})  queryPlanner：默认，只会输出被查询优化器选择出来的查询计划winningPlane executionStats：除了输出被查询优化器选择出来的查询计划winningPlane，并执行语句（如果是写操作，不会真正操作数据库），给出分析结果，比如扫描的行数，使用什么索引，耗时，返回的条数等 allPlansExecution：列出所有可能的查询计划并执行，给出所有方案的结果，mongo支持这种分析模式，但aws的documentDB不支持    # 常见的stage枚举： COLLSCAN：全表扫描 IXSCAN：索引扫描 FETCH：根据前面扫描到的位置抓取完整文档，相当于回表 IDHACK：针对_id进行查询 SHARD_MERGE 合并分片中结果 SHARDING_FILTER 分片中过滤掉孤立文档 SORT：进行内存排序，最终返回结果 SORT_KEY_GENERATOR：获取每一个文档排序所用的键值 LIMIT：使用limit限制返回数 SKIP：使用skip进行跳过 IDHACK：针对_id进行查询 COUNTSCAN：count不使用用Index进行count时的stage返回 COUNT_SCAN：count使用了Index进行count时的stage返回 TEXT：使用全文索引进行查询时候的stage返回 SUBPLA：未使用到索引的$or查询的stage返回 PROJECTION：限定返回字段时候stage的返回 # 一个executionStats例子 { \u0026quot;queryPlanner\u0026quot; : { \u0026quot;plannerVersion\u0026quot; : 1.0, \u0026quot;namespace\u0026quot; : \u0026quot;库名.表名\u0026quot;, \u0026quot;winningPlan\u0026quot; : { \u0026quot;stage\u0026quot; : \u0026quot;SORT_AGGREGATE\u0026quot;, \u0026quot;inputStage\u0026quot; : { \u0026quot;stage\u0026quot; : \u0026quot;IXONLYSCAN\u0026quot;, \u0026quot;indexName\u0026quot; : \u0026quot;索引名\u0026quot;, \u0026quot;direction\u0026quot; : \u0026quot;forward\u0026quot; } } }, \u0026quot;executionStats\u0026quot; : { \u0026quot;executionSuccess\u0026quot; : true, \u0026quot;planningTimeMillis\u0026quot; : \u0026quot;0.276\u0026quot;, \u0026quot;nReturned\u0026quot; : \u0026quot;1\u0026quot;, \u0026quot;executionTimeMillis\u0026quot; : \u0026quot;10517.898\u0026quot;, # 执行时间 \u0026quot;totalKeysExamined\u0026quot; : \u0026quot;10519.0\u0026quot;, # 总扫描数 \u0026quot;totalDocsExamined\u0026quot; : \u0026quot;567542\u0026quot;, # 总扫描文档数 \u0026quot;executionStages\u0026quot; : { \u0026quot;stage\u0026quot; : \u0026quot;SORT_AGGREGATE\u0026quot;, \u0026quot;nReturned\u0026quot; : \u0026quot;1\u0026quot;, \u0026quot;executionTimeMillisEstimate\u0026quot; : \u0026quot;10517.497\u0026quot;, \u0026quot;inputStage\u0026quot; : { \u0026quot;stage\u0026quot; : \u0026quot;IXONLYSCAN\u0026quot;, # \u0026quot;nReturned\u0026quot; : \u0026quot;567542\u0026quot;, # 返回的文档数 \u0026quot;executionTimeMillisEstimate\u0026quot; : \u0026quot;9786.122\u0026quot;, # 执行此阶段的耗时 \u0026quot;indexName\u0026quot; : \u0026quot;索引名\u0026quot;, \u0026quot;direction\u0026quot; : \u0026quot;forward\u0026quot; } } }, \u0026quot;serverInfo\u0026quot; : { \u0026quot;host\u0026quot; : \u0026quot;oms\u0026quot;, \u0026quot;port\u0026quot; : 27017.0, \u0026quot;version\u0026quot; : \u0026quot;4.0.0\u0026quot; }, \u0026quot;ok\u0026quot; : 1.0, \u0026quot;operationTime\u0026quot; : Timestamp(1640604817, 1) }  对表的或对库的，使用stats()方法，比如db.getCollection(\u0026quot;order\u0026quot;).stats()，或者db.stats()，可以查询整张表的信息或整个库的统计信息  # 针对库的 { \u0026quot;db\u0026quot; : \u0026quot;order\u0026quot;, # 库名 \u0026quot;collections\u0026quot; : 11.0, # 集合数 \u0026quot;objects\u0026quot; : 4938161.0, # 对象数 \u0026quot;storageSize\u0026quot; : 4236443648.0, # 占用磁盘大小 \u0026quot;indexes\u0026quot; : 32.0, # 索引数 \u0026quot;indexSize\u0026quot; : 1424130048.0, # 索引大小 \u0026quot;fileSize\u0026quot; : 5660573696.0, # 文件大小 \u0026quot;ok\u0026quot; : 1.0, \u0026quot;operationTime\u0026quot; : Timestamp(1640604514, 1) } # 针对表的 { \u0026quot;ns\u0026quot; : \u0026quot;oms.order\u0026quot;, \u0026quot;count\u0026quot; : 749031.0, # 数量 \u0026quot;size\u0026quot; : 1479336225.0, # 大小 \u0026quot;avgObjSize\u0026quot; : 1975.90745, # 每个对象的平均大小 \u0026quot;storageSize\u0026quot; : 2164432896.0, # 存储大小 \u0026quot;capped\u0026quot; : false, \u0026quot;nindexes\u0026quot; : 1.0, # 索引个数 \u0026quot;totalIndexSize\u0026quot; : 31129600.0, # 总索引大小 \u0026quot;indexSizes\u0026quot; : { \u0026quot;_id_\u0026quot; : 94568448.0, \u0026quot;id\u0026quot; : 77438976.0, \u0026quot;idx_createAt_desc\u0026quot; : 31129600.0, # 各个索引的大小 }, \u0026quot;ok\u0026quot; : 1.0, \u0026quot;operationTime\u0026quot; : Timestamp(1640327496, 1) }  慢查询   复制集Replica Set机制 一般是一主多从，所有实例存储相同数据\n特点  数据分发，将数据从一个区域复制到另一区域，减少另一区域的读延时 读写分离 异地容灾，快速切换可用节点 复制集主要用于实现高可用，增加复制集的节点数目并不会提升写性能，因为写操作都发生在主节点上，但可以提升读性能。提升性能一般是使用分片的机制  机制  一般部署奇数个复制集，至少三个，且均要有投票权，一般分为一个主节点，用于接受写入操作和选举时的投票，两个从节点，复制主节点上的数据和选举时的投票 主节点上所有的写操作，会被记录到oplog日志中，从节点通过在主节点上打开一个tailable游标不断获取新进入主节点的oplog，进行重放，实现与主节点的数据一致 通过选举实现故障恢复  每两个节点间 每2s互发心跳，5次心跳未收到时判断为节点失联 当主节点失联时，从节点发起选举，选出新的主节点，当失联的是从节点，则不会产生选举 选举算法采用raft算法 复制集最多可以有50个节点，但具有投票权的节点最多7个   由于从节点有机会成为主节点，所以最好保证多节点的版本，配置上保持一致  分片机制 分片主要是对同一实例水平的横向扩展，将原本一个实例里的数据拆分出来多分片存储，最多支持1024个分片，所以实际上是所有分片的数据加起来才是完整数据，每一个分片也是一个复制集\n特点   提升访问性能，降低节点的压力，对外提供同一入口，屏蔽内部集群部署，通过分片路由节点mongos对请求分发到不同的分片，也有负载均衡。分发的规则是通过配置节点configServer决定，配置节点存储了数据与分片的映射。\nmongos、configServer、分片均以复制集为单位，部署多个复制集，实现高可用\n  数据存储时会自动均衡，当mongo发现数据存储分布不均衡时，会做chunk迁移，chunk的概念类似MySQL中的页，一个chunk包含多个文档 = 一页中包含多行记录\n  支持动态扩容\n  可基于集合(即表)进行分片\n  设计原则  分片模式  基于范围：根据id或者其他字段按范围进行分片存储，每一个分片的数据上是相邻的，但可能会导致热点数据分布不均 基于哈希：根据指定字段进行hash，数据分布均匀，但范围查询效率低 基于地域、时效、tag分区   分片键的选择最好选择基数大(比如id，范围大)，分布均匀，在查询条件中可以明确定位 数据量不超过3TB，尽可能保持2TB一个分片 分片的数量根据存储容量、集合(即表)和索引的占比、并发量、来决定，mongodb默认会使用物理机60%的内存来  文档模型设计原则   传统关系型数据库设计，从概念模型 -》逻辑模型 -》物理模型，关系明确，遵循三范式（1.要有主键，列不可分，2.每列与主键相关，3.不能存在传递依赖(不允许字段冗余)），表现形式上，一对多关系，外键在多那张表上，多对多关系，会有第三张表来做关联\n对于文档模型，一般对应关系型数据库设计的逻辑模型阶段，通过嵌套实体数组，map或者引用字段来处理实体间的关系，字段冗余限制宽松\n  实体间的关系，一对一使用嵌套map来表示；一对多使用嵌套数组表示；多对多使用嵌套数组+冗余字段来表示；此外，也可以通过嵌套数组存id + 另一张表来表示实体间的关系，通过id来进行联表（使用aggregate + $lookup）\n注意：嵌套时要注意整个文档大小，限制是16M，读写比列，也要注意数组长度大小，一般会使用id引用模式来解决；$lookup只支持left outer join，不支持分片表\n  模式套用：\n 场景：时序数据，解决：对于同一个实体，将其变化字段的存储和更新，聚合在内嵌数组中 场景：大文档、多相似字段、多相似索引，比如sku的多属性，解决：将多个相似字段转成内嵌数组，索引建立在内嵌数组的实体字段中， 场景：由于schemeless的特性，不同版本存在不一样的字段，需要对字段做校验，解决：增加一个版本字段，通过该字段进行判断 场景：计数统计，每秒要进行计数更新，不需要准确的计数，解决利用mongo的随机计数器$inc， 场景：精确统计，排行版，解决：使用预聚合字段，比如增加排名字段，每次更新依赖字段的同时，更新排名字段，进行排行查询时，直接根据排行字段进行排序即可，不用重新聚合    索引 原理 MMap存储引擎的 主键索引、普通索引、组合索引的数据结构都是B-树，而wiredtiger存储引擎则使用LSM树，类似b+树，现在wiredtiger引擎是默认的存储引擎\nLSM树参考\n为什么MongoDB MMAP引擎使用b-树作为索引的数据结构，而MySQL使用b+树？\n 两种数据结构的主要区别在于b-树查询的深度比较随机，节点包含了全部数据，而b+树比较平均，b-树的全表扫描比b+树差 关系型数据库因为关联性强的原因，会经常出现联表操作，b+树可以加快全表扫描的速度，而Mongo鼓励表关系用数组嵌套表示，对全表扫描的需求不是特别强烈 Mongo一般也用于定点查询，使用b-树单次查询的效率会比较高  调优  nosql语句最后添加.explain()来查看执行情况，类似MySQL的explain，一般关注executeTimeMillis(扫描花费的时间)、totalDocsExamined(扫描的总条数)、executionStages.docsExamined、executionStages.inputStage.stage(使用到什么索引)这几个字段 创建组合索引时，一般以能够 精确匹配 -》排序 -》范围匹配 的字段顺序进行创建 可以创建部分索引，比如对id\u0026gt;50的文档才会对id创建索引；对有值的字段才建索引，需要在创建索引时使用partialFilterExpression表达式 创建索引时使用后台创建索引，{background: true}  事务 mongo是分布式数据库，通过部署多复制集来实现，单个MongoDB server不支持事务，至少需要一主一从两个。\n4.0版本支持复制集多文档事务，4.2版本支持分片事务，但其实3.2版本有支持单文档原子操作了，主要是用在单文档嵌套更新上，默认自动的，MongoDB中文档 = 行记录。\n另外，Spring 5.1.1 / SpringBoot 2.x 以上 @Transactional可以支持Mongo，需要为TransactionManager注入MongoDBFactory\n分布式下的写操作 对于事务内多文档执行写操作，保证原子性，失败能回滚\nwriteConcern参数 通过参数writeConcern来保证写操作到达多少个节点才算成功，写操作会等到写到复制集后才会返回\n使用：nosql语句里包含{writeConcern: {w: 1, timeout: 3000}}，单位毫秒，超时不意味着都写失败，有可能已经写入了一些节点了，写操作性能会取决于节点的数量，会影响写入的时间，但对集群不会有压力\n 默认值为1，表示数据被写入到一个节点就算成功 =[数字x]，表示数据被写入到x个节点就算成功 =0，表示发出的写操作，不关心是否成功 =majority（推荐）表示发出的写操作需要写入到多数复制集上才算成功，一般写操作写入主节点，由主节点同步给从节点 =all，需要全部节点确认，但如果同步时有节点挂了，此时写操作会被阻塞  journal参数 写操作时，会先写到内存，在写入磁盘（journal日志文件和数据文件），参数journal字段来保证达到哪步操作才算成功，由配置文件里storage.journal.enabled控制是否开启，storage.journal.commitInternalMs决定刷盘时间间隔\n使用：nosql语句里包含 {journal: {j: true}} 会保证每次写入都会进行刷盘\n =true，表示写操作要落到journal日志文件才算成功 =false，表示写操作到达内存就算成功  journal日志与oplog日志的区别   journal日志：由MongoDB引擎层使用，主要用于控制写操作是否立即持久化，因为如果发生写操作，mongo一般是先将写操作写入内存，在定时（默认是1分钟）将内存里的数据刷盘持久化。\n如果写操作的成功判定出现在写完内存后，如果此时宕机，将会丢失写操作的记录，如果判定发生在写完journal日志之后，如果宕机可以利用journal日志进行恢复。\n写入journal日志时，也是先写入内存，再定时（默认是100ms）写入磁盘\n  oplog日志：只用在主从复制集的使用，通过oplog来实现节点间的数据同步，从节点获取主节点的oplog日志后进行重放，同步数据。oplog本身在MongoDB里是一个集合(表)\nmongo的一次写操作，包括 1.将文档数据写进集合 2.更新集合的索引信息 3.写入oplog日志，（此时只是写入内存），这三个步骤是原子性的，要么成功要么失败。一次写操作可以是批量的也可以是单条，一次写操作对应一条journal日志。\n  开启jourbal和writeConcern后的数据写入顺序：\n 写操作进来，先将写操作写入journal缓存，将写操作产生的数据写入数据缓存，将journal内存里的写操作日志同步到oplog里，异步响应给客户端； 后台线程检测到日志变化，将oplog同步给从节点；另外，后台每100ms会将journal缓存里的日志刷盘，每60s将数据缓存里的数据刷盘    分布式下读操作 readPreference参数 读操作时，决定使用那一个节点来处理读请求，也可以配合tag使用，来指定节点访问\n使用：可以设置在连接语句里，驱动程序API，nosql语句+.readPref(\u0026ldquo;primary\u0026rdquo;)\n =primary（默认，推荐），只选择主节点 =primaryPreferred：优先选择主节点，不可用时使用从节点 =secondary：只选择从节点，一般用于历史数据查询，针对时效性低的数据，或者报表服务的操作 =secondaryPreferred：优先选择从节点，不可用时选择主节点 =nearest：选择最近节点，根据ping time决定，一般用于异地部署  readConcern参数 读操作时，决定这个节点的数据哪些是可读的，类似MySQL中的隔离性\n使用：需要在mongo的配置文件的server参数中，增加enableMajorityReadConcern: true，\n对于值枚举未available、local、majority、linearizable，需要在查询语句后 + .readConcern(\u0026ldquo;local或者其他枚举\u0026rdquo;)开启使用；\n对于snapshot，则是在开启事务时指定，var session = db.getMongo().startSession(); session.startTransaction({readConcern: {level: \u0026quot;snapshot\u0026quot;}, writeConcern: {w: \u0026quot;majority\u0026quot;}}); session作为事务对象，配合snapshot快照、MVCC、redo log来实现事务。\n  =available：读取所有可用数据\n  =local（默认）：读取所有可用且属于当前分片的数据，类似available，只是限制在分片上，\nmongo为了实现数据均衡，有分片chunk迁移机制，当分片1将数据迁移至分片2，但还没迁移完成时，此时两个分片都存在该chunk数据，但是此时该数据仍属于分片1，=local时不能读到分片2的该数据，=available时则可以\n  =majority：读取在多数节点上提交完成的数据，通过MVCC机制实现，作用类似MySQL中的提交读隔离级别。\n比如，当读操作指向主节点，且写操作已经同步到多数从节点时，才可以读到该数据；当读操作指向一个从节点，且从节点完成写操作，且通知了主节点并得到主节点的响应时，才能读到该数据。\n作用：\n 主要是防止分布式数据库脏读，这里脏读指的是，在一次写操作到达多数节点前读取了这个写操作，又由于故障之类的导致写操作回滚了，此时读到的数据就算脏读。 配合writeConcern=mahority来实现读写分离，向主节点写入数据，从节点也能读到数据    =linearizable：线性读取文档，性能较差，作用类似MySQL中的serializable(串行化)隔离级别。\n只对读取单个文档时有效，保证在写操作完成后一定可以读到，一次读操作需要所有节点响应\n  =snapshot：读取最近快照中的数据，作用类似MySQL中的可重复读隔离级别，事务默认60s，需要4.2的驱动，会影响chunk迁移，多文档读操作必须使用主节点读\n 全局事务管理器通过自旋创建快照 实现在同一事务下读取同一数据的一致性，也可以读取同一事务内的写操作； 当多事务内出现写操作冲突时，排在后面事务的写操作会失败，触发abort错误，此时只能把该事务抛弃，再重新开始事务 当事务外和事务内的写操作冲突，事务内的写操作早于事务外的写操作，事务外的写操作会进入等待，直到事务内提交，才会执行    Change Stream   类似MySQL中的触发器，但它是异步的，非事务，实现变更追踪，在应用回调中触发，可同时触发多个触发器，故障后会从故障点恢复执行（故障前会拿到id，通过id来恢复）\n  基于oplog实现，在oplog上开启tailable cursor追踪复制集上的变更操作，可追踪的事件包括ddl、dml语句，在ddl、dml语句执行后触发\n  使用时需要开启readConcern，且设置read/writeConcern：majority，只推送已经在大多数节点上提交的变更操作\n  使用：db.[具体的collectionName].watch([...]).操作函数，当满足watch里的条件时会触发操作方法\n  使用场景：跨集群复制、微服务变更数据库时通知其他微服务\n  备份和恢复 备份主要是为了防止误操作，一个节点的误操作会同步到其他节点，备份同时也可以进行数据回溯\n备份方案   延迟节点备份，通过设置一个延迟节点，比如让其与主节点的存储差距延迟一个小时的数据量，通过oplog + oplog window重放实现\n  全量备份，数据文件快照(一般是某个时间点) + oplog(该时间点之后操作日志记录)实现\nmongodump -h [需要备份的mongo实例的host:port] -d [数据库名称] -o [备份存放路径]命令保存数据快照\n恢复：mongorestore -h [目标Mongo实例的host:port] -d [数据库实例名称] [[快照所在的路径]| -dir [快照所在的路径]]\n  监控 一般使用mongodb的ops manager，或者grafana实现\n监控指标 通过db.serverStatus()方法来获取指标，serverStatus方法记录的是自mongo启动以来的指标数据，常见指标有很多，一般会关注下面几个\n connections：连接数信息 locks：mongoDB使用锁的情况 network：网络使用情况统计 opconters：CURD执行次数统计 repl：复制集配置信息 men：内存使用情况 scan and order：每秒内存排序操作的平均比例 oplog window：代表oplog可容纳多长时间的操作，表示从节点可以离线多久后可以追上主节点的数据 wiredTiger：包含大量WirdTiger引擎执行情况的信息，如block-manager：WT数据块的读写情况，session：session使用量，concurrentTransactions：ticket使用情况 metrics：一系列指标统计信息  参考 极客时间 - mongodb高手课\n","date":"2021-11-28T00:00:00Z","permalink":"http://nixum.cc/p/mongodb/","title":"MongoDB"},{"content":"[TOC]\n底层原理 容器技术的核心功能，就是通过约束和修改进程的动态表现，从而为其创造出一个边界。\nNamespace - 隔离 进程只能看到被规定的视图，即 隔离，比如通过docker启动一个/bin/sh，再在容器里通过ps命令查看该/bin/sh进程的pid，会发现它的pid是1，但是实际上它在外部的宿主机里的pid是10，使得让在容器里运行的进程以为自己就在一个独立的空间里，实际上只是进行了逻辑的划分，本质还是依赖宿主机。\n作用：在同一台宿主机上运行多个用户的容器，充分利用系统资源；不同用户之间不能访问对方的资源，保证安全。\n常见的Namespace类型有：\n PID Namespace：隔离不同容器的进程 Network Namespace：隔离不同容器间的网络 Mount Namespace：隔离不同容器间的文件系统  与虚拟化的区别：虚拟化是在操作系统和硬件上进行隔离，虚拟机上的应用需要经过虚拟机在经过宿主机，有两个内核，本身就有消耗，而容器化后的应用仅仅只是宿主机上的进程而已，只用到宿主机一个内核\n因为namespace隔离的并不彻底，由于内核共享，容器化应用仍然可以把宿主机的所有资源都吃掉，有些资源不同通过namespace隔离，比如修改了容器上的时间，宿主机上的时间也会被改变，因此需要Cgroups\nCgroups - 资源限制 是用来制造约束的主要手段，即对进程设置资源能够使用的上限，如CPU、内存、IO设备的流量等\n比如，限定容器只能使用宿主机20%的CPU\n1  docker run -it --cpu-period=100000 --cpu-quota=20000 ubuntu /bin/bash    Cgroups 通过不同的子系统限制了不同的资源，每个子系统限制一种资源。每个子系统限制资源的方式都是类似的，就是把相关的一组进程分配到一个控制组里，然后通过树结构进行管理，每个控制组都设有自己的资源控制参数。\n 常见的Cgroups子系统\n CPU 子系统，用来限制一个控制组（一组进程，你可以理解为一个容器里所有的进程）可使用的最大 CPU。 memory 子系统，用来限制一个控制组最大的内存使用量。 pids 子系统，用来限制一个控制组里最多可以运行多少个进程。 cpuset 子系统， 这个子系统来限制一个控制组里的进程可以在哪几个物理 CPU 上运行。  Cgroups 有 v1 和 v2 两个版本，v1中每个进程在各个Cgroups子系统中独立配置，可以属于不同的group，比较灵活但因为每个子系统都是独立的，会导致对同一进程的资源协调困难，比如同一容器配置了Memory Cgroup和Blkio Cgroup，但是它们间无法相互协作。\nv2针对此做了改进，使各个子系统可以协调统一管理资源。\nMount Namespace与rootfs(根文件系统) 挂载在容器根目录上、用来为容器进程提供隔离后执行环境的文件系统，即容器镜像，也是容器的根文件系统。Mount Namespace保证每个容器都有自己独立的文件目录结构。\n镜像可以理解为是容器的文件系统（一个操作系统的所有文件和目录），它是只读的，挂载在宿主机的一个目录上。同一台机器上的所有容器，都共享宿主机操作系统的内核，如果容器内应用修改了内核参数，会影响到所有依赖的应用。而虚拟机则都是独立的内核和文件系统，共享宿主机的硬件资源。\n 上面的读写层通常也称为容器层，下面的只读层称为镜像层，所有的增删查改操作都只会作用在容器层，相同的文件上层会覆盖掉下层。知道这一点，就不难理解镜像文件的修改，比如修改一个文件的时候，首先会从上到下查找有没有这个文件，找到，就复制到容器层中，修改，修改的结果就会作用到下层的文件，这种方式也被称为copy-on-write。\n 注意点 容器是“单进程模型”，单进程模型并不是指容器只能运行一个进程，而是指容器没有管理多个进程的能力，它只能管理一个进程，即如果在容器里启动了一个Web 应用和一个nginx，如果nginx挂了，你是不知道的。\n另外，直到JDK 8u131以后，java应用才能很好的运用在docker中，在此之前可能因为docker隔离出的配置和环境，导致JVM初始化默认数值出错，因此如果使用以前的版本，需要显示设置默认配置，比如直接规定堆的最大值和最小值、线程数之类的\n进程 Linux中的进程状态\n \n活着的进程有两种状态：\n 运行态(TASK_RUNNING)：进程正在运行，或 处于run queue队列里等待 睡眠态(TASK_INTERRUPTIBLE、TASK_UNINTERRUPTIBLE)：因为需要等待某些资源而被放在了wait queue队列，该状态包括两个子状态：  可被打断状态(TASK_INTERRUPTIBLE)：此时ps查看的Stat的值为 S 不可被打断状态(TASK_UNINTERRUPTIBLE)：此时ps查看的Stat的值为 D    进程退出时会有两个状态：\n  EXIT_ZOMBIE状态：僵尸状态；之所以有这个状态是为了给父进程可以查看子进程PID、终止状态、资源使用信息的机会，如果子进程直接消失，父进程则没有机会掌握子进程具体的终止情况。\n  EXIT_DEAD状态：真正结束退出时一瞬间的状态\n  init进程 init进程也称1号进程，是第一个用户态进程，由它直接或间接创建了Namespace中的其他进程。Linux系统本身在启动后也是这么干的，会先执行内核态代码，然后根据缺省路径尝试执行1号进程的代码，从内核态切换到用户态，比如Systemd。\n在容器中，无法使用SIGKILL(-9)和SIGTOP(19)这两个信号杀死1号进程，但对于其他kill信号(比如默认的kill信号是SIGTERM)，如果init进程注册了自己处理该信号的handler，则1号进程可以做出响应。但是，如果SIGKILL和SIGTOP信号是从Host Namespace里发出的，则可以被响应，因为此时容器里的1号进程在宿主机上只是一个普通进程。\n有时无法被kill掉的原因：\nLinux执行kill命令，实际上只是发送了一个信号给到Linux进程，可被调度的进程在收到信号后，一般会从 **默认行为(每个信号都有)、忽略、捕获后处理(需要用户进程自己针对这个信号做handler)**中进行操作，但SIGKILL和SIGTOP这两个信号是特权信号，不允许被自行捕获处理也无法忽略，只能执行系统的默认行为。\n执行kill命令时，会调用一系列内核函数进行处理，其中有一个函数sig_task_ignored会判断是否要忽略这个信号。Linux内核里的每个Namespace里的init进程，会忽略只有默认handler的信号，即如果我们的进程有处理相关信号的handler，就可以响应。可以通过cat /proc/{进程pid}/status | grep -i SigCgt查看该进程注册了哪些handler。\nZOMBIE进程 ps aux查看Linux进程，STAT里的状态是Z，ps后有defunct标记，表示该进程为僵尸进程，此时该进程不可被调度。僵尸进程是Linux进程退出状态的一种，进程处于此状态下，无法响应kill命令，虽然资源被释放，但是仍然占用着进程号。\n形成的原因：\n父进程在创建完子进程后，没有对子进程进行后续的管理。\n影响：\nLinux内核在初始化系统时，会根据CPU数目现在进程最大数量通过/proc/sys/kernel/pid_max查看，对Linux系统而言，容器是一组进程的集合，如果容器中的应用创建过多，就会出现fork bomb行为，不断建立新进程消耗系统资源，如果达到了Linux最大进程数，导致系统不可用。\n解决方案：\n因此对于容器，也要限制容器内的进程数量，通过pdis Cgroup来完成，限制进程数目大小。在一个容器建立后，创建容器的服务会在/sys/fs/cgroup/pids下建立一个子目录作为控制组，里面的pids.max文件表示容器允许的最大进程数目。当容器内进程达到最大限制后再起新进程，会报错Resource temporarily unavailable\n当出现僵尸进程后，父进程可以调用wait函数（该函数是同步阻塞）或者调用waitpid函数（该函数仅在调用时检查僵尸进程，如果没有则返回，不会阻塞等待）回收子进程资源，避免僵尸进程 的产生。或者kill掉僵尸进程的父进程，此时僵尸进程会归附到init进程下，利用init进程的能力回收僵尸进程的资源。\ninit进程是所有进程的父进程，init进程具备回收僵尸进程的能力。\n或者把容器内的init进程替换为tini进程，该进程具备自动回收收子进程的能力。\n进程的退出 当我们停止一个容器时，比如docker stop，容器内的init进程会收到SIGTERM信号，而其他进程会收到SIGKILL信号，这意味着只有init进程才能注册handler处理信号实现graceful shotdown，而其他进程不行，直接就退出了。\n如果想要容器内的其他进程能收到SIGTERM信号，只能在init进程中注册一个Handler，将收到的信号转发到子进程中，在init进程退出之前把子进程都停掉，子进程就不会收到SIGKILL信号了。\nCPU  \nCPU Cgroup 容器会使用CPU Cgroup来控制CPU的资源使用。CPU Cgroup只会对用户态us和ni、内核态sy做限制，不对wa、hi、si这些I/O或者中断相关做限制。\nCPU Cgroup一般会通过一个虚拟文件系统挂载点的方式，挂载在/sys/fs/cgroup/cpu目录下，每个子目录为一个控制组，各个目录间是一个树状的层级关系。\n对于普通调度类型，每个目录下有三个文件对应三个参数：\n  cpu.cfs_period_us：CFS的调度周期，单位微秒\n  cpu.cfs_quota_us：一个调度周期内该控制组被允许运行的时间，单位微秒，CPU最大配额 = cpu.cfs_quota_us / cpu.cfs_period_us \n  cpu.shares：CPU Cgroup对控制组之间的CPU分配比例，只有当控制组间的CPU配额超过了CPU可以资源的最大值，则会启用该参数进行配额分配。\n  通过Linux中cpu.cfs_period_us是个固定值，Kubernetes的pod中，限制容器CPU使用率的requestCPU和limitCPU就是通过调整其余两个参数来实现。\n在容器内使用top命令查看CPU使用率，显示的是宿主机的CPU使用率以及单个进程的CPU使用率，无法查到该容器的CPU使用率。\n因为top命令对于单个进程读取/proc/[pid]/stat里面包含进程用户态和内核态的ticks数目，对于整个节点读取的是/proc/stat里各个不同CPU类型的ticks数目，因为这些文件不属于任何一个Namespace，因此无法读取单个容器CPU的使用率，只能通过CPU Cgroup的控制组内的cpuacct.stat参数文件计算得到，该参数文件包含了这个控制组里所有进程的内核态ticks和用户态ticks的值，带入公式即可计算得到。\n ticks是Linux操作系统中的一个时间单位，Linux通过自己的时钟周期性产生中断，每次中断会触发内核做一次进程调度，一次中断就是一个ticks\nutime：表示进程在用户态部分在Linux调度中获得CPU的ticks，这个值会一直累加\nstime：表示进程在内核态部分在Linux调度中获得CPU的ticks，这个值会一直累加\nHZ：时钟频率\net：utime_1和utime_2这两个值的时间间隔\n进程的 CPU 使用率 =((utime_2 – utime_1) + (stime_2 – stime_1)) * 100.0 / (HZ * et * CPU个数 )\n Load Average平均负载 Load Average是指Linux进程调度器中一段时间内，可运行队列里的进程平均数 + 休眠队列中不可打断的进程平均数。\n所以，有可能在使用top命令时观察到 明明CPU空闲率很高，但是Load Average的数值也很高，CPU性能下降，因为此时休眠队列中有很多在等待的进程，这些进程的stat是D，这种状态可能是IO或者信号量锁的访问导致。\n内存 Linux进程的内存申请策略：Linux允许进程在申请内存时overcommit，即允许进程申请超过实际物理内存上限的内存。因为申请只是申请内存的虚拟地址，只是一个地址范围，只有真正写入数据时，才能得到真实的物理内存，当物理内存不够时，Linux就会根据一定的策略杀死某个正在运行的进程，当容器内存使用率超过限制值时，容器就会出现OOM Killed。\nOOM Killed的标准是通过oom_badness函数决定，通过以下两个值的乘积决定：\n 进程已经使用的物理内存页面数 每个进程的OOM校准值oom_score_adj，在/proc/[pid]/oom)score_adj文件中，该值用于调整进程被OOM Kill的几率  **Linux的内存类型：**有两类，一类是内核使用的内存，如页表、内核栈、slab等各种cache pool；另一类是用户态使用的内存，如堆内存、栈内存、共享库内存、文件读写的Page Cache。\nRSS（Resident Set Size）：进程真正申请到的物理页面的内存大小，RSS内存包括了进程的代码段内存，堆内存、栈内存、共享库内存，每一部分RSS内存的大小可查看/proc/[pid]/smaps\n**Page Cache：**为了提高磁盘文件的读写性能，Linux在有空闲的内存时，默认会把读写过的页面放在Page Cache里，一旦进程需要更多的物理内存，但剩余的内存不够，则会使用(page frame reclaim)这种内存页面回收机制，根据系统里空闲物理内存是否低于某个阈值，决定是否回收Page Cache占用的内存。\nMemory Cgroup Memory Cgroup一般会通过一个虚拟文件系统挂载点的方式，挂载在/sys/fs/cgroup/memory目录下，每个子目录为一个控制组，各个目录间是一个树状的层级关系。每个目录有很多参数文件，跟OOM相关的有3个\n memory.limit_in_bytes：控制控制组里进程可使用的内存最大值，父节点的控制组内该值可以限制它的子节点所有进程的内存使用，Kubernetes的limit改的就是该值，而request仅在调度时计算节点是否满足。 memory.oom_control：当控制组中的进程内存使用达到上限时，该参数决定是否触发OOM Killed，默认是会触发OOM Killed，只能杀死控制组内的进程，无法杀死节点上的其他进程。当值设置为1时，表示控制组内进程即使达到limit_in_bytes设置的值，也不会触发OOM Killed，但是这可能会影响控制组中正在申请物理内存页面的进程，造成该进程处于停止状态，无法继续运行。 memory.usage_in_bytes：表示当前控制组里所有进程实际使用的内存总和，与limit_in_bytes的值越接近，越容易触发OOM Killed。  当发送OOM killed时，可以查看内核日志 journalctl -k 或者 查看日志文件 /var/log/message\nMemory Cgroup只统计了RSS和Page Cache这两部分内存，两者的和等于memory.usage_in_bytes，当控制组里的进程要申请新的物理内存，但memory.usage_in_bytes已超过memory.limit_in_bytes，此时会启用page frame reclaim内存页面回收机制，回收Page Cache的内存。\n判断容器真实的内存使用量，不能仅靠Memory Cgroup里的memory.usage_in_bytes，而需要memory.stat里的rss值，该值才真正反应了容器使用的真实物理内存的大小。\nSwap 容器可以使用Swap空间，但会导致Memory Cgroup失效，比如在Swap空间足够的情况下，设置容器limit为512MB，然后容器申请了1G内存，是可以申请成功的，此时容器的RSS为512MB，Swap只剩下512MB空闲。\n为了解决这个问题，可以使用swapiness，参数文件在/proc/sys/vm/swapiness，通过它来设置Swap使用的权重，用于定义Page Cache内存和匿名内存释放的比例值，取值范围是0到100，默认是60。100表示匿名内存和Page Cache内存的释放比例是100：100；60表示匿名内存和PageCache内存的释放比例是60：140，此时PageCache内存的释放优先于匿名内存；0不会完全禁止Swap的使用，在内存特别紧张的时候才会启用Swap来回收匿名内存。\nswapiness也存在于Memory Cgroup控制组中，参数文件是memory.swappiness，该值的优先级会大于全局的swappiness，但有一点跟全局的swappiness不太一样，当memory.swappiness=0时，是完全不使用Swap空间的。\n通过memory.swappiness参数可以使得需要使用Swap容器和不需要使用Swap的容器，同时运行在同一个节点上。\n存储 Linux两种文件 I / O模式：\nDirect I/O：用户进程写磁盘文件，通过Linux内核文件系统 -\u0026gt; 块设备层 -\u0026gt; 磁盘驱动 -\u0026gt; 磁盘硬件，直到落盘。\nBuffer I/O：先把文件数据写入内存就返回，Linux内核有现成会把内存中的数据再写入磁盘，性能更佳。\n当文件数据先写入内存时，存在内存的数据叫 dirty pages\n 当 dirty pages 数量超过 dirty_background_ratio（百分比，默认是10%） 对应的内存量的时候，内核 flush 线程就会开始把 dirty pages 写入磁盘 ;\n当 dirty pages 数量超过 dirty_ratio （百分比，默认是20%）对应的内存量，这时候程序写文件的函数调用 write() 就会被阻塞住，直到这次调用的 dirty pages 全部写入到磁盘；\ndirty_writeback_centisecs，时间值，单位是百分之一秒，默认是5秒，即每5s会唤醒内核的flush线程来处理dirty pages；\ndirty_expire_centisecs，时间值，单位是百分之一秒，默认是30s，定义dirty pages在内存的存放的最长时间，如果超过该时间，就会唤醒内核的flush线程处理dirty pages；\n UnionFS 将多个目录(处于不同的分区)一起挂载在一个目录下，实现多目录挂载。这种方式可以使得同一节点上，多个容器使用同一份基础镜像，减少磁盘冗余的镜像数据。OverlayFS是其中的一种实现。\n比如容器A和容器B都使用了ubuntu作为基础镜像，放在目录/ubuntu上，容器A使用的额外应用程序放在appA目录，容器B的放在appB目录，将/ubuntu目录和appA目录同时挂载在containA目录下，作为容器A看到的文件系统，同理容器B也是，此时节点就只需要保存一份ubuntu镜像文件即可。\nOverlayFS OverlayFS是一种堆叠文件系统，依赖并建立在其他文件系统之上，如EXT4FS、XFS等，并不直接参与磁盘空间结构的划分，仅将原来底层文件系统中不同目录进行合并，用户见到的Overlay文件系统根目录下的内容就来自挂载时指定的不同目录的合集，OverlayFS会将挂载的文件进行分层。\n lower：文件的最底层，不允许被修改，只读，OverlayFS支持多个lowerdir，最多500个； upper：可读写层，文件的创建、修改、删除都会在这一层反应，只有一个； merged：挂载点目录，用户实际对文件的操作都在这里进行； work：存放临时文件的目录，如果OverlayFS中有文件修改，中间过程中会临时存放文件到这里；  上下层同名目录合并，上下层同名文件覆盖，lower层文件写时会拷贝(copy_up)到upper层进行修改，不影响lower层的文件。\n比如，在merged目录里新键文件，这个文件会出现在upper目录中；删除文件，这个文件会在upper目录中消失，但在lower目录里不会有变化，只是upper目录中会增加一个特殊文件告诉OverlayFS该文件不能出现在merged目录里，表示它被删除；修改文件，会在upper目录中新建一个修改后的文件，而lower目录中原来的文件不会改变。\n在Docker中，容器镜像文件可以分成多个层，每层对应lower dir的一个目录，容器启动后，对镜像文件中的修改会保存在upper dir里。Docker会将镜像层作为lower dir，容器层作为upper dir，最后挂载到容器的merge挂载点，即容器的根目录下。\nOverlayFS本身没有限制文件写入量的功能，需要依赖底层文件系统，比如XFS文件系统的quota，限制upper目录的写入大小。\nBlkio Cgroup IOPS：每秒钟磁盘读写的次数\n吞吐量：每秒钟磁盘读取的数据量，有时也称为带宽\n两者的关系：吞吐量 = 数据块大小 * IOPS\nBlkio Cgroup一般会通过一个虚拟文件系统挂载点的方式，挂载在/sys/fs/cgroup/blkio目录下，每个子目录为一个控制组，各个目录间是一个树状的层级关系。每个目录有很多参数文件，有4个主要参数来限制磁盘 I/O性能。\n blkio.throttle.read_iops_device：磁盘读取IOPS限制 blkio.throttle.read_bps_device：磁盘读取吞吐量限制 blkio.throttle.write_iops_device：磁盘写入IOPS限制 blkio.throttle.write_bps_device：磁盘写入吞吐量限制  但在Cgroup V1，只有Direct I/O才能通过Blkio Cgroup限制，Buffered I/O不能，因为Buffered I/O会用到Page Cache，但V1版本各个Cgroup子系统相互独立，所以没办法做限制。\nCgroup V2才可以，通过配置Blkio Cgroup和Memory Cgroup即可解决。有个问题是，如果Memory Cgroup的memory.limit_in_bytes设置得比较小，而容器中进程有大量的IO、这样申请新的Page Cache内存时，又会不断释放老的内存页面，带来了额外的开销，可能会产生写入波动。\n网络 容器网络一个Network Namespace网络栈包括：网卡、回环设备、路由表、iptables规则。\nNetwork Namespace Network Namespace主要用来隔离网络资源，如\n 网络设备，比如 lo(回环设备)、eth0等，可以通过ip link命令查看 IPv4和IPv6协议栈，IP层以及上面的TCP和UDP协议栈都是每个Namespace独立工作，这些参数大都在/proc/sys/net目录下，包括TCP和UDP的port资源。 IP路由表，也是每个Namespace独立工作，使用ip route命令查看 防火墙规则，即iptables，每个Namespace可独立配置 网络状态信息，可以从/proc/net和/sys/class/net里查看，包括了上面几种资源的状态信息  在宿主机上，可以使用lsns -t net命令查看系统已有的Network Namespace，使用nsenter或ip netns这个命令进入某个Network Namespace里查看具体的网络配置。\n网络相关参数很大一部分放在了/proc/sys/net目录下，比如tcp相关的参数，可以直接修改，也可以使用sysctl命令修改。\n出于安全考虑，对于非privileged容器，/proc/sys是read-only挂载的，容器启动后无法内部修改该目录下相关的网络参数，只能通过runC sysctl相关接口，在容器启动时对容器内的网络参数进行修改，比如\n如果使用Docker，可以加上 --sysctl 参数名=参数值来修改，如果是K8s，则需要用到 allowed unsaft sysctl这个特性了。\n可以使用系统函数clone()或unshare()来创建Namespace，比如Docker或者containerd启动容器时，是通过runC间接调用unshare函数启动容器的。\n网络通信 容器与外界通信，总共分成两步：\n  数据包从容器的Network Namespace发送到Host Network Namespace\n  数据包从Host Network Namespace，从宿主机的eth0上发送出去\n  可以使用 tcpdump抓包工具 查看数据包在各个设备接口的日志\n比如查看容器内eth0接收数据包的情况 ip netns exec [pid] tcpdump -i eth0 host [目标ip地址] -nn，查看veth_host或其他设备使用 tcpdump -i [设备名如veth_host、docker0、eth0] host [目标ip地址] -nn \n同一节点下容器间的通信 在 Linux 中能够起到虚拟交换机作用的网络设备，是网桥。它是一个工作在数据链路层的设备，主要功能是根据 MAC 地址来将数据包转发到网桥的不同端口（Port）上，因此Docker 项目会默认在宿主机上创建一个名叫 docker0 的网桥，凡是连接在 docker0 网桥上的容器，就可以通过它来进行通信。\n容器里会有一个eth0网卡，作为默认的路由设备，连接到宿主机上一个叫vethxxx的虚拟网卡，而vethxxx网卡又插在了docker0网桥上，这一套虚拟设备就叫做Veth Pair。每个容器对应一套VethPair设备，多个容器会将其Veth Pair注册到宿主机的docker0网桥上，即Veth Pair相当于是连接不同Network Namespace的网线，一端在容器，一端在宿主机。此时，数据包就能从容器的Network Namespace发送到Host Network Namespace上了。\ndocker0和容器会组成一个子网，docker0上的ip就是这个子网的网关ip。\n网络请求实际上就是在这些虚拟设备上进行映射（经过路由表，IP转MAC，MAC转IP）和转发，到达目的地。\n同一节点内，容器间通信一般流程：容器A往容器B的IP发出请求，请求先经过容器A的eth0网卡，发送一个ARP广播，找到容器B IP对应的MAC地址，宿主机上的docker0网桥，把广播转发到注册到其身上的其他容器的eth0，容器B收到该广播后把MAC地址发给docker0，docker0回传给容器A，容器A发送数据包给docker0，docker0接收到数据包后，根据数据包的目的MAC地址，将其转发到容器B的eth0，\n同一节点内，宿主机与容器通信一般流程：宿主机往容器的IP发出请求，这个请求的数据包先根据路由规则到达docker0网桥，转发到对应的Veth Pair设备上，由Veth Pair转发给容器内的应用。\n关于容器缺省使用的peer veth方案，由于从容器的veth0到宿主机的veth0会有一次软中断，带来了额外的开销，时延会高一些，可以使用 ipvlan/macvlan的网络接口替代，ipvlan/macvlan 直接在物理网络接口上虚拟出接口，容器发送网络数据包时直接从容器的eth0发送给宿主机的eth0，减少了转发次数，实延就降低了。但是该方案无法使用iptables规则，Kubernetes的service就无法使用该方案。\n \n容器访问另一节点 一个节点内的容器访问另一个节点一般流程：先经过docker0网桥，出现在宿主机上，根据路由表或者nat的方式，知道目标节点是其他机器，则将数据转发到宿主机的eth0网卡上，再发往目标节点。\n其实跟同一节点内，宿主机与容器通信类似，最终转化为节点间的通信。\n所以当容器无法访问外网时，就可以检查docker0网桥是否能ping通，查看docker0和Veth Pair设备的iptables规则是否有异常。\n \n不同节点下容器间的通信 默认配置下，不同节点间的容器、docker0网桥，是不知道彼此的，没有任何关联，想要跨主机容器通信，就需要在多主机间在建立一个公共网桥，所有节点的容器都往这个网桥注册，才能进行通信。通过每台宿主机上有一个特殊网桥来构成这个公用网桥，这个技术被称为overlay network（覆盖网络）。\n常见的解决方案是Flannel、Calico等。\n \nDocker的网络模型   bridge模式（默认）\nDocker进程启动时，会在主机上创建一个名为docker0的虚拟网桥，此主机上启动的Docker容器会分配一个Network Namespace，通过eth0-veth虚拟设备连接到宿主机的docker0网桥上。\n  host模式\n此模式下容器不会获得独立的Network Namespace，和宿主机共用一个Network Namespace，容器也不会虚拟自己的网卡和ip，而是用宿主机的ip和端口。\n  container模式\n指定新创建的容器和一个已存在的容器共享一个Network Namespace，新创建的容器不会创建自己的网卡和IP，而是和指定的容器共享IP、端口范围\n  none模式\nDocker容器拥有自己的Network Namespace，但并不为Docker容器进行任何网络配置，需要手动添加。\n  安全 进程的权限分为两类，特权用户进程(进程有效用户ID是0，root用户的进程)、非特权用户进程(进程有效用户ID非0，非root用户的进程)，特权用户可以执行Linux系统上所有操作，而非特权用户执行这些操作会被内核限制。\n在kernel2.2开始，Linux把特权用户的特权做了划分，每个被划分出来的单元称为capability，比如运行iptables命令，对应的进程需要有CAP_NET_ADMIN这个capability。非root用户启动的进程默认没有任务capabilities，root用户则包含了所有。子父进程的capabilities有继承属性。查看进程拥有的capability命令：cat /proc/[pid]/status | grep Cap。\n对于privileged容器，实际上就是拥有了所有capability，允许执行所有特权操作，比如docker容器启动时增加参数 --privileged。容器缺省启动时，是root用户，但只允许了15个capabilities。\n一般只给容器所需操作的最小capabilities，而不是直接给privileged，因为privileged的权限太大，容器可以轻易获取宿主机上的所有资源，比如直接访问磁盘设备，修改宿主机上的文件。\n为了保证容器运行不对宿主机造成安全影响，可以在容器中指定用户。\n在docker中，启动容器命令后面加-u [uid]/[gid]，或者在写Dockerfile时指定(这样启动容器就不用加-u)，容器里的用户与宿主机上的用户共享，即容器上的uid实际上是宿主机上的uid(root用户也是，只是容器里capabilities只有15个，而宿主机上的root有全部)，这样会产生限制，因为在Linux上每个用户的资源是有限的，比如打开文件数目、最大进程数等，如果有多个容器共享同一个uid，就会互相影响。\n为了解决容器uid共享问题，可以使用User Namespace。\nUser Namespace本质上是将容器中的uid/gid与宿主机上的uid/gid建立映射，同时也支持嵌套映射。比如划分宿主机上的uid的范围1000-1999，对应容器uid的0到999，容器里uid也可以继续嵌套映射。\n使用User Namespace，可以把容器中root用户映射称宿主机上的普通用户，解决uid冲突共享问题，目前Kebernetes还不支持User Namespace(当前时间20210920，不过github上有pr了)\nrunC 与 OCI  OCI：围绕容器格式和运行时制定一个开放的工业化标准，包含容器运行时标准 （runtime spec）和 容器镜像标准（image spec）。 runC：一个轻量工具，基于libcontainer库，由golang语言实现，不需要docker引擎，它根据 OCI 标准来创建和运行容器的，但不包含镜像管理功能。 OCI bundle ：包括容器的文件系统和一个 config.json 文件。有了容器的根文件系统后就可以通过 runc spec 命令来生成 config.json 文件，config.json 文件用于说明如何运行容器，包括要运行的命令、权限、环境变量等等内容  OCI 定义的容器状态：\n  creating：使用 create 命令创建容器，这个过程称为创建中。 created：容器已经创建出来，但是还没有运行，表示镜像文件和配置没有错误，容器能够在当前平台上运行。 running：容器里面的进程处于运行状态，正在执行用户设定的任务。 stopped：容器运行完成，或者运行出错，或者 stop 命令之后，容器处于暂停状态。这个状态，容器还有很多信息保存在平台中，并没有完全被删除。 paused：暂停容器中的所有进程，可以使用 resume 命令恢复这些进程的执行。   Docker 进程   dockerd ：Docker Engine守护进程，直接面向操作用户。dockerd 启动时会启动 containerd 子进程，他们之前通过RPC进行通信。 containerd ：dockerd和runc之间的一个中间交流组件。他与 dockerd 的解耦是为了让Docker变得更为的中立，而支持OCI 的标准 。 containerd-shim ：用来真正运行的容器的，每启动一个容器都会起一个新的shim进程， 它主要通过指定的三个参数：容器id，boundle目录（containerd的对应某个容器生成的目录，一般位于：/var/run/docker/libcontainerd/containerID）， 和运行命令（默认为 runc）来创建一个容器。 docker-proxy ：用户级的代理路由。只要你用 ps -elf 这样的命令把其命令行打出来，你就可以看到其就是做端口映射的。如果不想要这个代理的话，可以在 dockerd 启动命令行参数上加上： \u0026ndash;userland-proxy=false 这个参数。   Dockerfile 指令详解\n RUN  后面一般接shell命令，但是会构建一层镜像\n要注意RUN每执行一次指令都会在docker上新键一层，如果层数太多，镜像就会太过膨胀影响性能，虽然docker允许的最大层数是127层。\n有多条命令可以使用\u0026amp;\u0026amp;连接\n CMD  要注意CMD只允许有一条，如果有多条只有最后一条会生效\n参考 极客时间-深入剖析k8s-张磊\n极客时间-容器实战高手课\n","date":"2021-11-06T00:00:00Z","permalink":"http://nixum.cc/p/%E5%AE%B9%E5%99%A8/","title":"容器"},{"content":"[TOC]\n对微服务的理解 http://dockone.io/article/3687\n限流 下面的方案都是单机版本的，在分布式环境下可以把限流的实例放到Redis里，或者直接使用Lua脚本实现，保证并发安全。\n固定窗口 规定单位时间内可访问的次数，比如规定接口一分钟内只能访问10次，以第一次请求为起始，计数1，一分钟内计数超过10后，后续的请求直接拒绝，只能等到这一分钟结束后，重置计数，重新开始计数。\n但是这样有个问题，如果在大部分请求集中在第一个窗口的后10s内，和第二个窗口的前10s内，虽然他们都符合限流策略，但是在临界的20s内，请求还是有可能压垮系统。\n算法Demo：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47  type fixWinLimiter struct { lock *sync.Mutex maxLimitCount int64 // 最大限制数 \tcurrentCount int64 // 当前计数 \tfixInterval int64 // 为了简化，单位设置为秒 \tlastReqStartAt int64 // 秒级时间戳 } func NewFixWinLimit(fixInterval int64, maxLimitCount int64) *fixWinLimiter { return \u0026amp;fixWinLimiter{ lock: new(sync.Mutex), maxLimitCount: maxLimitCount, fixInterval: fixInterval, } } func (f *fixWinLimiter) IsPass() bool { f.lock.Lock() defer f.lock.Unlock() now := time.Now().Unix() if now - f.lastReqStartAt \u0026gt; f.fixInterval { f.currentCount = 0 f.lastReqStartAt = now return true } if f.currentCount + 1 \u0026gt;= f.maxLimitCount { return false } f.currentCount++ return true } // 测试 func main() { // 10s内只允许通过10次 \tf1 := NewFixWinLimit(10, 10) for i := 0; i \u0026lt; 20; i++ { go func(index int) { if f1.IsPass() { fmt.Println(fmt.Sprintf(\u0026#34;pass: %d\u0026#34;, index)) } else { fmt.Println(fmt.Sprintf(\u0026#34;no pass: %d\u0026#34;, index)) } }(i) } time.Sleep(100 * time.Second) }   滑动窗口 与固定窗口类似，只是以时间片划分，比如规定窗口一秒内只能请求5次，每次请求时会按照前一秒内的请求数量进行判断，超过则拒绝。\n这种算法本质上只是把固定窗口计数限流的时间片变小了，仍然有可能出现固定窗口计数限流的临界点问题。\n算法Demo：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55  type slidingWinLimiter struct { lock *sync.Mutex limitInterval int64 // 限制的时间间隔 \tlastReqAt int64 // 上一次请求的时间  winCount []int64 // 窗口计数 \tcurrentWinIndex int64 // 当前窗口索引 \tcurrentWinMaxLimit int64 // 当前窗口最大限制数 \twinNum int64 // 窗口数量 } func NewSlidingWinLimiter(limitInterval int64, winMaxLimitCount int64, winNum int64) *slidingWinLimiter { return \u0026amp;slidingWinLimiter{ lock: new(sync.Mutex), limitInterval: limitInterval, lastReqAt: time.Now().Unix(), winCount: make([]int64, winNum), currentWinMaxLimit: winMaxLimitCount, winNum: winNum, } } func (s *slidingWinLimiter) IsPass() bool { s.lock.Lock() defer s.lock.Unlock() now := time.Now().Unix() // 每个窗口的时间间隔 \teachWinInterval := float64(s.limitInterval) / float64(s.winNum) // 判断前后两次请求的时间差是否在当前窗口内，不是则重置当前窗口，使用下一个窗口 \tif float64(now-s.lastReqAt) \u0026gt; eachWinInterval { s.winCount[s.currentWinIndex] = 0 s.currentWinIndex = (s.currentWinIndex + 1) % s.winNum s.lastReqAt = now } // 判断是否超过当前窗口的限制 \tif s.winCount[s.currentWinIndex] \u0026gt;= s.currentWinMaxLimit { return false } s.winCount[s.currentWinIndex]++ return true } func main() { f1 := NewSlidingWinLimiter(1, 10, 1) for i := 0; i \u0026lt; 20; i++ { go func(index int) { if f1.IsPass() { fmt.Println(fmt.Sprintf(\u0026#34;pass: %d\u0026#34;, index)) } else { fmt.Println(fmt.Sprintf(\u0026#34;no pass: %d\u0026#34;, index)) } }(i) } time.Sleep(100 * time.Second) }   漏桶 控制流水，水(请求)持续加入桶中，底部以恒定速度流出，如果加水速度大于漏出速度，水则溢出，请求拒绝。即宽进严出，无论请求多少，请求的速率有多大，都按照固定的速率流出。\n一般用在对第三方提供服务的请求限制上，比如我们服务接入shopify的服务，为了不触发shopify的限流，就可以使用漏桶算法。\n算法Demo：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54  type bucketLimiter struct { lock *sync.Mutex lastReqAt int64 // 单位：秒 \tbucketCap int64 // 桶的容量 \tbucketBalance int64 // 桶的余量 \trate int64 // 每时间单位内漏桶的漏出速率 } func NewBucketLimiter(rate int64, bucketCap int64) *bucketLimiter { return \u0026amp;bucketLimiter{ lock: new(sync.Mutex), bucketCap: bucketCap, bucketBalance: 0, rate: rate, lastReqAt: time.Now().Unix(), } } func (b *bucketLimiter) IsPass() bool { b.lock.Lock() defer b.lock.Unlock() now := time.Now().Unix() // 计算时间差内可通过的计数量 \tdiffInterval := now - b.lastReqAt diffCount := diffInterval * b.rate // 计算桶中剩余的量 \tb.bucketBalance -= diffCount if b.bucketBalance \u0026lt; 0 { b.bucketBalance = 0 } b.lastReqAt = now // 判断是否加水后是否溢出 \tif b.bucketBalance+1 \u0026lt;= b.bucketCap { b.bucketBalance++ return true } return false } // 测试 func main() { // 单位时间内漏出速度是2，桶的容量是5 \tf1 := NewBucketLimiter(2, 5) for i := 0; i \u0026lt; 20; i++ { go func(index int) { if f1.IsPass() { fmt.Println(fmt.Sprintf(\u0026#34;pass: %d\u0026#34;, index)) } else { fmt.Println(fmt.Sprintf(\u0026#34;no pass: %d\u0026#34;, index)) } }(i) } time.Sleep(100 * time.Second) }   令牌桶 类似信号量，令牌桶会单独维护一个令牌的存储桶，为这个桶设置一个上限，同时又会有令牌持续将放入桶中，以应对一定的突发流量，比如桶的上限是1000，每秒持续放入1000个令牌，当前1s有800个请求发生并消耗令牌，由于每秒会放入1000个令牌，后1s就有1200个令牌可以被消耗，因此下一秒可以应对1200个请求。\n漏桶和令牌桶的区别：漏桶是限制流出速度，令牌桶是限制流入速度。\n算法Demo：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54  type tokenLimiter struct { lock *sync.Mutex lastReqAt int64 // 单位：秒 \tavailableToken int64 // 可用的令牌数量 \tmaxTokenLimit int64 // 最大令牌数量 \trate int64 // 每时间单位内token的加入速率 } func NewTokenLimiter(rate int64, maxTokenLimit int64) *tokenLimiter { return \u0026amp;tokenLimiter{ lock: new(sync.Mutex), lastReqAt: time.Now().Unix(), maxTokenLimit: maxTokenLimit, availableToken: maxTokenLimit, rate: rate, } } func (t *tokenLimiter) IsPass() bool { t.lock.Lock() defer t.lock.Unlock() now := time.Now().Unix() // 计算时间差内可通过的计数量 \tdiffInterval := now - t.lastReqAt diffCount := diffInterval * t.rate // 把可通过的量加入令牌桶里 \tt.availableToken += diffCount if t.availableToken \u0026gt; t.maxTokenLimit { t.availableToken = t.maxTokenLimit } t.lastReqAt = now // 判断是否有令牌可取 \tif t.availableToken \u0026gt; 0 { t.availableToken-- return true } return false } // 测试 func main() { // 单位时间内漏出速度是2，桶的容量是5 \tf1 := NewTokenLimiter(2, 5) for i := 0; i \u0026lt; 20; i++ { go func(index int) { if f1.IsPass() { fmt.Println(fmt.Sprintf(\u0026#34;pass: %d\u0026#34;, index)) } else { fmt.Println(fmt.Sprintf(\u0026#34;no pass: %d\u0026#34;, index)) } }(i) } time.Sleep(100 * time.Second) }   容错 一旦发现上游服务调用失败，为了进一步减少错误的影响，可以设置容错策略\n FailFast 快速失败：当消费者调用远程服务失败时，立即报错，消费者只发起一次调用请求。 FailOver 失败自动切换：当消费者调用远程服务失败时，重新尝试调用服务，重试的次数一般需要指定，防止无限次重试。 FailSafe 失败安全：当消费者调用远程服务失败时，直接忽略，请求正常返回报成功。一般用于可有可无的服务调用。 FailBack 失败自动恢复：当消费者调用远程服务失败时，定时重发请求。一般用于消息通知。 Forking 并行调用：消费者同时调用多个远程服务，任一成功响应则返回。  熔断 作用：防止下游服务不断地尝试可能超时和失败的服务，能达到应用程序执行而不必等待上游服务修正错误；下游服务可以自我诊断上游服务的错误是否已修正，如果没有，则不放量，如果有，则会慢慢增加请求，再次尝试调用。\n上游服务是指那些不依赖于任何其他服务的服务，而下游服务依赖于上游服务的服务。\n熔断一般分为三种状态：\n Closed关闭：服务正常时，熔断处于关闭状态 Open开启：当我们设定10s的滑动窗口内错误率达90%，则从Closed变为Open状态 HalfOpen半开启：再经过10s的窗口期，此时从Open状态转为HalfOpen状态，按照 0.5 * (Now() - Start()) / Duration 的公式放量，直到成功率变为90%，转为Closed状态，否则转为Open状态。  滑动窗口的时间不能设置太长，否则熔断恢复时间也会变长；错误统计只统计系统异常不通知业务异常。\n降级 作用：解决资源不足喝访问量增加的矛盾。在有限资源下，放弃部分无关紧要的服务，保证主业务流程能平稳运行。\n降级一般可以是将部分功能关闭，简化，或者将强一致性变成最终一致性。\n负载均衡 常用算法  随机轮询：在可用的服务中，随机选择一个进行调用 顺序轮询：按请求的顺序分配给各个服务器，适用于各台服务器性能相同 加权轮询：给各个服务器附上权重值，按权重的高低分配请求，适用于各台服务器性能不同，性能高的服务器权重也高 加权随机轮询：随机 + 二分查找的方式负载均衡，时间复杂度为O(logn) 最少链接：将请求发送给当前最少连接数的服务器上 加权最少链接：在最少连接的基础上，根据服务器的性能为每台服务器分配权重，再根据权重计算出每台服务器能处理的连接数 IP地址哈希：哈希均匀分布 二次随机选择轮询：适合后端节点权重一致的情况，通过两次随机算法，获取到两个节点，对比节点CPU等信息，选择最优节点；（比较流行） 会话保持：根据客户端IP或cookie进行会话保持，同一个客户端每次选取后端节点的IP保持一致，适用于节点保持登录验证会话的场景，比较少用。  探活 负载均衡器上面一般会挂一个服务的多个复制集，进行流量的负载均衡，因此需要检查这多个复制集的健康情况，保证流量能正确路由到可用节点上。\n主动健康检查 设定一定时间间隔内对各个复制集执行ping操作，一般在获取节点数量过少的场景下才触发，避免长时间频繁的ping操作增加节点负担。\n比如通过当前节点数与15分钟前的节点数的比较，当小于80%时触发主动健康检查。\n被动健康检查 通过检查节点真实流量的响应结果，判断节点是否正常\n服务注册与发现 调用模式   调用方发现模式\n由一个服务发现系统和其提供得SDK组成，SDK由调用方使用，同时SDK也可以提供负载均衡和故障转移等功能；代表例子：Eureka\n各个调用方在启动时会向服务发现系统注册其实例信息，比如ip、port、serviceName等，待到调用方需要调用其他服务时，便根据被调用方服务的名称去服务发现服务查询对应的ip和port，发起请求。\n这种方式有个缺点，就是调用方可能有多个实例，而SDK只能针对一个调用方做负载均衡，多实例下仍然有可能导致请求负载不均衡；可能还需要准备多种语言的SDK。\n  服务端发现模式\n由一个专门的load balancer服务与服务发现系统配合，该load balancer服务会实时订阅服务发现系统中各个节点的信息，起到一个反向代理的作用，将收到的请求分发到对应的服务。代表例子：kube-proxy，在Kubernetes中，各个节点上会运行一个kube-proxy，kube-proxy会实时watch Service和Endpoint对象，当其配置发生变化后，会在各自的Node节点设置相关的iptables或IPVS规则，便于Pod内的服务通过service的clusterIP，经过iptables或IPCS设置的规则进行路由和转发。\n这种方式的好处是调用方无需感知服务发现系统，所有服务发现相关的功能都被隔离在load balancer和服务发现系统之间，但是会多一层转发，也就多一次延迟，多一次发生故障的机会。\n  基本功能   提供服务地址与域名的映射注册、查找和更新\n订阅机制有三种：\n 服务发现系统push推送，可以基于socket长连接推送，比如Zookeeper；基于HTTP连接的Long Polling，将各个服务的注册信息推送给各个节点，但是这种长连接的方式会存在消息丢失问题。 调用方SDK定时轮询，向服务发现系统拉取各个节点的注册信息，但可能存在延迟问题 以上两种方式相结合，比如Consul，调用方和服务发现系统间会建立一个最长30s的HTTP长连接，如果发生变更，调用方就会立即收到推送，如果超过30s，调用方会立即建立新连接，开始新一轮订阅。    提供多种负载均衡方案\n  健康检查，比如心跳检测\n  服务主动探活：服务注册到注册中心后，定时发送续租请求到注册中心，表明自己存活\n优点：该方案可最大程度避免IP重用导致节点在旧服务依然存活的问题(比如k8s环境)\n缺点：造成注册中心写操作变多，特别是在强一致性的注册中心上，频繁的节点变更会导致产生大量的通知事件，在同步到多个注册中心复制集时性能不佳；另外，仍然可能发生服务虽无法对外提供服务了，但仍然可以发送续租请求；面对不同的客户端需要提供不同语言的SDK\n  注册中心主动探活：服务提供健康检查接口，比如/ping，注册中心定时访问验证节点存活；k8s的Pod探针机制\n优点：一定程度上解决服务主动探活不能说明服务健康的问题\n缺点：IP重用问题，比如A、B两个服务都有相同的端口和/ping接口做健康检查，但是当发生服务替换时，无法区分是哪个服务，除非加上名称检查\n  服务外挂一个负载均衡器实现服务探活、与注册中心的通信\n优点：注册中心和服务均不需要主动探活，均交由负载均衡器实现\n缺点：需要外挂的负载均衡器，增加成本\n    注册中心需要保证CP或者AP，一般来说，注册中心对一致性C的要求不是很高，因为节点的注册和反注册后通知到客户端也需要时间；对可用性的优先级较高。\n  将上述功能包装成SDK，简化使用\n  可能遇到的问题 注册中心\n  注册中心挂掉时，此时可以持久化之前注册的Provider节点信息，并在重启后进入保护模式一段时间，在此期间先不剔除不健康的Provider节点(因为宕机期间Provider心跳无法上报)，否则可能导致在一个TTL内大量的Provider节点失效；\n  需要网络闪断保护，当监测到大面积Provider心跳没有上报，则自动进入保护模式，该模式下不会剔除因心跳上报失败的Provider；\n  注册中心故障时，服务注册功能失效，服务也无法执行扩容操作，可以在服务内缓存服务注册表，保证服务可通信；\n  调用方\n 当调用方访问到某个服务发现系统不可用时，可以立即切换到下一个节点尝试； 调用方优先使用缓存的服务注册表，当接收到的服务注册表的节点过少时，放弃使用； 如果调用方重启了，内存中的数据不存在，可以走本地配置降级； 通过在load balancer中加入被动健康检查和主动健康检查来剔除服务注册表中失效的节点，保证服务注册表的可用性   参考Service Mesh的Envoy设计，不完全信任注册中心推送过来的服务注册表；     发现状态 健康检查成功 健康检查失败     发现 路由 不路由   未发现 路由 不路由、剔除失败节点    面对节点和服务频繁变更，导致广播风暴的场景，可以通过合并设定时间内产生的消息后再进行推送，目的是为了减少广播次数，但是这样会影响消息的时效性，要注意设定的时间不宜过长。  常见的注册中心区别    特征 Nocas Eureka Zookeeper Consul ETCD     一致性协议 AP或CP AP CP CP CP   健康检查 TCP、HTTP、MySQL、Client Beat TTL Keep Alive TCP、HTTP、gRPC、Cmd TTL   网络异常保护 支持 支持 不支持 支持 不支持   雪崩保护 支持 支持 不支持 不支持 不支持   自动注销实例 支持 支持 支持 不支持 支持   访问协议 HTTP、DNS HTTP TCP HTTP、DNS HTTP   跨注册中心同步 支持 不支持 不支持 支持 不支持   K8s集成 支持 不支持 不支持 支持 支持   语言实现 Java Java Java GO GO    这里再说下K8s提供的默认服务发现（1.12后默认使用coreDNS），通过为Pod挂一个Service 或者 Pod 定义了hostname + subdomain，k8s也会为其生成Pod的 DNS A记录，然后 k8s 会把 Service 或 Pod 产生的DNS记录写入 CoreDNS 的 cache 或者 ETCD 中，同时在Pod的/etc/resolv.conf文件中添加CoreDNS服务的访问配置，Pod即可通过名称进行访问（同一命名空间下可以直接使用Pod名称进行访问，不同命名空间需要 Pod名称.命名空间名称 访问，或者直接使用Service的DNS名称访问）。\nCoreDNS会监听集群内所有Service API，当服务不可用时移除记录，在新服务创建时插入新记录，这些记录会存储在CoreDNS的cache或者ETCD中。\n参考：https://github.com/kubernetes/dns/blob/master/docs/specification.md\n比如有如下 /etc/resolv.conf文件\n1 2 3 4  # /etc/resolv.conf nameserver 10.100.0.10 # coreDNS的IP search cafe.svc.cluster.local svc.cluster.local cluster.local us-west-2.compute.internal options ndots:5   其含义是：DNS 服务器为 10.100.0.10，当查询关键词中 . 的数量少于 5 个，则根据 search 中配置的域名进行查询，当查询都没有返回正确响应时再尝试直接查询关键词本身。\n比如执行 host -v cn.bing.com 时会看到下面即此查询\n1 2 3 4 5 6  Trying \u0026#34;cn.bing.com.cafe.svc.cluster.local\u0026#34; Trying \u0026#34;cn.bing.com.svc.cluster.local\u0026#34; Trying \u0026#34;cn.bing.com.cluster.local\u0026#34; Trying \u0026#34;cn.bing.com.us-west-2.compute.internal\u0026#34; Trying \u0026#34;cn.bing.com\u0026#34; ...    ","date":"2021-10-20T00:00:00Z","permalink":"http://nixum.cc/p/%E5%BE%AE%E6%9C%8D%E5%8A%A1/","title":"微服务"},{"content":"[TOC]\n内存模型 这里的内存模型不是指内存分配、整理回收的规范，而是在并发环境下多goroutine读取共享变量时变量的可见性条件。\n由于不同的架构和不同的编译器优化，会发生指令重排，导致程序运行时不一定会按照代码的顺序执行，因此两个goroutine在处理共享变量时，能够看到其他goroutine对这个变量进行的写结果。\nhappens-before：程序的执行顺序和代码的顺序一样，就算真的发生了重排，从行为上也能保证和代码的指定顺序一样。\nGo不像Java有volatile关键字实现CPU屏障来保证指令不重排，而是使用不同架构的内存屏障指令来实现同一的并发原语。\nGo只保证goroutine内部重排对读写顺序没有影响，如果存在共享变量的访问，则影响另一个goroutine。因此当有多个goroutine对共享变量的操作时，需要保证对该共享变量操作的happens-before顺序。\n证heppen before的手段   init函数：同一个包下可以有多个init函数，多个签名相同的init函数；main函数一定在导入的包的init函数执行之后执行；当有多个init函数时，从main文件出发，递归找到对应的包 - 包内文件名顺序 - 一个文件内init函数顺序执行init函数。\n  全局变量：包级别的变量在同一个文件中是按照声明顺序逐个初始化的；当该变量在初始化时依赖其它的变量时，则会先初始化该依赖的变量。同一个包下的多个文件，会按照文件名的排列顺序进行初始化。\ninit函数也是如此，当init函数引用了全局变量a，运行main函数时，肯定是先初始化a，再执行init函数。\n当init函数和全局变量无引用关系时，先初始化全局变量，再执行init函数\n  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37  var ( a = c + b // == 9  b = f() // == 4  c = f() // == 5  d = 3 // 全部初始化完成后 == 5 ) func f() int { d++ return d } --- func init() { a += 1 fmt.Println(a) fmt.Println(4) } var a = getA() func getA() int { fmt.Println(2) return 2 } // 运行后，输出2，3，4 --- func init() { fmt.Println(4) } var a = getA() func getA() int { fmt.Println(2) return 2 } // 运行后，输出2，4    goroutine：启动goroutine的go语句执行，一定happens before此goroutine内的代码  1 2 3 4 5 6 7 8 9  var a string func f() { print(a) } func hello() { a = \u0026#34;hello\u0026#34; go f() } 执行hello方法，必定打印出hello    channel：  send操作必定heppen before于receive操作； close一个channel的操作，必定happen before从关闭的channel中读取一个零值；   此外还有Mutex / RWMutex、WaitGroup、Once、atomic  Channel Channel的设计基于CSP模型。\nCSP模型（Communicating Sequential Process，通信顺序进程），允许使用进程组来描述系统，独立运行，并且只通过消息传递的方式通信。\n本质上就是，在使用协程执行函数时，不通过内存共享(会用到锁)的方式通信，而是通过Channel通信传递数据。\n数据结构 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  type hchan struct { qcount uint // 已经接收但还没被取走的元素个数，即channel中的循环数组的元素个数 \tdataqsiz uint // channel中的循环数组的长度 \tbuf unsafe.Pointer // channel中缓冲区数据指针，buf是一个循环数组，buf的总大小是elemsize的整数倍 \telemsize uint16 // 当前channel能够收发的元素大小 \tclosed uint32 elemtype *_type // 当前channel能够收发的元素类型 \tsendx uint // 指向底层循环数组buf，表示当前可发送的元素位置的索引值，当sendx=dataqsiz时，会回到buf数组的起点，一旦接收新数据，指针就会加上elemsize，移向下个位置 \trecvx uint // 指向底层循环数组buf，表示当前可接收的元素位置的索引值 \trecvq waitq // 等待队列，存储当前channel因缓冲区空间不足而接收阻塞的goroutine列表，双向链表 \tsendq waitq // 等待队列，存储当前channel因缓冲区空间不足而发送阻塞的goroutine列表，双向链表  lock mutex // 互斥锁，保证每个读channel或写channel的操作都是原子的 } type waitq struct { first *sudog last *sudog }   基本   chan是引用类型，使用make关键字创建，未初始化时的零值是nil，如\nch := make(chan, string, 10)，创建一个能处理string的缓冲区大小为10的channel，效果相当于异步队列，除非缓冲区用完，否则不会阻塞；\nch := make(chan, string)，则创建了一个不存在缓冲区的channel，效果相当于同步阻塞队列，即如果连续发送两次数据，第一次如果没有被接收的话，第二次就会被阻塞。\n  channel作为通道，负责在多个goroutine间传递数据，解决多线程下共享数据竞争问题。\n  当 chan是 nil时，对chan的发送和接收的调用者总是阻塞的\n  带有 \u0026lt;- 的chan是有方向的，不带 \u0026lt;- 的chan是双向的，比如\n  1 2 3  chan string // 双向chan，可以发送和接收string  chan\u0026lt;- struct{} // 只能发送struct到chan中  \u0026lt;-chan int // 只能从chan中接收int    chan可以是任何类型的，比如可以是 chan\u0026lt;- 类型，\u0026lt;-总是尽量和左边的chan结合，比如  1 2 3 4  chan\u0026lt;- chan int // 等价于 chan\u0026lt;- (chan int) chan\u0026lt;- \u0026lt;-chan int // 等价于 chan\u0026lt;- (\u0026lt;-chan int) \u0026lt;-chan \u0026lt;-chan int // 等价于 \u0026lt;-chan (\u0026lt;-chan int) chan (\u0026lt;-chan int) // 等价于 chan (\u0026lt;-chan int)    接收数据时可以有两个返回值，第一个是返回的元素，第二个是bool类型，表示是否成功地从chan中读取到一个值。如果是false，说明chan以及被close并且chan中没有缓存的数据，此时第一个元素是零值。所以，如果接收时第一个元素是零值，可能是sender真的发送了零值，也可能是closed并且没有元素导致的。 双向chan可以赋值给单向chan，但反过来不可以  初始化 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  func makechan(t *chantype, size int) *hchan { ... elem := t.elem // 略去检查代码  mem, overflow := math.MulUintptr(elem.size, uintptr(size)) ... var c *hchan switch { case mem == 0: // chan的size或者元素的size是0，不必创建buf  c = (*hchan)(mallocgc(hchanSize, nil, true)) c.buf = c.raceaddr() case elem.ptrdata == 0: // 元素不是指针，分配一块连续的内存给hchan数据结构和buf  c = (*hchan)(mallocgc(hchanSize+mem, nil, true)) // hchan数据结构后面紧接着就是buf  c.buf = add(unsafe.Pointer(c), hchanSize) default: // 元素包含指针，那么单独分配buf  c = new(hchan) c.buf = mallocgc(mem, elem, true) } // 元素大小、类型、容量都记录下来  c.elemsize = uint16(elem.size) c.elemtype = elem c.dataqsiz = uint(size) lockInit(\u0026amp;c.lock, lockRankHchan) return c }   发送数据 使用ch \u0026lt;- \u0026quot;test\u0026quot;发送数据，最终会调用chansend函数发送数据，该函数设置了阻塞参数为true。\n 如果chan是nil，则把发送者的goroutine park（阻塞休眠），此时发送者将被永久阻塞。 如果chan没有被close，但是chan满了，则直接返回false，但是由于阻塞参数为true，这部分不会被执行。 如果chan被close了，再往里发数据会触发panic。 当存在等待的接收者时，通过send函数，从接收队列recvq中取出最先进入等待的goroutine，直接发送数据，不需要先放到buf中。 当没有接收者，且缓冲区存在空余空间时，会使用chanbuf计算出下一个可以存储数据的位置，将要发送的数据拷贝到缓冲区并增加sendx索引和qcount计数器，将发送的数据写入channel缓冲区，写入后就返回成功。 当不存在缓冲区或者缓冲区已满，会先调用getg函数获取正在发送者的goroutine，执行acquireSudog函数创建sudog对象，设置此次阻塞发送的相关信息（如发送的channel、是否在select控制结构中和待发送数据的内存地址、发送数据的goroutine），将该sudog对象加入sendq队列，调用goparkunlock函数让当前发送者的goroutine进入等待，表示当前goroutine正在等待其他goroutine从channel中接收数据，等待调度器唤醒。调度器唤醒后，将一些属性值设置为零，并释放sudog对象，表示向channel发送数据结束。  接收数据 使用 str \u0026lt;- ch 或 str, ok \u0026lt;- ch ok用于判断ch是否关闭，如果没有ok，可能会无法分配str接收到的零值是发送者发的还是ch关闭接收数据，会转化为调用chanrecv1和chanrecv2函数，但最终会调用chanrecv函数接收数据。chanrecv1和chanrecv2函数都是设置阻塞参数为true。\n  如果chan是nil，则把接收者的goroutine park（阻塞休眠），接收者被永久阻塞。\n  如果当前channel已经被关闭且缓冲区不存在任何数据，此时会清除ep指针中的数据并立即返回。\n  如果chan已经被close，且队列中没有缓存元素，返回selected为true，received为false。\n  当channel的sendq队列存在等待状态的goroutine时，如果是unbuffer的chan，直接使用recv函数直接从阻塞的发送者中获取数据；如果是有buffer的chan，则从sendq队列的头中读取一个值，并把这个发送者的值加入队列的尾部，即优先获取发送者的数据。\n  当channel的sendq队列没有等待状态的goroutine，且缓冲区存在数据时，从channel的缓冲区中的recvx的索引位置接收数据，如果接收数据的内存地址不为空，会直接将缓冲区里的数据拷贝到内存中，清除队列中的数据，递增recvx，递减qcount，完成数据接收。当发送recvx超过channel的buf时，会将其归零。\n这个和chansend共用一把锁，所以不会有并发问题。\n  当channel的sendq队列没有等待状态的goroutine，且缓冲区不存在数据时，创建sudog对象，加入recvq队列，当前goroutine进入阻塞状态，等待其他goroutine向channel发送数据。\n  关闭  如果 chan 为 nil，close 会 panic 如果 chan 已经 closed，再次 close 也会 panic。 否则的话，如果 chan 不为 nil，chan 也没有 closed，就把等待队列中的 sender（writer）和 receiver（reader）从队列中全部移除并唤醒。  应用场景  实现生产者 - 消费组模型，数据传递，比如worker池的实现 信号通知：利用 如果chan为空，那receiver接收数据的时候就会阻塞等待，直到chan被关闭或有新数据进来 的特点，将一个协程将信号(closing、closed、data ready等)传递给另一个或者另一组协程，比如 wait/notify的模式。 任务编排：让一组协程按照一定的顺序并发或串行执行，比如实现waitGroup的功能 实现互斥锁的机制，比如，容量为 1 的chan，放入chan的元素代表锁，谁先取得这个元素，就代表谁先获取了锁   共享资源的并发访问使用传统并发原语；\n复杂的任务编排和消息传递使用 Channel；\n消息通知机制使用 Channel，除非只想 signal 一个 goroutine，才使用 Cond；\n简单等待所有任务的完成用 WaitGroup，也有 Channel 的推崇者用 Channel，都可以；\n需要和 Select 语句结合，使用 Channel；需要和超时配合时，使用 Channel 和 Context。\n 注意点：使用chan要注意panic和goroutine泄露，另外，只要一个 chan 还有未读的数据，即使把它 close 掉，你还是可以继续把这些未读的数据消费完，之后才是读取零值数据。\n在使用chan和select配合时要注意会出现goroutine泄漏的情况：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  func process(timeout time.Duration) bool { ch := make(chan bool) go func() { // 模拟处理耗时的业务  time.Sleep((timeout + time.Second)) ch \u0026lt;- true // block  fmt.Println(\u0026#34;exit goroutine\u0026#34;) }() // 如果上面的协程任务处理的时间过长，触发下面select的超时机制，此时process函数返回，之后当上面的协程任务执行完之后，由于process已经执行完，下面result接收chan的值被回收，导致没有接收者，导致上面的协程任务一直卡在 ch \u0026lt;- true，进而导致goroutine泄漏。解决方案就是使用容量为1的ch即可。  select { case result := \u0026lt;-ch: return result case \u0026lt;-time.After(timeout): return false } }       nil empty full not full \u0026amp; empty closed     receive block block read value read value 返回未读的元素，读完后返回零值   send block write value block writed value panic   close panic closed，没有未读元素 closed，保留未读元素 closed，保留未读元素 panic    并发包 Mutex - 互斥锁 数据结构 1 2 3 4 5 6 7 8 9 10 11  type Mutex struct { state int32 // 分成三部分，最小一位表示锁是否被持有，第二位表示是否有唤醒的goroutine，第三位表示是否处于饥饿状态，剩余的位数表示等待锁的goroutine的数量，最大数量为2^(32-3)-1个，以goroutine初始空间为2k，则达到最大数量时需要消耗1TB内存 \tsema uint32 // 信号量变量，用来控制等待goroutine的阻塞休眠和唤醒 } const ( mutexLocked = 1 \u0026lt;\u0026lt; iota // 持有锁的标记 \tmutexWoken // 唤醒标记 \tmutexStarving // 饥饿标记 \tmutexWaiterShift = iota // 阻塞等待的waiter数量  starvationThresholdNs = 1e6 }   基本   只有Lock和Unlock两个方法，用于锁定临界区\n  Mutex的零值是没有goroutine等待的未加锁状态，不会因为没有初始化而出现空指针或者无法获取到锁的情况，so无需额外的初始化，直接声明变量即可使用var lock sync.Mutex，或者是在结构体里的属性，均无需初始化\n  如果Mutex已被一个goroutine获取了锁，其他等待的goroutine们会一直等待，组成等待的队列，当该goroutine释放锁后，等待的goroutine是以先进先出的队列排队获取锁；如果此时有新的goroutine也在获取锁，会参与到获取锁的竞争中，如果等待队列中的goroutine等待超过1ms，则会加入队头优先获取锁，新来的goroutine加入到队尾，以此解决等待的goroutine的饥饿问题。\n  Unlock方法可以被任意goroutine调用，释放锁，即使它本身没有持有这个锁，so写的时候要牢记，谁申请锁，就该谁释放锁，保证在一个方法内被调用\n  必须先使用Lock方法才能使用Unlock方法，否则会panic，重复释放锁也会panic\n  自旋的次数与自旋次数、cpu核数，p的数量有关\n  注意Mutex在使用时不能被复制，比如方法传参的没有使用指针，导致执行方法的参数时被复制\n  Mutex是不可重入锁，获取锁的goroutine无法重复获取锁，因为Mutex本身不记录哪个goroutine拥有这把锁，因此如果要实现可重入锁，则需要对Mutex进行包装，实现Locker接口，同时记录获取锁的goroutine的id和重入次数\n获取goroutine id的方法：\n​\t1.使用runtime.Stack()方法获取栈帧里的goroutine id\n​\t2.获取运行时的g指针，反解出g的TLS结构，获取存在TLS结构中的goroutine id\n​\t3.给获取锁的goroutine设置token，进行标记\n  Lock方法  调用Lock的goroutine通过CAS的方式获取锁，如果获取到了直接返回，否则进入lockSlow方法 在lockSlow方法内，意味着锁已经被持有，当前调用Lock方法的goroutine正在等待，且非饥饿状态，其首先会自旋，尝试获取锁，而无需休眠，主要是在当临界区耗时很短的场景下提高性能 非饥饿状态下抢锁，先在state锁标志位+1，如果锁已经被持有或者此时是饥饿状态，state的waiter的数量+1，进入等待 如果此时是饥饿状态，并且锁还被持有，state设置为饥饿状态，清除mutexWoken标记，表示非唤醒状态 使用3中的锁位，CAS尝试加锁，如果成功，检查原来的锁是未加锁状态，并且也不是饥饿状态，则成功获取锁，返回 如果是未加锁状态，判断是否是第一次加入waiter队列（通过waitStartTime变量），如果是，则加入队尾，如果不是首次，则加入到队首(设置sema的值)，阻塞等待，直至被唤醒（因为锁被释放了） 唤醒后，如果是饥饿状态（即当前时间 - waitStartTime \u0026gt; 1ms），在state锁标志位+1，waiter数-1，获取锁，然后判断没有其他waiter或者此goroutine等待时间没有超过1ms，清除饥饿标记  Unlock方法  将state的锁位-1，如果state=0，即此时没有加锁，且没有正在等待获取锁的goroutine，则直接结束方法，如果state != 0，执行unlockSlow方法 如果Mutex处于饥饿状态，直接唤醒等待队列中的waiter 如果Mutex处于正常状态，如果没有waiter，或者已经有在处理的情况，则直接释放锁，state锁位-1，返回；否则，waiter数-1，设置唤醒标记，通过CAS解锁，唤醒在等待锁的goroutine（此时新老goroutine一起竞争锁）  基于Mutex的拓展  可重入锁 增加tryLock方法，通过返回true或false来表示获取锁成功或失败，主要用于控制获取锁失败后的行为，而不用阻塞在方法调用上 增加等待计数器，比如等待多少时间后还没获取到锁则放弃 增加可观测性指标，比如等待锁的goroutine的数量，需要使用unsafe.Pointer方法获取Mutex中的state的值，解析出正在等待的goroutine的数量 实现线程安全的队列，通过在出队和入队方法中使用Mutex保证线程安全  RWMutex - 读写锁 数据结构 1 2 3 4 5 6 7 8  type RWMutex struct { w Mutex // 互斥锁解决多个writer的竞争  writerSem uint32 // writer信号量  readerSem uint32 // reader信号量  readerCount int32 // reader的数量，可以是负数，负数表示此时有writer等待请求锁，此时会阻塞reader  readerWait int32 // writer等待完成的reader的数量 } const rwmutexMaxReaders = 1 \u0026lt;\u0026lt; 30 // 最大的reader数量   基本  主要提升Mutex在读多写少的场景下的吞吐量，读时共享锁，写时排他锁，基于Mutex实现 由5个方法构成：  Lock/Unlock：写操作时调用的方法。如果锁已经被 reader 或者 writer 持有，那么，Lock 方法会一直阻塞，直到能获取到锁；Unlock 则是配对的释放锁的方法。 RLock/RUnlock：读操作时调用的方法。如果锁已经被 writer 持有的话，RLock 方法会一直阻塞，直到能获取到锁，否则就直接返回；而 RUnlock 是 reader 释放锁的方法。 RLocker：这个方法的作用是为读操作返回一个 Locker 接口的对象。它的 Lock 方法会调用 RWMutex 的 RLock 方法，它的 Unlock 方法会调用 RWMutex 的 RUnlock 方法   同Mutex，RWMutex的零值是未加锁状态，无需显示地初始化 由于读写锁的存在，可能会有饥饿问题：比如因为读多写少，导致写锁一直加不上，因此go的RWMutex使用的是写锁优先策略，如果已经有一个writer在等待请求锁的话，会阻止新的reader请求读锁，优先保证writer。如果已经有一些reader请求了读锁，则新请求的writer会等待在其之前的reader都释放掉读锁后才请求获取写锁，等待writer解锁后，后续的reader才能继续请求锁。 同Mutex，均为不可重入，使用时应避免复制；要注意reader在加读锁后，不能加写锁，否则会形成相互依赖导致死锁；注意reader是可以重复加读锁的，重复加读锁时，外层reader必须=里层的reader释放锁后自己才能释放锁。 必须先使用RLock / Lock方法才能使用RUnlock / Unlock方法，否则会panic，重复释放锁也会panic。 可以利用RWMutex实现线程安全的map  RLock / RUnlock 方法  RLock时，对readerCount的值+1，判断是否\u0026lt; 0，如果是，说明此时有writer在竞争锁或已持有锁，则将当前goroutine加入readerSem指向的队列中，进行等待，防止写锁饥饿。 RUnlock时，对readerCount的值-1，判断是否\u0026lt;0，如果是，说明当前有writer在竞争锁，调用rUnlockSlow方法，对readerWait的值-1，判断是否=0，如果是，说明当前goroutine是最后一个要解除读锁的，此时会唤醒要请求写锁的writer。  Lock方法 RWMutex内部使用Mutex实现写锁互斥，解决多个writer间的竞争\n 调用w的Lock方法加锁，防止其他writer上锁，反转 readerCount的值，使其变成负数（readerCount - rwmutexMaxReaders + rwmutexMaxReaders）告诉reader有writer要请求锁 如果此时readerCount != 0，说明当前有reader持有读锁，需要记录需要等待完成的reader的数量，即readerWait的值（readerWaiter + 第1步算的readerCount的值），并且如果此时readerWait != 0，将当前goroutine加入writerSema指向的队列中，进行等待。直到有goroutine调用RUnlock方法且是最后一个释放锁时，才会被唤醒。  Unlock方法  反转readerCount的值（readerCount + rwmutexMaxReaders），使其变成reader的数量，唤醒这些reader 调用w的Unlock方法释放当前goroutine的锁，让其他writer可以继续竞争。  WaitGroup 数据结构 1 2 3 4 5 6 7 8  type WaitGroup struct { // 避免复制，使用vet工具在编译时检测是否被复制  noCopy noCopy // 因为64bit值的原子操作需要64bit对齐，但是32bit编译器不支持，所以数组中的元素在不同的架构中不一样  // 如果地址是64bit对齐，数组前两个元素做state，后一个元素做信号量；如果地址是32bit对齐，数组后两个元素做state，第一个元素做信号量  // 高32bit是WaitGroup的计数值，低32bit是waiter的计数,另外32bit是用作信号量  state1 [3]uint32 }   基本  state的值由32bit的值表示信号量，64bit的值表示计数和waiter的数量组成。因为原子操作只能64bit对齐，而计数值和waiter的数量是一个64bit的值，在64bit的编译器上，一次读取是64bit，刚好可以直接操作，但是如果是32bit的机器，一次只能读32bit，为了保证进行64bit对齐时一定能获取到计数值和waiter的值，在进行64bit的原子操作对齐时，第一次是对齐到了一个空32bit和第一个32bit的值，第二次对齐就能保证获取了。 同RWMutex，WaitGroup的三个方法内还很多data race检查，保证并发时候共享数据的正确性，一旦检查出有问题，会直接panic 一开始设置WaitGroup的计数值必须大于等于0，否则会过不了data race检查，直接panic Add的值必须=调用调用Done的次数，当Done的次数超过计数值，也会panic Wait方法的调用一定要晚于Add，否则会导致死锁 WaitGroup可以在计数值为0时可重复使用 noCopy是一个实现了Lock接口的结构体，且不对外暴露，其Lock方法和Unlock方法都是空实现，用于vet工具检查WaitGroup在使用过程中有没有被复制；当我们自定义的结构不想被复制使用时，也可以使用它。 使用时要避免复制  Add方法  原子的将WaitGroup的计数值加到state上，如果当前的计数值 \u0026gt; 0，或者 waiter的数量等于0，直接返回 否则，即代表当前的计数值为0，但waiter的数量不一定为0，此时state的值就是waiter的数量 将state的值设置为0，即waiter的数量设置为0，然后唤醒所有waiter  Done方法  调用Add方法，只是参数为-1，表示计数值 - 1，有一个waiter完成其任务；waiter指的是调用Wait方法的goroutine  Wait方法  循环内不断检测state的值，当其计数值为0时，说明所有任务已经完成，调用这个方法的goroutine不必继续等待，直接返回，结束该方法 否则，说明此时还有任务没完成，调用该方法的goroutine成为waiter，把waiter的数量 + 1，加入等待队列，阻塞自己  Cond = condition + Wait/Notify 数据结构 1 2 3 4 5 6  type Cond struct { noCopy noCopy // 使用vet工具在编译时检测是否被复制  checker copyChecker // 用于运行时被检测是否被复制  L Locker // 当观察或者修改等待条件的时候需要加锁  notify notifyList // 等待队列 }   基本   初始化时，要指定使用的锁，比如Mutex\n  Cond 是等待某个条件满足，这个条件的修改可以被任意多的 goroutine 更新，而且 Cond 的 Wait 不关心也不知道其他 goroutine 的数量，只关心等待条件。\n  Signal方法，类似Java的notify方法，允许调用者唤醒一个等待此Cond的goroutine，如果此时没有waiter，则无事发生；如果此时Cond的等待队列中有多个goroutine，则移除队首的goroutine并唤醒；\n使用Signal方法时不强求已调用了加锁方法\n  Broadcast方法，类似Java的notifyAll方法，允许调用者唤醒等待此Cond的所有goroutine，如果此时没有waiter，则无事发生；如果此时Cond的等待队列中有多个goroutine，则清空整个等待队列，全部唤醒；\n使用Broadcast方法时不强求已调用了加锁方法\n  Wait方法，类似Java的wait方法，把调用者的goroutine放入Cond的等待队列中并阻塞，直到被Signal或Broadcast方法唤醒\n调用Wait方法时必须已调用了加锁方法，否则会panic，因为Wait方法内是先解锁，将当前goroutine加入到等待队列，然后解锁，阻塞休眠当前goroutine，直到被唤醒，然后加锁\n调用Wait后一定要检测等待条件是否满足，还需不需要继续等待，在等待的goroutine被唤醒不等于等待条件已满足，可能只是被某个goroutine唤醒而已，被唤醒时，只是得到了一次检测机会。\n  Once 数据结构 1 2 3 4  type Once struct { done uint32 m Mutex }   基本  sync.Once只有一个Do方法，入参是一个无参数无返回值的函数，当且仅当第一次调用Do方法的时候该函数才会执行，即使之后调用了n次、入参的值不一样都不会被执行 可以将sync.Once与想要只初始化一次的对象封装成一个结构体，提供只初始化一次该值的方法，常用于初始化单例资源、并发访问只初始化一次的共享资源、需要延迟初始化的场景等 Once传入的函数参数，就算在执行时发生panic，Once也会认为已经执行过了，so如果要知道Once里传入的方法是否执行成功，模仿Do函数自己写一个返回参数的入参方法 内部的实现非常简单，就是一个flag + 一个双重校验锁  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  func (o *Once) Do(f func()) { // 判断flag是否被置为0，即函数是否还没被执行过 \tif atomic.LoadUint32(\u0026amp;o.done) == 0 { o.doSlow(f) } } func (o *Once) doSlow(f func()) { o.m.Lock() defer o.m.Unlock() if o.done == 0 { // 因为其他最外层的判断+LoadUnit32没有被锁保护，so这里得原子操作  defer atomic.StoreUint32(\u0026amp;o.done, 1) f() } }   并发安全的map   将map与RWMutex封装成一个结构体，使用读写锁封装map的各种操作即可\n  使用RWMutex封装的并发安全的map，因为锁的粒度太大，性能不会太好；通过减少锁的粒度和持有锁的时间，可以提升新能，常见的减少锁的粒度是将锁分片，将锁进行分片，分别控制map中不同的范围的key，类似JDK7中的ConcurrentHashMap的segment锁实现。\n  官方出品的sync.Map，但它只有在部分特殊的场景里才有优势，比如一个只会增长的map，一个key只会被写一次，读很多次；或者 多个goroutine为不相交的键集读、写和重写键值对\nsync.Map内部有两个map，一个只读read，一个可写dirty，对只读read的操作(读、更新、删除)不需要加锁，以此减少锁对性能的影响。\n如果read中读不到，就会加锁读取dirty里的，同时增加miss的值(miss表示读取穿透的次数)，当miss的值=dirty的长度时，就会将dirty提升为read，然后清空dirty，避免总是从dirty中加锁读取\n加锁读取dirty时，还要再检查read，确定read中真的不存在才会操作dirty\ndirty提升为read时，只需简单的赋值即可，创建新的dirty时，需要遍历把read中非expunged的值赋给dirty\n删除key时，只是对该key打上一个expunged标记，只有在将dirty提升为read时才会清理删除的数据\nsync.Map没有len方法，要获取里面有多少个key只能遍历获取\n  Pool 数据结构 ![](https://github.com/Nixum/Java-Note/raw/master/picture/go syncPool数据结构.png)\n 每次垃圾回收时，Pool会把victim中的对象移除，然后把local的数据给victim，local置为nil，如果此时有Get方法被调用，则会从victim中获取对象。通过这种方式，避免缓存元素被大量回收后再再次使用时新建很多对象 获取重用对象时，先从local中获取，获取不到再从victim中获取 poolLocalInternal用于CPU缓存对齐，避免false sharing private字段代表一个缓存元素，且只能由相应的一个P存取，因为一个P同时只能执行一个goroutine，所以不会有并发问题 shared字段可以被任意的P访问，但是只有本地的P次啊能pushHead/popHead，其他P可以popTail，相当于只有一个本地P作为生产者，多个P作为消费者，它由一个local-free的队列实现  基本  sync.Pool用于保存一组可独立访问的临时对象，它池化的对象如果没有被其他对象持有引用，可能会在未来某个时间点被回收掉 sync.Pool是并发安全的，多个gotoutine可以并发调用它存取对象； 不能复制使用 在1.13以前，保证并发安全使用了带锁的队列，1.13后，改成了lock-free的队列实现，避免锁对性能的影响 包含了三个方法：New、Get、Put；Get方法调用时，会从池中移走该元素 当Pool里没有元素可用时，Get方法会返回nil；可以向Pool中Put一个nil的值，Pool会将其忽略 当使用Pool作为buffer池时，要注意buffer如果太大，reset后它就会占很大空间，引起内存泄漏，因此在回收元素时，需要检查大小，如果太大了就直接置为null，丢弃即可  Get方法  将当前goroutine固定在P上，优先从local的private字段取出一个元素，将private置为null 如果取出的元素为null，从当前的local.shared的head中取出一个元素，如果还取不到，调用getSlow函数去其他shared中取 getSlow函数会遍历所有local，从它们的shared的head中弹出一个元素，如果还没有，则对victim中以在同样的方式(先从private里找，找不到再在shared里找)获取一遍 如果还取不到，则调用New函数生成一个，然后返回  因为当前的goroutine被固定在了P上，在查找元素时不会被其他P执行\nPut方法  如果Put进来的元素是null，直接返回 固定当前goroutine，如果本地private没有值，直接设置，否则加入到shared中  原子操作   依赖atomic包，因为没有泛型，目前该包支持int32、int64、uint32、unit64、uintptr、Pointer的原子操作，比如Add、CompareAndSwap、Swap、Load、Store等（Pointer不支持Add），对于有符号的数值来说，Add一个负数相当于减\n  对于现代多核操作系统来说，由于cache、指令重排、可见性问题，一个核对地址的值的更改，在更新到主内存中前，会先存在多级缓存中，此时，多个核看到该数据可能还没看到更新的数据，还在使用旧数据，而atomic包提供的方法会提供内存屏障的功能，保证赋值数据的完整性和可见性\n  atomic操作的对象是一个地址，不是变量值\n  用atomic实现的lock-free的队列\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72  package queue import ( \u0026#34;sync/atomic\u0026#34; \u0026#34;unsafe\u0026#34; ) // lock-free的queue type LKQueue struct { head unsafe.Pointer tail unsafe.Pointer } // 通过链表实现，这个数据结构代表链表中的节点 type node struct { value interface{} next unsafe.Pointer } func NewLKQueue() *LKQueue { n := unsafe.Pointer(\u0026amp;node{}) return \u0026amp;LKQueue{head: n, tail: n} } // 入队 func (q *LKQueue) Enqueue(v interface{}) { n := \u0026amp;node{value: v} for { tail := load(\u0026amp;q.tail) next := load(\u0026amp;tail.next) if tail == load(\u0026amp;q.tail) { // 尾还是尾  if next == nil { // 还没有新数据入队  if cas(\u0026amp;tail.next, next, n) { //增加到队尾  cas(\u0026amp;q.tail, tail, n) //入队成功，移动尾巴指针  return } } else { // 已有新数据加到队列后面，需要移动尾指针  cas(\u0026amp;q.tail, tail, next) } } } } // 出队，没有元素则返回nil func (q *LKQueue) Dequeue() interface{} { for { head := load(\u0026amp;q.head) tail := load(\u0026amp;q.tail) next := load(\u0026amp;head.next) if head == load(\u0026amp;q.head) { // head还是那个head  if head == tail { // head和tail一样  if next == nil { // 说明是空队列  return nil } // 只是尾指针还没有调整，尝试调整它指向下一个  cas(\u0026amp;q.tail, tail, next) } else { // 读取出队的数据  v := next.value // 既然要出队了，头指针移动到下一个  if cas(\u0026amp;q.head, head, next) { return v // Dequeue is done. return  } } } } } // 将unsafe.Pointer原子加载转换成node func load(p *unsafe.Pointer) (n *node) { return (*node)(atomic.LoadPointer(p)) } // 封装CAS,避免直接将*node转换成unsafe.Pointer func cas(p *unsafe.Pointer, old, new *node) (ok bool) { return atomic.CompareAndSwapPointer( p, unsafe.Pointer(old), unsafe.Pointer(new)) }   Weighted = Semaphore信号量 数据结构 1 2 3 4 5 6  type Weighted struct { size int64 // 最大资源数  cur int64 // 当前已被使用的资源  mu sync.Mutex // 互斥锁，对字段的保护  waiters list.List // 等待队列，通过channel实现通知机制 }   基本   信号量中的PV操作，P：获取资源，如果获取不到，则阻塞，加入到等待队列中；V：释放资源，从等待队列中唤醒一个元素执行P操作\n  二进位信号量，或者说只有一个计数值的信号量，其实相当于go中的Mutex互斥锁\n  初始化时，必须指定初始的信号量\n  只调用Release方法会直接panic；Release方法传入负数，会导致资源被永久持有；因此要保证请求多少资源，就释放多少资源\n  Mutex中使用的sema是一个信号量，只是其实现是在runtime中，并没有对外暴露，在扩展包中，暴露了一个信号量工具Weighted\n  Weighted分为3个方法：Acquire方法，相当于P操作，第一个参数是context，可以使用context实现timeout或cancel机制，终止goroutine；正常获取到资源时，返回null，否则返回ctx.Err，信号量计数值不变。\nRelease方法，相当于V操作，可以释放n个资源，返回给信号量；\nTryAcquire方法，尝试获取n个资源，但不会阻塞，成功时返回true，否则一个也不获取，返回false\n  信号量的实现也可通过buffer为n的channel实现，只是一次只能请求一个资源，而Weighted一次可以请求多个\n  Acquire方法  加锁，判断可用资源 \u0026gt;= 入参所需的资源数，且没有waiter，说明资源足够，直接cur+上所需资源数，解锁返回 如果所需资源数\u0026gt;最大资源数，说明是不可能任务，解锁，依赖ctx的Done方法返回，否则一直等待 如果资源数不够，将调用者加入等待队列，并创建一个read chan，用于通知唤醒，解锁 等待唤醒有两种条件，一种是通过read chan唤醒，另一种是通过ctx.Done唤醒  Release方法  加锁，当前已使用资源数cur - 入参要释放的资源数，唤醒等待队列中的元素，解锁 唤醒等待队列的元素时，会遍历waiters队列，按照先入先出的方式唤醒调用者，前提是释放的资源数要够队首的元素资源的要求，比如释放100个资源，但是队首元素要求101个资源，那队列中的所有等待者都将继续等待，直到队首元素出队，这样做是为了避免饥饿  SingleFlight 结构体 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  // 代表一个正在处理的请求，或者已经处理完的请求 type call struct { wg sync.WaitGroup // 这个字段代表处理完的值，在waitgroup完成之前只会写一次, waitgroup完成之后就读取这个值  val interface{} err error forgotten bool // 指示当call在处理时是否要忘掉这个key  dups int // 相同的key的请求数  chans []chan\u0026lt;- Result } // group代表一个singleflight对象 type Group struct { mu sync.Mutex // protects m  m map[string]*call // lazily initialized }   基本   SingleFlight可以合并多个请求为一个请求，再将该请求的结果返回给多个请求，从而达到合并并发请求的目的，减少并发调用的数量。比如有多个相同的读请求查库，那就可以合并成一个请求查库，再把结果响应回这多个请求中；或者是解决缓存击穿问题，降低对下游服务的并发压力\n  底层由Mutex和Map实现，Mutex保证并发读写保护，Map保存同一个key正在处理的请求\n  包含3个方法，Do方法：提供一个key和一个函数，对于同一个key，在同一时间只有一个函数在执行，之后同一个key并发的请求会等待，等到第一个执行的结果就是该key的所有结果，调用完成后，会移除这个key。返回值shared表示结果是否来自多个相同请求。\nDoChan方法：类似Do方法，只是返回是一个chan，待入参函数执行完，产生结果后就能在chan中接收这个结果\nForget方法：告诉Group忽略这个key，之后这个key的请求会执行入参函数，而不是等待前一个未完成的入参函数的结果\n  CyclicBarrier - 循环栅栏 数据结构 1 2 3 4 5 6 7 8 9 10 11 12  type CyclicBarrier interface { // 等待所有的参与者到达，如果被ctx.Done()中断，会返回ErrBrokenBarrier  Await(ctx context.Context) error // 重置循环栅栏到初始化状态。如果当前有等待者，那么它们会返回ErrBrokenBarrier  Reset() // 返回当前等待者的数量  GetNumberWaiting() int // 参与者的数量  GetParties() int // 循环栅栏是否处于中断状态  IsBroken() bool }   基本  类似Java的CyclicBarrier，允许一组goroutine相互等待，到达一个共同的执行点再继续往下执行；同时也可被重复使用。 CyclicBarrier是一个接口，然后有两个初始化的方法，New方法，指定循环栅栏的参与者数量即可初始化；NewWithAction方法，除了指定参与者数量，第二个参数是一个函数，表示在最后一个参与者到达之后，但其他参与者还没放行之前，会调用该函数 每个参与的goroutine都会调用Await方法进行阻塞，当调用Await方法的goroutine的个数=参与者的数量时，Await方法造成的阻塞才会解除  ErrGroup   类似WaitGroup，只是功能更丰富，多了与Context集成，可以通过Context监控是否发生cancel；error可以向上传播，把子任务的错误传递给Wait的调用者\n  ErrGroup用于并发处理子任务，将一个大任务拆成几个小任务，通过Go方法并发执行。\n  ErrGroup有三个方法：withContext、Go、Wait，用法与WaitGroup相似，只是不需要设置计数值，且可以通过Wait方法获取子任务返回的错误，但它只会返回第一个出现的错误，如果所有子任务都执行成功，返回null；当发生错误时不会立即返回，而是等到其他任务完成了才会返回。\n  Go方法会创建一个goroutine来执行子任务，如果并发的量太大，会导致创建大量的goroutine，带来goroutine的调度和GC压力，占用更多资源，解决方案可以是使用worker pool或者信号量来控制goroutine的数量或保持重用\n  子任务如果发生panic会导致程序崩溃\n  检测工具  go race detector：主要用于检测多个goroutine对共享变量的访问是否存在协程安全问题。编译器通过探测所有内存的访问，加入代码监视对内存地址的访问，在程序运行时，监控共享变量的非同步访问，出现race时，打印告警信息。比如在运行时加入race参数go run -race main.go，当执行到一些并发操作时，才会检测运行时是否有并发问题 命令go vet xxx.go可以进行死锁检测  ","date":"2021-03-22T00:00:00Z","permalink":"http://nixum.cc/p/go%E5%B9%B6%E5%8F%91/","title":"Go并发"},{"content":"[TOC]\n以下基于go.1.14\n函数内联优化 函数内联优化：在A函数中调用了B函数，内联后，B函数的代码直接在A函数内原地展开，代替这个函数实现，当有多次调用时，就会多次展开\ngo在编译时会自动判断函数是否可以内联，当函数内包含以下内容时不会被内联：闭包调用，select，for，defer，go关键字创建的协程等。\n内联的好处：因为函数调用被内联了，可以减少栈帧的创建，减少读写寄存器的读取，减少参数和函数的拷贝，提升性能\n缺点：堆栈panic显示的行数可能不准确、增加编译出来的包的大小\n编译时使用go build -gcflags=\u0026quot;-m -m\u0026quot; main.go可以知道编译器的内联优化策略，\ngo编译时默认会使用内联优化，使用go build --gcflags=\u0026quot;-l\u0026quot; main.go可禁掉全局内联，如果传递两个或以上-l，则会打开内联\n数组  声明时必须指定固定长度，因为编译时需要知道数组长度以便分配内存，如var arr1 [5]int，或者var arr2 = [5]int{1,2,3}, 其余数字为0 数组长度最大是2Gb 当数组类型是整形时，所有元素都会被自动初始化为0，即声明完数组，数组会被设置类型的默认值 可以使用new()来创建，如var arr3 = new([3]int)，arr3的类型是*[3]int，arr1、arr2的类型是[5]int 函数的参数可以是[5]int, 表明入参是数组，如果是[]int，表明入参是slice。类型[3]int和[5]int是两种不同的类型。 数组是值类型，赋值和传参会进行拷贝，函数内部的修改不会影响原始数组。 如果数组中的元素个数小于或等于4个，所有变量会直接在栈上初始化；当数组元素大于4个，变量就会在静态存储区初始化然后拷贝到栈上。  切片Slice 数据结构 slice本质是一个结构体，所以它是值类型是不难理解的，它仅仅只是对数组的一种包装，且该结构体不包含任何函数，任何对slice的处理都是go的内置函数来处理的。\n1 2 3 4 5  type Slice struct { ptr unsafe.Pointer // 指向数组的指针 \tlen int // 切片长度 \tcap int // 切片容量 }   基本   创建时无需指定长度，如 slice1 := []int{1,2,3}, 此时长度和容量均为3\n  从数组上截取arr1 := [5]int; var slice2 []int = arr1[1:3], 此时长度2，容量5，且对slice2的修改会影响arr1。\n  可以使用make([]type, len, cap)来创建，len必填，cap非必填，如果cap不填，初始cap=len。如slice4 := make(int[], 5, 10)，长度5，容量10。\n  可以使用new来创建，比如 new([100]int)[0:50] 效果等同于 make([]int, 50, 100)，或者 slice := *new([]int) 为空切片\n  空切片：slice := make([]int, 0) 或 slice := []int{}，nil切片：var slice []int 或 slice := *new([]int)；两者的区别在于，空切片会指向一个内存地址，但它没有分配任何的内存空间；nil切片是直接指向nil。\n打印时，两者的结果均为[], len=0， cap=0，但nil切片与nil比较的结果为true，空切片与nil的比较结果为false。\n  切片是对数组的一个连续片段的引用，对于切片底层数组是引用类型，作为函数参数时，虽然是传切片的值，但是底层数组传递指针，函数内部的修改会影响原始数组\n  一个数组可以创建多个slice，一个slice也可以创建多个slice，但是新老slice会共用底层数组，新老slice的修改都会互相影响。但是如果新slice经过append，使得slice底层数组扩容了，此时slice引用了新的数组，此时新老slice就不会互相影响了。\n  使用new()和make()的区别\n 看起来二者没有什么区别，都在堆上分配内存，但是它们的行为不同，适用于不同的类型。\nnew (T) 为每个新的类型 T 分配一片内存，初始化为 0 并且返回类型为 *T 的内存地址：这种函数 返回一个指向类型为 T，值为 0 的地址的指针，它适用于值类型如数组和结构体；它相当于 \u0026amp;T{}。 make(T) 返回一个类型为 T 的初始值，它只适用于 3 种内建的引用类型：切片、map 和 channel。\n  range遍历的注意点：使用range遍历时，底层的slice会发生一次拷贝，即range指向的slice是原始slice的拷贝，长度也是；将slice的每个元素赋值给v时，也发生了一次拷贝，无法通过修改v来修改slice。  demo\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  arr := []int{1, 2, 3} // 比如此时arr的地址是0xc00000e380 \tfor _, v := range arr { // 此时arr的地址是0xc00000c3c0，可见发生了拷贝，v的地址就一直不变的 \tarr = append(arr, v) } fmt.Println(arr) // 打印：1 2 3 1 2 3  arr2 := []int{1, 2, 3} newArr := []*int{} for _, v := range arr2 { newArr = append(newArr, \u0026amp;v) } for _, v := range newArr { fmt.Printf(\u0026#34;%v \u0026#34;, *v) // 打印3 3 3，因为是v指向了同一个指针 \t}   扩容 原理 当使用append()函数向slice追加元素，会根据slice的容量判断是否需要扩容。另外，因为slice是值传递，append()函数不会修改传入的slice，返回是重新对底层数组、长度、容量做包装，返回新slice。\n 如果slice容量够用，则直接把新元素追加进去，长度 + 1，返回原slice 原slice容量不够，将slice扩容，得到新的slice 将新元素追加到新slice，长度 + 1，返回新slice  另外，copy函数拷贝两个slice时，会将源slice拷贝到目标slice，如果目标slice的长度\u0026lt;源slice，不会发生扩容。\nDemo：\n1 2 3 4 5 6 7 8 9 10 11  data := [10]int{} slice := data[5:8] slice = append(slice, 9)// slice=? data=? slice = append(slice, 10, 11, 12)// slice=? data=? 结果： // 第一次append后结果 slice = [0 0 0 9] data = [0 0 0 0 0 0 0 0 9 0] // 第二次append后结果 slice = [0 0 0 9 10 11 12] data = [0 0 0 0 0 0 0 0 9 0]   扩容策略 扩容实际上包括两部分：计算容量的规则 和 内存对齐\n  如果期望容量大于当前容量的两倍就会使用期望容量，期望容量指的是把元素加进去后的容量。\n  如果当前切片的长度小于 1024，扩容两倍。\n  如果当前切片的长度大于 1024 就会每次增加 25% 的容量，即扩容1.25倍，直到新容量大于期望容量。\n  此时只是确定切片的大致容量，之后会判断切片中元素所占字节大小，如果字节大小是1、2、8的倍数时，会进行内存对齐，这个操作之后，扩容后的容量可能会 \u0026gt; 原容量的两倍 或 1.25倍。\n内存对齐主要是为了提高内存分配效率，减少内存碎片。\n  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  // 此时期望容量是2 + 3 = 5 \u0026gt; 旧容量的两倍 2 * 2 = 4，期望容量为5，占40个字节，触发内存对齐，向上取整为48字节，此时新容量为 48 / 8 = 6。 s1 := []int64{1, 2} s1 = append(s1, 4, 5, 6) fmt.Printf(\u0026#34;len=%d, cap=%d\\n\u0026#34;,len(s1),cap(s1)) // len=5, cap=6  // 第一次append，扩容，拷贝旧数据到新数组，容量增长两倍； // 第二次append，没有产生新数组，只将元素进行追加； // 第三次append，扩容，拷贝旧数据到新数组，容量增长两倍； s := []int{1, 2} s = append(s, 4) fmt.Printf(\u0026#34;len=%d, cap=%d\\n\u0026#34;,len(s),cap(s)) // len=3, cap=4 s = append(s, 5) fmt.Printf(\u0026#34;len=%d, cap=%d\\n\u0026#34;,len(s),cap(s)) // len=4, cap=4 s = append(s, 6) fmt.Printf(\u0026#34;len=%d, cap=%d\\n\u0026#34;,len(s),cap(s)) // len=5, cap=8   Map 数据结构 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  type hmap struct { count int // 哈希表中元素的数量 \t// 1：可能有迭代器使用buckets，2：可能有迭代器使用oldbuckets，4：有协程正在向map中写入key，8：等量扩容 \tflags uint8 // 记录map的状态  B uint8 // buckets的数量，len(buckets) = 2^B \tnoverflow uint16 // 溢出的bucket的个数 \thash0 uint32 // 哈希种子，为哈希函数的结果引入随机性。该值在创建哈希表时确定，在构造方法中传入  buckets unsafe.Pointer // 桶的地址 \toldbuckets unsafe.Pointer // 扩容时用于保存之前buckets的字段，大小是当前buckets的一半或0.75 \tnevacuate uintptr // 迁移进度，小于nevacuate的表示已迁移  extra *mapextra // 用于扩容的指针，单个桶装满时用于存储溢出数据，溢出桶和正常桶在内存上是连续的 } type mapextra struct { overflow *[]*bmap oldoverflow *[]*bmap nextOverflow *bmap } type bmap struct { tophash [bucketCnt]uint8 // len为8的数组，即每个桶只能存8个键值对 } // 但由于go没有泛型，哈希表中又可能存储不同类型的键值对，所以键值对所占的内存空间大小只能在编译时推导， // 无法先设置在结构体中，这些字段是在运行时通过计算内存地址的方式直接访问，这些额外的字段都是编译时动态创建 type bmap struct { topbits [8]uint8 // 通过tophash找到对应键值对在keys和values数组中的下标  keys [8]keytype values [8]valuetype pad uintptr overflow uintptr // 每个桶只能存8个元素，超过8个时会存入溢出桶，溢出桶只是临时方案，溢出过多时会进行扩容 }    go map 结构图 \n基本   创建：m := map[string]int{\u0026quot;1\u0026quot;: 11, \u0026quot;2\u0026quot;: 22}或者 m := make(map[string]int, 10)\n  当使用字面量的方式创建哈希表时，如果{}中元素少于或等于25时，编译器会转成make的方式创建，再进行赋值；如果超过了25个，编译时会转成make的方式创建，同时为key和value分别创建两个数组，最后进行循环赋值\n  直接声明 var m map[string]int此时创建了一个nil的map，此时不能被赋值，但可以取值，虽然不会panic，但会得到零值\n  key不允许为slice、map、func，允许bool、numeric、string、指针、channel、interface、struct\n即key必须支持 == 或 != 运算的类型\n  map的容量为 装载因子6.5 * 2^B 个元素，装载因子 = 哈希表中的元素 / 哈希表总长度，装载因子越大，冲突越多。\n  拉链法解决哈希冲突（指8个正常位和溢出桶），除留余数法得到桶的位置（哈希值的低B位）。\n  key的哈希值的低B位计算获得桶的位置，高8位计算得到tophash的位置，进而找到key的位置。\n  溢出桶也是一个bmap，bmap的overflow会指向下一个溢出桶，所以溢出桶的结构是链表，但是它们跟正常桶是在一片连续内存上，都在buckets数组里。\n  每个桶存了8个tophash + 8对键值对。\n  map是非线程安全的，扩容不是一个原子操作，通过hmap里的flag字段在并发修改时进行fast-fail。\n  map的遍历是无序的，每次遍历出来的结果的顺序都不一样。\n  创建初始化 最终会在运行时调用makemap函数进行创建和初始化，\n  计算哈希表占用的内存是否溢出或者超出能分配的最大值\n  调用fastrand()获取随机哈希种子\n  根据hint来计算需要的桶的数量，即计算B的值，用于初始化桶的数量 = 2^B；hint是一个预置的长度。\n这个值不知道怎么来的，本人的机器debug时发现默认创建map时，hint=137，B=5\n  调用makeBucketArray()分配连续的空间，创建用于保存桶的数组\n当桶的数量小于2^4时，由于数据较少，哈希冲突的可能性较小，此时不会创建溢出桶。\n当桶的数量大于2^4时，就会额外创建2^(B-4)个溢出桶，溢出桶与普通桶在内存空间上是连续的。\n  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37  func makemap(t *maptype, hint int, h *hmap) *hmap { ... // 函数内，计算桶的数量 \tB := uint8(0) //计算得到合适的B \tfor overLoadFactor(hint, B) { B++ } h.B = B ... } func overLoadFactor(count int, B uint8) bool { // 常量loadFactorNum=13 ，loadFactorDen=2，bucketCnt=8，bucketShift()函数返回2^B \treturn count \u0026gt; bucketCnt \u0026amp;\u0026amp; uintptr(count) \u0026gt; loadFactorNum*(bucketShift(B)/loadFactorDen) } func makeBucketArray(t *maptype, b uint8, dirtyalloc unsafe.Pointer) (buckets unsafe.Pointer, nextOverflow *bmap) { base := bucketShift(b) nbuckets := base if b \u0026gt;= 4 { nbuckets += bucketShift(b - 4) sz := t.bucket.size * nbuckets up := roundupsize(sz) if up != sz { nbuckets = up / t.bucket.size } } buckets = newarray(t.bucket, int(nbuckets)) // 如果多申请了桶，将多申请的桶放在nextOverflow里备用 \tif base != nbuckets { nextOverflow = (*bmap)(add(buckets, base*uintptr(t.bucketsize))) last := (*bmap)(add(buckets, (nbuckets-1)*uintptr(t.bucketsize))) last.setoverflow(t, (*bmap)(buckets)) } return buckets, nextOverflow }   查找与插入 v := m[key]使用函数mapaccess1()进行查找， v, ok := m[key]使用函数mapaccess2()进行查找\nmapaccess2也会调用mapaccess1，只是返回的时候会返回多一个用于表示当前键值对是否存在的布尔值。\nkey的定位  找buckets数组中的bucket的位置：key经过哈希计算得到哈希值，取出hmap的B值，取哈希值的后B位，计算后面的B位的值得到桶的位置（实际上这一步就是除留余数法的取余操作）。 确定使用buckets数组还是oldbuckets数组：判断oldbuckets数组中是否为空，不为空说明正处于扩容中，还没完成迁移，则重新计算桶的位置，并在oldbuckets数组找到对应的桶；如果为空，则在buckets数组中找到对应的桶。 在桶中找tophash的位置：用key哈希计算得到的哈希值，取高8位，计算得到此bucket桶中的tophash，之后在桶中的正常位遍历比较。 每个桶是一整片连续的内存空间，先遍历bucket桶中的正常位，与桶中的tophash进行比较，当找到对应的tophash时，根据tophash进行计算得到key，根据key的大小计算得到value的地址，找到value。 如果bucket桶中的正常位没找到tophash，且overflow不为空，则继续遍历溢出桶overflow bucket，直到找到对应的tophash，再根据key的大小计算得到value的地址，找到value。  1 2 3 4 5 6 7 8 9 10 11 12 13 14  // 计算得到bucket桶在buckets数组中的位置 bucket := hash \u0026amp; bucketMask(h.B) b := (*bmap)(unsafe.Pointer(uintptr(h.buckets) + bucket*uintptr(t.bucketsize))) // 计算得到tophash，miniTopHash用于表示迁移进度 top := uint8(hash \u0026gt;\u0026gt; (sys.PtrSize*8-8)) if top \u0026lt; minTopHash { top += minTopHash } return top // 计算key和value，dataoffset是tophash[8]所占用的大小，所以key的地址就是：b的地址 + dataOffset的偏移 + 对应的索引i * key的大小 k := add(unsafe.Pointer(b), dataOffset+i*uintptr(t.keysize)) val = add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr(t.keysize)+i*uintptr(t.valuesize))    go map 结构图 \n插入 插入也需要先定位到key的位置后才能进行插入，定位key的操作跟上面是类似的，调用mapassign函数。\n 先判断hmap是否为空，是否是并发写入，如果是直接抛错误。 对key进行哈希，如果bucket桶为空，则分配bmap数组，如果没有指定长度，则惰性分配。 根据key的哈希值的低B位，计算得到桶的位置；判断是否正在扩容。 根据key的哈希值的高8位，计算出tophash，先遍历正常位，从第一个tophash开始，比较桶上的每个tophash是否等于计算得到的tophash，如果不等，再判断该tophash是否为空，如果为空，计算key和value的内存地址，进行插入。如果不为空，则遍历下一个tophash。 如果桶上的tophash与计算的tophash相等，说明发生了哈希冲突，先计算key的地址，找到key，判断key是否相等，如果相等，计算key对应的value地址，将value的值进行更新。 如果key不相等，遍历下一个tophash，直到正常位遍历完成，如果此时还不能插入，继续遍历溢出桶，如果溢出桶为空，退出循环 判断是否扩容，如果需要扩容，则扩容后（扩容迁移）继续从步骤5上继续，如果不用扩容则走步骤8 如果还没进行插入，说明正常位已经满了，且还不需要扩容，此时会调用newoverflow函数，先使用hmap预先在noverflow中创建好的桶，如果有，遍历这个创建好的桶链表，直到可以放入新的键值对；如果没有，则创建一个桶，增加noverflow计数，将新键值对放入这个桶中，然后将新桶挂载到当前桶overflow字段，成为溢出桶。  mapassign函数\nnewoverflow函数\n扩容 扩容条件 没有正在进行扩容 \u0026amp;\u0026amp; （负载因子超过6.5 || 存在过多溢出桶 overflow buckets）\n即!h.growing() \u0026amp;\u0026amp; (overLoadFactor(h.count+1, h.B) || tooManyOverflowBuckets(h.noverflow, h.B))\n扩容策略 增量扩容 - 降低哈希冲突 overLoadFactor函数，该函数返回true，表示哈希表内的元素过多，哈希冲突的概率变大，查找可能在找到桶，遍历完桶内的元素，还要继续遍历溢出桶链表，此时需要**增量扩容，扩容为原来的两倍 **，降低哈希冲突的概率。\n1 2 3 4  func overLoadFactor(count int, B uint8) bool { // loadFactorNum = 13, loadFactorDen = 2, 即count \u0026gt; 8 \u0026amp;\u0026amp; count / (2^B) \u0026gt; 6.5  return count \u0026gt; bucketCnt \u0026amp;\u0026amp; uintptr(count) \u0026gt; loadFactorNum *(bucketShift(B)/loadFactorDen) }   等量扩容 - 提高桶的利用率，防止内存泄漏，加快查询效率 tooManyOverflowBuckets函数，该函数返回true，表示由于某一个桶满后，开始使用溢出桶，不断的插入数据到溢出桶，又不断的删除正常桶上的正常位，但此时哈希表的数量又没超阈值，即空桶太多，溢出桶的数量太多，而每次查找又得先遍历正常位，查找效率变低，此时需要等量扩容，容量不变，重新迁移键值对。\n1 2 3 4 5 6 7 8 9 10  func tooManyOverflowBuckets(noverflow uint16, B uint8) bool { // 如果负载因子太低, 不操作。  // 如果负载因子太高，maps的扩容和缩容会使用大量未使用的内存  // 太多指的是溢出桶的数量和buckets数组的数量一样.  if B \u0026gt; 15 { B = 15 } // The compiler doesn\u0026#39;t see here that B \u0026lt; 16; mask B to generate shorter shift code.  return noverflow \u0026gt;= uint16(1)\u0026lt;\u0026lt;(B\u0026amp;15) }   触发扩容 触发扩容条件时，会执行hashGrow函数，进行新桶的分配，此时还未迁移数据。\n  首先会判断是增量扩容还是等量扩容，如果是增量扩容，B + 1，如果是等量扩容，B + 0\n  将当前buckets数组挂在hmap的oldbuckets字段，当前的溢出桶挂在hmap.mapextra.oldoverflow\n  创建新的buckets数组，容量为新的B值，预创建溢出桶（溢出桶的数量看上面创建初始化逻辑），然后将新的buckets数组挂在buckets字段，新的溢出桶挂在hmap.mapextra.nextOverflow字段上\n  触发扩容条件，对新桶进行内存分配，只是创建了新的桶，旧数据还在旧桶上，之后还需要完成数据迁移。\nhashGrow函数\n扩容迁移 扩容迁移发生在 mapassign 和 mapdelete 函数中，即进行插入、修改、删除时，才会调用growWrok函数和evacuate函数，完成真正的迁移工作后，才会进行插入、修改或删除。\n迁移时是渐进式迁移，一次最多迁移两个bucket桶。\n  在插入、修改或删除中，如果发现oldbuckets数组不为空，表示此时正在扩容中，需要进行扩容迁移，调用growWork函数，growWork函数调用一次evacuate函数，如果调用完成后，hmap的oldbuckets还是非空，则再调用一次evacuate函数，加快迁移进程。\n  进入evacuate函数，如果是等量扩容，B值不变，老bucket桶上的键值计算出来的桶的序号不变，tophash不变，此时会将老桶上的键值对依次地一个个转移到新桶上，使这些键值对在新桶上排列更加紧凑；\n如果是增量扩容，容量变为原来的两倍，B值+1，老bucket桶上的键值计算出来的桶的序号改变，这些键值对计算后的bucket桶的序号可能跟之前一样，也可能是相比原来加上2^B，取决于key哈希值后 老B+1 位的值是0还是1，tophash不变，原来老bucket桶上的键值对会分流到两个新的bucket桶上。将老bucket桶上的键值对和其指向的溢出桶上的键值对进行迁移，依次转移到新桶上，每迁移完一个，hmap的nevacuate计数+1，直到老bucket桶上的键值对迁移完成，最后情况oldbuckets和oldoverflow字段\n  evacuate函数\n删除 调用delete函数，无论要删除的key是否存在，delete都不会返回任何结果。删除实际上也是一个key的定位 + 删除的操作，定位到key后，将其键值对置空，hmap的count - 1，tophash置为empty。\n遍历 对go中的map是无序的，每次遍历出来的顺序都是不一样的，go在每次遍历map时，并不是固定地从0号bucket桶开始遍历，每次都是从一个随机值序号的bucket桶开始遍历，并且是从这个bucket桶的一个随机序号的 正常位开始遍历。\n  首先从buckets数组中，随机确定一个索引，作为startBucket，然后确定offset偏移量，得到桶中的正常位的位置，作为起始key的地址。 遍历当前bucket及bucket.overflow，判断当前bucket是否正在扩容中，如果是则跳转到3，否则跳转到4。 如果是在扩容中，遍历时会先到当前bucket扩容前的老的bucket桶中遍历那些能迁移到当前桶的key。  假如原先的buckets为0，1，那么扩容后的新的buckets为0，1，2，3，此时我们遍历到了buckets[0]， 发现这个bucket正在扩容，那么找到bucket[0]所对应的oldbuckets[0]，遍历里面的key，这时候仅仅遍历那些key经过hash后，可以散列到bucket[0]里面的部分key；同理，当遍历到bucket[2]的时候，发现bucket正在扩容，找到oldbuckets[0]，然后遍历里面可以散列到bucket[2]的那些key。\n遍历当前这个bucket即可。 继续遍历bucket下面的overflow链表。 如果遍历到了startBucket，说明遍历完了，结束遍历。   Runtime  不同于Java，Go没有虚拟机，很多东西比如自动GC、对操作系统和CPU相关操作都变成了函数，写在runtime包里。 Runtime提供了go代码运行时所需要的基础设施，如协程调度、内存管理、GC、map、channel、string等内置类型的实现、对操作系统和CPU相关操作进行封装。 诸如go、new、make、-\u0026gt;、\u0026lt;-等关键字都被编译器编译成runtime包里的函数 build成可执行文件时，Runtime会和用户代码一起进行打包。  Goroutine 基本   GPM模型 - M：N调度模型\n其他模型：\n N：1 即 N个协程绑定1个线程，优点：协程在用户态线程即可完成切换，由协程调度器调度，不涉及内核态，无需CPU调度，轻量快速；缺点：无法使用多核加速，一旦某协程阻塞，会导致线程阻塞，此时并行变成串行 1：1 即 1个协程绑定1个线程，优点：解决N：1模型的缺点；缺点：调度均有协程调度器和CPU调度，代价较大，无法并行 M：N 即 M个协程绑定N个线程，由协程调度器调度，线程在内核态通过CPU抢占式调用，协程在用户态通过协作式调度    一般线程会占有1Mb以上的内存空间，每次对线程进行切换时会消耗较多内存，恢复寄存器中的内容还需要向操作系统申请或销毁对应的资源，每一次上下文切换都需要消耗~1us左右的时间，而Go调度器对goroutine的上下文切换为~0.2us，减少了80%的额外开销。\n  协程本质是一个数据结构，封装了要运行的函数和运行的进度，交由go调度器进行调度，不断切换的过程。由go调度器决定协程是运行，还是切换出调度队列(阻塞)，去执行其他满足条件的协程。\ngo的调度器在用户态实现调度，调度的是一种名叫协程的执行流结构体，也有需要保存和恢复上下文的函数，运行队列。\n协程同步造成的阻塞，只是调度器切换到别的协程去执行了，线程本身并不阻塞。\n  Go的调度器通过使用与CPU数量相等的线程减少线程频繁切换的内存开销，同时在每一个线程上执行额外开销更低的Goroutine来降低操作系统和软件的负载。\n  1.2~1.3版本使用基于协作的抢占式调度器（通过编译器在函数调用时插入抢占式检查指令，在函数调用时检查当前goroutine是否发起抢占式请求），但gouroutine可能会因为垃圾回收和循环长时间占用资源导致程序暂停。\n从1.14版本开始使用基于信号的抢占式调度，垃圾回收在扫描栈时会触发抢占式调度，但抢占时间点不够多，还不能覆盖全部边缘情况。\n之所以要使用抢占式的，是因为不使用抢占式时，只有当goroutine主动让出CPU资源才能触发调度，可能会导致某个goroutine长时间占用线程，造成其他goroutine饿死；另外，垃圾回收需要暂停整个程序，在STW时，整个程序无法工作。\n  早期调度模型-MG模型  goroutine early schedule \n线程M想要处理协程G，都必须访问全局队列GRQ，当多个M访问同一资源时需要加锁保证并发安全，因此M对G的创建，销毁，调度都需要上锁，造成激烈的锁竞争，导致性能较差。\n另外，当M0执行G0，但G0又产生了G1，此时为了继续执行G0，需要将G1移给M1，造成较差的局部性，因为一般情况下这两个G是有一定的关联性的，如果放在不同的M会增加系统开销；CPU在多个M之间切换也增加了系统开销。\n为了解决早期调度器模型的缺点，采用了GMP模型。\n调度器的GPM模型 goroutine完全运行在用户态，借鉴M：N线程映射关系，采用GPM模型管理goroutine。\n G：即goroutine，代码中的go func{}，代表一个待执行的任务 M：即machine，操作系统的线程，由操作系统的调度器调度和管理。 P：即processor，处理器的抽象，运行在线程上的本地调度器，用来管理和执行goroutine，使得goroutine在一个线程上跑，提供了线程需要的上下文，（局部计算资源，用于在同一线程写多个goroutine的切换），负责调度线程上的LRQ，是实现从N：1到N：M映射的关键。存在的意义在于工作窃取算法。  GPM三者的关系与特点   p的个数取决于GOMAXPROCS，默认使用CPU的个数，这些P会绑定到不同内核线程，尽量提升性能，让每个核都有代码在跑。\n  M的数量不一定和P匹配，可以设置多个M，M和P绑定后才可运行，多余的M会处于休眠状态。\n调度器最多可创建10000个M，但最多只有GOMAXPROCS个活跃线程能够正常运行。\n所以一般情况下，会设置与P一样数量的M，让所有的调度都发生在用户态，减少额外的调度和上下文切换开销。\n一个G最多占有CPU 10ms，防止其他G饿死。\n  P包含一个LRQ(Local Run Queue本地运行队列)，保存P需要执行的goroutine的队列。LRQ是一个长度为256的环形数组，有head和tail两个序号，当数量达到256时，新创建的goroutine会保存在GRQ中。\n当在G0中产生G1，此时会G1会优先加入当前的LRQ队列，保证其在同一个M上执行\n  调度器本身包含一个GRQ(Global Run Queue全局运行队列)，保存所有未分配的goroutine，存于全局遍历sched中。GRQ是一个链表，由head，tail两个指针，从GRQ中获取G需要上锁。\n  在没有P的情况下，所有G只能放在一个GRQ(全局队列)中，当M执行完G，且没有G可执行时，必须锁住该全局队列才能取G。\n  P持有G的LRQ(本地队列)，而持有P的M执行完G后在P本地队列中没有发现其他G可执行时，会先检查全局队列、网络，如果都没有可执行的G，这时就会从其他P的队列偷取一个G来执行（即工作窃取），否则，进入自旋状态。\nM进入自旋，是为了避免频繁的暂止和复始产生大量的开销，当其他G准备就绪时，首先被调度到自旋的M上，其次才是去复始新线程。\n自旋只会持续一段时间，如果自旋期间没有G需要调度，则之后会进入暂止状态，等待复始。\nM自旋时会调用G0协程，G0协程主要负责调度时协程的切换。\nM是否新建取决于正在自旋的M或者休眠的M的数量。\n如果LRQ满了，会把LRQ中随机一半G放到GRQ中；当M发现自己的LRQ、GRQ都没有G了，则会从其他M中窃取一半的G放到自己的LRQ。\n  运行时的G会尝试唤醒其他空闲的M和P进行组合，被唤醒的M和P由于刚被唤醒，进入自旋状态，G0发现P的本地队列没有G，则会从全局队列里获取G放入本地队列，获取数量n = min(len(GQ)/GOMAXPROCS + 1, len(GQ/2))\n  空闲的M链表，主要用于保存无G可运行时而进入休眠的M，也保存在全局变量sched中，进入休眠的M会等待信号量m.park的唤醒。\n  空闲的P链表，当无G可运行时，拥有P的M会释放P并进入休眠状态，释放的P会变成空闲状态，加入到空闲的P链表中，也保存在全局变量sched中，当M被唤醒时，其持有的P也会重新进入运行状态。\n   go中还有特殊的M和G, 它们是M0和G0.\nM0是启动程序后的主线程, 这个M对应的实例会在全局变量M0中, 不需要在heap上分配, M0负责执行初始化操作和启动第一个G， 在之后M0就和其他的M一样了.\nG0是仅用于负责调度的G, G0不指向任何可执行的函数, 每个M都会有一个自己的G0, 在调度或系统调用时会使用G0的栈空间, 全局变量的G0是M0的G0.\n 调度的时机  go调度器，本质是为需要执行的G寻找M以及P，不是一个实体，调度是需要发生调度时由M执行runtime.schedule方法进行 调度器初始化时，会依次调用mcommoninit：初始化M资源池、procresize：初始化P资源池、newproc：G的运行现场和调度队列 channel、mytex等sync操作发生协程阻塞 time.sleep IO GC 主动yield 运行过久或系统调度过久  总的调度流程  goroutine schedule \n5.1 当M执行某个G时发生syscall或其他阻塞操作，M会阻塞，如果当前有一些G在执行，runtime会把这个线程M从P中摘除，然后再创建一个新的M（操作系统线程或者复用其他空闲线程）来服务这个P，即此时的M会直接管理阻塞的G，之前跟它绑定的P转移到其他M，执行其他G。\n当原阻塞的M系统调用或阻塞结束时，其绑定的这个G要继续往下执行，会优先尝试获取之前的P，若之前的P已经跟其他M绑定，则尝试从空闲的P列表获取P，将G放入这个P的本地队列，继续执行。如果获取不到P，则该M进入休眠，加入休眠队列，G则放入全局队列，等其他P消费它。\n调度Demo 单核机器，只有一个处理器P，系统初始化两个线程M0和M1，处理器P优先绑定线程M0，线程M1进入休眠状态。目前P正在处理G0，LRQ里的G1、G2、G3等待处理，GRQ里的G4、G5等到分配。\n如果G0短时间处理完，P就会从LRQ取出G1进行处理，LRQ从GRQ取出G4进行分配；\n goroutine runtime_1 \n如果G0处理得很慢，系统就会让M0休眠，挂起G0，唤醒线程M1，将LRQ转移给M1进行处理；\n如果此时G1也处理得很慢，此时会阻塞，或者休眠M1，唤醒M0，回去继续处理G0；切换M和G的操作由sysmon协程进行处理，即抢占式由sysmon函数实现。\n如果G1处理得很快，则继续获取LRQ里的下一个G；待LRQ里的G都执行完了，切回M0，继续处理G0。\n goroutine runtime_2 \n如果是多核的，有多个P，多个M，当有一个P处理完所有的G后，会先从GRQ中获取G，如果获取不到，就会从另一个P的LRQ里取走一半G，继续处理。\nsysmon协程 由sysmon协程进行协作式抢占，对goroutine进行标记，执行goroutine时如果有标记就会让出CPU，对于syscall过久的P，会进行M和P的分配，防止P被占用过久影响调度。\n go sysmon goroutine \nM：Machine M本质是一个循环调度，不断的执行schedule函数，查找可运行的G。会在自旋与休眠的状态间转换。\n没有状态标记，只是会处于以下几个场景：\n 自旋：M正在从LRQ中获取G，此时M会拥有一个P 拥有一个P，执行G中的代码 进行系统调用或者G的阻塞操作，此时M会释放P 休眠，无G可执行，不拥有P，此时存在空闲线程队列  G：Goroutine的状态  go goroutine state \ngoroutine的状态不止以下几种，只是这几种比较常用\n   G状态 值 说明     _Gidle 0 刚刚被分配，还没被初始化   _Grunnable 1 表示在runqueue上，即LRQ，还没有被执行，此时的G才能被M执行，进入Grunning状态   _Grunning 2 执行中，不在runqueue上，与M、P绑定   _Gsyscall 3 在执行系统调用，没有执行go代码，没在runqueue上，只与M绑定，此时P转移到其他M中   _Gwaiting 4 被阻塞（如IO、GC、chan阻塞、锁）不在runqueue，但一定在某个地方，比如channel中，锁排队中等   _Gdead 6 现在没有在使用，也许执行完，或者在free list中，或者正在被初始化，可能有stack   _Gcopystack 8 栈正在复制，此时没有go代码，也不在runqueue上，G正在获取一个新的栈的空间，并把原来的内容复制过去，防止GC扫描   _Gscan 0x1000 与runnable、running、syscall、waiting等状态结合，表示GC正在扫描这个G的栈    P：Processor的状态  go processor state \n   状态 描述     _Pidle 空闲，无可运行的G，这时M拥有的P会加入空闲P队列中，LRQ为空   _Prunning 被线程 M 持有，并且正在执行G或者G0（即用户代码或者调度器逻辑）   _Psyscall 用户代码触发了系统调用，此时P没有执行用户代码   _Pgcstop 被线程 M 持有，且因gc触发了STW而停止   _Pdead 当运行时改变了P的数量时，多余的P会变成此状态    泄露与排查 goroutine的泄露一般会导致内存的泄露，最终导致OOM，原因一般是该运行完成的goroutine一直在运行，没有结束，可能的原因是goroutine内阻塞，死循环。\n检查工具：pprof，请求/debug/pprof/goroutine接口或者heap接口，判断内存占用走势，分析内存使用情况。一般的走势是整体向上递增，伴随一个一个的峰谷。\nGC 基本  使用可达性分析判断对象是否被回收 三色标记法进行GC，本质是标记-清除算法，三色标记法是其改进版，主要是为了减少STW的时间 Go 语言为了实现高性能的并发垃圾收集器，使用三色抽象、并发增量回收、混合写屏障、调步算法以及用户程序协助等机制将垃圾收集的暂停时间优化至毫秒级以下  三色标记  白色：潜在垃圾，其内存可能会被垃圾收集器回收 灰色：活跃对象，因为存在指向白色对象的外部指针，垃圾收集器会扫描这些对象的子对象 黑色：活跃对象，包括不存在任何引用外部指针对象以及从根对象可达的对象   go gc简化过程 \n 初始对象都是白色，首先把所有对象都放到白色集合中 从根节点开始遍历对象，遍历到的对象标记为灰色，放入到灰色集合 遍历灰色对象，把自己标记为黑色，放入黑色集合，将其引用的对象标记为灰色，放入灰色集合 重复第3步，直到灰色集合为空，此时所有可达对象都被标记，标记阶段完成 清除阶段开始，白色集合里的对象为不可达对象，即垃圾，对内存进行迭代清扫，回收白色对象 重置GC状态，将所有的对象放入白色集合中   实际上并没有对应颜色的集合，对象被内存分配器分配在span中，span里有个gcmarkBits字段，每个bit代表一个slot被标记，白色对象该bit为0，灰色或黑色为1。\n每个p中都有wbBuf和gcw gcWork, 以及全局的workbuf标记队列, 实现生产者-消费者模型, 在这些队列中的指针为灰色对象, 表示已标记, 待扫描.\n从队列中取出来并把其引用对象入队的为黑色对象, 表示已标记, 已扫描. (runtime.scanobject).\n 写屏障 在标记过程中，用户程序可能会修改对象的指针，导致标记错误，对象被错误回收，因此在标记阶段需要STW，此时也无法并发或增量执行。\n 想要在并发或增量的标记算法中保证正确性，需要达成任意一种三色不变性\n 强三色不变性：黑色对象不会指向白色对象，只会指向灰色对象或黑色对象 弱三色不变性：黑色对象指向的白色对象必须包含一条从灰色对象经由多个白色对象的可达路径   go中使用了写屏障来保证标记的正确性。写屏障是在写入指针前执行的一小段代码，用以防止并发标记时指针丢失，这一小段代码Go是在编译时加入的。\nDijkstra的插入写屏障  go dijkstra插入写屏障 \n Dijkstra写屏障是对被写入的指针进行grey操作, 不能防止指针从heap被隐藏到黑色的栈中, 需要STW重扫描栈.\n Yuasa的删除写屏障  go yuasa删除写屏障 \n Yuasa写屏障是对将被覆盖的指针进行grey操作, 不能防止指针从栈被隐藏到黑色的heap对象中, 需要在GC开始时保存栈的快照.\n 垃圾收集过程   清理终止阶段；  暂停程序，所有的处理器在这时会进入安全点（Safe point）； 如果当前垃圾收集循环是强制触发的，我们还需要处理还未被清理的内存管理单元；   标记阶段；  将状态切换至 _GCmark、开启写屏障、用户程序协助（Mutator Assiste）并将根对象入队； 恢复执行程序，标记进程和用于协助的用户程序会开始并发标记内存中的对象，写屏障会将被覆盖的指针和新指针都标记成灰色，而所有新创建的对象都会被直接标记成黑色； 开始扫描根对象，包括所有 Goroutine 的栈、全局对象以及不在堆中的运行时数据结构，扫描 Goroutine 栈期间会暂停当前处理器； 依次处理灰色队列中的对象，将对象标记成黑色并将它们指向的对象标记成灰色； 使用分布式的终止算法检查剩余的工作，发现标记阶段完成后进入标记终止阶段；   标记终止阶段；  暂停程序、将状态切换至 _GCmarktermination 并关闭辅助标记的用户程序； 清理处理器上的线程缓存；   清理阶段；  将状态切换至 _GCoff 开始清理阶段，初始化清理状态并关闭写屏障； 恢复用户程序，所有新创建的对象会标记成白色； 后台并发清理所有的内存管理单元，当 Goroutine 申请新的内存管理单元时就会触发清理；     GC触发时机 太难了。。。有时间再继续整理\n参考 Go入门指南\n深入理解Slice底层实现\nGo 语言设计与实现\nGolang源码-Map实现原理分析\nGo map原理剖析\n深度解密Go语言之channel\nGC\n图解Go协程调度原理，小白都能理解 \n深入golang runtime的调度\ngopher meetup-深入浅出Golang Runtime-yifhao\nGolang调度器GMP原理\n【golang】GMP调度详解\n极客时间 - Go并发编程实战\n","date":"2020-11-07T00:00:00Z","permalink":"http://nixum.cc/p/go/","title":"Go"},{"content":"[TOC]\nZooKeeper ZooKeeper保证的是CP，不保证每次服务请求的可用性，在极端环境下，ZooKeeper可能会丢弃一些请求，消费者程序需要重新请求才能获得结果。另外在进行leader选举时集群都是不可用，所以说，ZooKeeper不能保证服务可用性。\n使用场景  集群管理，监控节点存活状态 主节点选举，当服务以master-salve模式进行部署，当主节点挂掉后选出新的主节点 服务发现 分布式锁，提供独占锁、共享锁 分布式自增id 搭配Kafka、dubbo等使用  特点  顺序一致性：同一客户端发起的事务请求，最终将会严格地按照顺序被应用到 ZooKeeper 中去。 原子性：所有事务请求的处理结果在整个集群中所有机器上的应用情况是一致的，也就是说，要么整个集群中所有的机器都成功应用了某一个事务，要么都没有应用。 单一系统映像：无论客户端连到哪一个 ZooKeeper 服务器上，其看到的服务端数据模型都是一致的。 可靠性：一旦一次更改请求被应用，更改的结果就会被持久化，直到被下一次更改覆盖。  数据模型 类似文件系统，根节点为 / ，每创建一个节点会从根节点开始挂，树形结构，每个数据节点称为znode，可以存储数据，每个znode还有自己所属的节点类型和节点状态\n  持久节点：一旦创建就一直存在，直到将其删除。 持久顺序节点：一个父节点可以为其子节点 维护一个创建的先后顺序 ，这个顺序体现在 节点名称 上，是节点名称后自动添加一个由 10 位数字组成的数字串，从 0 开始计数。 临时节点：临时节点的生命周期是与 客户端会话 绑定的，会话消失则节点消失 。临时节点 只能做叶子节点 ，不能创建子节点。 临时顺序节点：父节点可以创建一个维持了顺序的临时节点(和前面的持久顺序性节点一样)。   ZAB协议 通过ZAB协议保证注册到ZooKeeper上的主从节点状态同步，该协议有两种模式\n  崩溃恢复\n当整个 Zookeeper 集群刚刚启动或者Leader服务器宕机、重启或者网络故障导致不存在过半的服务器与 Leader 服务器保持正常通信时，所有服务器进入崩溃恢复模式，首先选举产生新的 Leader 服务器，然后集群中 Follower 服务器开始与新的 Leader 服务器进行数据同步。\n  消息广播\n当集群中超过半数机器与该 Leader 服务器完成数据同步之后，退出恢复模式进入消息广播模式，Leader 服务器开始接收客户端的事务请求生成事物提案（超过半数同意）来进行事务请求处理。\n  选举算法和流程 ZooKeeper集群机器要求至少三台机器，机器的角色分为Leader、Follower、Observer\n Leader选举：一个节点只要求获得半数以上投票，就可以当选为准Leader Discovery发现：准Leader收集其他节点的数据，并将最新的数据复制到自身 Synchronization同步：准Leader将自身最新数据复制给其他落后节点，并告知其他节点自己正式当选为Leader Broadcast广播：Leader正式对外服务，处理client请求，对消息进行广播，当收到一个写请求后，会生成Proposal广播给各个Follower节点，一半以上Follower节点应答后，Leader再发送Commit命令给各个Follower，告知他们提交相关提案。  默认使用FastLeaderElection算法，比如现在有5台服务器，每台服务器均没有数据，它们的编号分别是1, 2, 3, 4, 5按编号依次启动，它们的选择举过程如下：\n 服务器1启动，给自己投票，然后发投票信息，由于其它机器还没有启动所以它收不到反馈信息，服务器1的状态一直属于Looking。 服务器2启动，给自己投票，同时与之前启动的服务器1交换结果，由于服务器2的编号大所以服务器2胜出，但此时投票数没有大于半数，所以两个服务器的状态依然是Looking。 服务器3启动，给自己投票，同时与之前启动的服务器1,2交换信息，由于服务器3的编号最大所以服务器3胜出，此时投票数正好大于半数，所以服务器3成为leader，服务器1,2成为Follower。 服务器4启动，给自己投票，同时与之前启动的服务器1,2,3交换信息，尽管服务器4的编号大，但之前服务器3已经胜出，所以服务器4只能成为Follower。 服务器5启动，后面的逻辑同服务器4成为Follower。  当 Leader 服务器出现网络中断、崩溃退出与重启等异常情况时，ZAB 协议就会进入恢复模式并选举产生新的Leader服务器。\n Leader election（选举阶段）：节点在一开始都处于选举阶段，只要有一个节点得到超半数节点的票数，它就可以当选准Leader。 Discovery（发现阶段）：在这个阶段，Followers 跟准 Leader 进行通信，同步 followers 最近接收的事务提议。 Synchronization（同步阶段）:同步阶段主要是利用 Leader 前一阶段获得的最新提议历史，同步集群中所有的副本。同步完成之后 准 leader 才会成为真正的 Leader。 Broadcast（广播阶段） 到了这个阶段，Zookeeper 集群才能正式对外提供事务服务，并且 Leader 可以进行消息广播。同时如果有新的节点加入，还需要对新节点进行同步。  通知机制 客户端会对某个znode建立一个watcher事件，当该znode发生变化时，这些客户端会收到ZooKeeper的通知，然后客户端根据znode的变化来做出相应的改变，类似观察者模式\nETCD 总体 是一个CP系统，分为三个版本，V1、V2和V3\n共识算法使用Raft。将复杂的一致性问题分解成Leader选举、日志同步、安全性三个独立子问题，只有集群一半以上节点存活即可提供服务，具备良好可用性。\n使用场景：配置存储、服务发现、主备选举，读多写少的场景\n \netcdctl支持负载均衡、健康检测、故障转移，3.4版本中负载均衡使用轮询算法，轮询endpoints的每个节点建立长连接，将请求发送给etcd server。client和server之间使用HTTP/2.0协议通信。\netcd server在处理一个请求时会先将一系列的拦截器串联成一个执行，常见的拦截器有debug日志、metrics统计、etcd learner节点请求接口和参数限制等能力，另外还要求执行一个操作前集群必须有Leader，若请求延时超过指定阈值，会打印来源IP的慢查询日志。\nV2版本 数据模型参考ZooKeeper，使用基于目录的层次模式，使用Restful 风格的API，提供常用的Get/Set/Delete/Watch等API，实现对key-value数据的查询、更新、删除、监听等操作。\nKey—Value存储上使用简单内存树，一个节点包含节点路径、父亲节点、孩子节点、过期时间、Value的值，是典型的低容量设计，数据全放内存，无需考虑数据分片，只保存Key的最新版本。\n在Kubernetes中的使用场景：\n 使用Kubernetes声明式API部署服务的时候，Kubernetes 的控制器通过etcd Watch 机制，会实时监听资源变化事件，对比实际状态与期望状态是否一致，并采取协调动作使其一致。Kubernetes 更新数据的时候，通过CAS 机制保证并发场景下的原子更新，并通过对key 设置TTL来存储Event事件，提升Kubernetes 集群的可观测性，基于TTL特性，Event事件key到期后可自动删除。\n 缺点：\n 功能局限：不支持范围查询、分页查询、多key事务 Watch机制可靠性问题：V2是内存型，不保存key历史版本的数据库，只在内存中使用滑动窗口保存最近1000条变更事件，当写请求比较多、网络波动等容易产生事件丢失问题 性能问题：使用HTTP/1.x协议，当请求响应较大时无法进行压缩；Json解析消耗CPU；当watcher较多时，由于不支持多路复用，会创建大量的连接；大量的TTL一样也需要为每个key发起续期，无法批量操作 内存开销问题：简单内存树保存key和value，量大时会导致较大的内存开销，保证可靠还需要全量内存树持久化到磁盘，消耗大量CPU和磁盘IO  V3版本 为了解决V2版本的缺点，才诞生了V3版本\n 在内存开销、Watch 事件可靠性、功能局限上，它通过引入B-tree、 boltdb 实现一个MVCC数据库，数据模型从层次型目录结构改成扁平的key-value，提供稳定可靠的事件通知，实现了事务，支持多key原子更新，同时基于boltdb的持久化存储，显著降低了etcd 的内存占用、避免了etcd v2 定期生成快照时的昂贵的资源开销。\n性能上，首先etcd v3 使用了 gRPC API，使用protobuf 定义消息，消息编解码性能相比JSON 超过2倍以上，并通过 HTTP/ 2.0 多路复用机制，减少了大量 watcher 等场景下的连接数。\n其次使用 Lease优化TTL机制，每个Lease具有一 个 TTL，相同的TTL 的key关联一 个Lease，Lease过期的时候自动删除相关联的所有key，不再需要为每个key单独续期。\n最后是etcd v3支持范围、分页查询，可避免大包等 expensive request。\n 读操作 客户端通过etcdctl发送get请求etcdctl get [key名称] --endpoints [多个etcd节点地址]，etcdctl通过负载均衡算法选择一个etcd节点，发起gRpc调用，etcd server收到请求后经过一系列gRpc拦截器后，进入KV Server模块。之后根据读的行为，进行对应的操作\n读操作之前，如果有一个写操作：client发出一个写请求后，若Leader收到写请求，会将此请求持久化到WAL日志，并传播到各个节点，若一半以上的节点持久化成功，则该请求对应的日志条目被标识为已提交，etcd server模块异步从Raft模块获取已提交的日志，应用到状态机(boltdb等)。\n串行读 直接读状态机（boltdb等）的数据返回，无需通过Raft协议与集群进行交互的模式，可能会读到旧数据。即：写请求广播到各个节点，但串行读可能读到某个还没进行写请求提交的节点(但可能其他节点已提交)，从而读到旧数据。\n这种读取方式低延时，高吞吐，适用于读取数据敏感度低、对数据一致性要求不高的场景。\n线性读（默认） 一旦一个值更新成功后（指有超过半数节点更新提交成功），任何线性读的client都能及时访问到。\nReadIndex：保证数据一致性  节点C收到一个线性读请求后，首先会从Leader获取集群最新的已提交的日志索引(committed index)； Leader收到ReadIndex请求后，为防止脑裂异常，会向各个Follower节点发送心跳确认，待一半以上节点确认Leader身份后，才能将已提交的索引(committed index))返回给节点C。 C节点继续等待，直到状态机上已应用索引(applied index)大于等于Leader的已提交索引(committed index)时，通知读请求，数据已赶上Leader，可以从状态机中访问数据  既然Follower节点都已经发送ReadIndex请求了，为啥不直接把读请求转发给Leader？原因是ReadIndex比较轻量，而读请求不轻，大量的读请求会造成Leader节点有比较大的负载。\nMVCC：支持Key多历史版本，多事务功能 核心是内存树形索引模型treeIndex + 嵌入式KV持久化存储库boltdb组成。\nboltdb会对key的每一次修改，都生成一个新的版本号对象，以版本号为key，value为用户key-value等信息组成的结构体。版本号全局递增，通过treeIndex模块保存用户key和版本号的映射。查询时，先去treeIndex模块查询key对应的版本号，根据版本号到boltdb里查询对应的value信息。\n  treeIndex基于B树实现，只会保存用户的key和版本号的映射，具体的value信息则保存再boltdb里。\n  boltdb基于B+树实现的kv键值库，支持事务，提供Get/Put等API，etcd通过boltdb实现一个key的多历史版本。在读取boltdb前，会从一个内存读事务buffer中，二分查找要访问的key是否在buffer里，提高查询速度。\n若buffer未命中，就进到boltdb中查询。boltdb通过bucket隔离集群元数据于用户数据，每个bucket对应一张表（一颗B+树），用户数据的key的值等于bucket名字，etcd MVCC元数据存放的bucket是meta。\n  写操作 客户端通过etcdctl发送put请求etcdctl put [key值] [value值] --endpoints [多个ectd节点地址]，etcdctl通过负载均衡算法选择一个etcd节点，发起gRpc调用，etcd server收到请求后经过一系列gRpc拦截器、Quota模块后，进入KV Server模块，KV Server模块向Raft模块提交一个写操作的提案。随后，Raft模块通过HTTP网络模块转发到集群的多数节点持久化，状态变成已提交，etcd server从Raft模块获取已提交的日志条目，传递给Apply模块，Apply模块通过MVCC模块执行命令内容，更新状态机。\netcd写入的value大小默认不超过1.5MB\nQuota模块 etcd的db文件配额只有2G，当超过时，整个集群变成只读，无法写入，这个限制主要是为了保证etcd的性能，官方建议最大不超过8G，不禁用配额。\n当etcd server收到写请求时，会先检测db大小 + 上请求时的key-value大小，判断是否超过配额，如果超过，会产生一个NO SPACE的告警，并通过Raft日志同步给其他节点，告知db无空间，并将告警持久化存储到db中，使得集群内其他节点也都拒绝写入，变成只读。\n如果达到配额后，再次去修改配额大小，还需要额外发送一个取消警告，消除NO SPACE告警带来的影响。\n其次是要检测etcd的压缩配置，如果没有机制去回收旧版本，会导致内存和db大小一直膨胀。\n回收机制有多种，常见的是保留最近一段时间的历史版本，给旧版本数据打上free标记，后续新写入的数据直接覆盖而无需申请新空间。另一种回收机制是回收空间，减少db大小，但会产生碎片，产生碎片就需要整理，旧的db文件数据会写入新的db文件，对性能影响较大。\nKVServer模块 写请求在通过Raft算法实现节点间的数据复制前，由KVServer进行一系列的检查。\n 限速判断，保证集群稳定，避免雪崩。如果Raft模块已提交的日志索引(committed index)比已应用到状态机的日志索引(applied index)超过了5000。 尝试获取请求中的鉴权信息，若使用了鉴权，则判断请求中的密码、token是否正确。 检查写入的包大小是否超过默认的1.5MB。 通过检查后，生成一个唯一的ID，并将该请求关联到一个对应的消息通知channel，向Raft模块发起一个提案，之后KV Server会等待写请求的返回，写入结果通过消息通知channel返回，或者超时，默认超时时间是7秒。  WAL模块   Raft模块收到提案后，如果当前节点是Follower节点，则转发给Leader，只有Leader才能处理写请求。\n  Leader收到提案后，通过Raft模块输出待转发给Follower节点的消息和待持久化的日志条目，日志条目记录了写操作的内容。\n  Leader节点从Raft模块获取到以上消息和日志条目后，将写请求提案消息广播给集群各个节点，同时需要把集群Leader任期号、投票信息、已提交索引、提案内容持久化待一个WAL日志文件中，用于保证集群一致性、可恢复性。\n  当一半以上节点持久化此日志条目后，Raft模块通过channel告知etcd server模块，写请求提案已被超半数节点确认，提案状态转为已提交，从channel中取出提案内容，添加到FIFO队列中，等待Apply模块顺序、异步依次执行提案内容。\n  WAL持久化机制：先将Raft日志条目内容序列化后保存到WAL记录的Data字段，计算Data的CRC值，设置Type为EntryType，组成一个完成的WAL记录，最后记录WAL记录的长度，顺序写入WAL长度，再写入记录内容，调用fsync持久化到磁盘。\n主要作用是为了保证etcd重启时，重放日志提案，保证命令的执行。\nWAL日志结构 LenField -------- 数据长度 Type ------------ WAL记录类型，有5种，分别是文件元数据记录、日志条目记录、状态信息记录、CRC、快照 CRC ------------- 校验码 Data ------------ WAL记录内容 Raft日志条目Data的结构 Term ----------- uint64，Leader任期号，随Leader选举增加 Index ---------- 日志条目的索引，单调递增，同时也用于确保幂等操作 Type ----------- 日志类型，如 普通的命令日志还是集群配置变更日志 Data ----------- 提案内容 Apply模块 从FIFO队列取出提案执行，同时会保证可靠性，包括crash重启，消费消息的幂等，防止重复提交。通过consistent index字段存储系统当前已执行过的日志条目索引 + 日志条目中的Index字段保证幂等。\n  从FIFO队列取出提案后，如果之前没被执行过，则进入到MVCC模块\netcd再进行更新时会为key生成一个版本号，版本号的生成单调递增，启动时默认是1，如果有持久化的数据，则读取boltdb中的数据的最大值，作为当前版本号，版本号格式{[版本号],[子版本号]}\n  写操作执行时，MVCC会递增当前版本号作为key的版本号，存储到treeIndex中。\n  将新生成的版本号做为key，写操作对应的value写入boltdb，每个key对应一个bucket，boltdb的value包括写操作的key名，key创建时的版本号，最后一次修改时的版本号，key自身修改的次数，写操作的value值，租约信息。将这些信息序列化成一个二进制数据，写入boltdb中，此时还只在boltdb的内存bucket buffer中。此时如果有读请求，会优先从bucket buffer中读取，其次才从boltdb读。\nboltdb不是每个value都是直接写到磁盘的，因为key递增，会顺序写入，所以会合并多个写事务请求，异步(默认每个100ms)，批量事务一次性提交，提高吞吐量。\n  Raft协议 Raft协议保证了etcd在节点故障、网络分区等异常场景下的高可用和数据强一致性。\n为了避免单点故障，常见的多副本复制方案有两种：主从复制和去中心化复制。\n除了复制方案，另一种是共识算法如Paxos或Raft。\nRaft将共识算法拆分成三个子问题：\n Leader选举，Leader故障后集群快速选出新Leader。 日志复制，集群只有Leader能写入日志，Leader负责复制日志到Follower节点，强制Follower节点与自己保持一致。 安全性，一个任期内集群只能产生一个Leader、已提交的日志条目在发生Leader选举时，一定会存在更高任期的新Leader日志中、各个节点的状态机应用的任意位置的日志条目内容一样。  Leader选举 Raft协议定义集群中节点的状态，任何时刻，每个节点肯定处于某一状态\n  Follower：同步Leader收到的日志，etcd启动时的默认状态。\n  Candidate：可以发起Leader选举。当Follower接收Leader的心跳信息超时时，转为此状态，并立即发起竞选Leader投票，自增任期号，投票给自己，并向其他节点发送竞选Leader投票的消息，当获得多数节点的支持后，即可变成Leader节点。\n节点收到竞选消息后可能出现两种情况：\n 节点判断发出竞选消息的节点的数据至少和自己一样新，并且任期号大于自己的，并且自己还没投票给其他节点，就可以投票给发出竞选消息的节点了 节点此时也发出了竞选，并投票给自己，此时将拒绝投票给发出竞选消息的节点，相互等待直至竞选超时，开启新一轮竞选。Raft引入随机数来保证各个节点发起各个竞选的时间不一致。  3.4版本后，节点在进入Candidate前，会先进入PreCandidate状态，此时不自增任期号，而是直接发起预投票，若可以获得多数节点支持，才能变成Candidate，自增任期号发起选举。\n  Leader：唯一性，拥有同步日志的特权，通过定时广播心跳给Follower节点，以维持Leader身份。Leader节点有一个任期号，充当Raft算法的逻辑时钟，可以比较各个节点的数据新旧、识别过期Leader等。\n默认的心跳间隔时间是100ms，默认的竞选超时时间是1000ms，当超时时间大于竞选时间时，节点从Follower状态转为Candidate状态。\n当现有Leader发现新的Leader任期号后，就会转为Follower节点；当现有Leader因为crash重启后，会先变成Follower，若此时无法与其他节点通信，也会进入选举流程，不过会先转为PreCandidate发起预投票，避免因数据落后且在Candidate状态因自增任期号，在恢复通信后造成选举异常。\n  日志复制 Leader节点通过NextIndex字段标识要发送给Follower节点的下一个日志条目索引，MatchIndex字段标识Follower节点已复制的最大日志条目索引，每个节点都有。\n一个日志条目被确认为已提交的前提是它需要被Leader同步到一半以上节点。\n 客户端发送写操作消息msg给etcd，进入Leader节点的Raft模块 Leader节点的etcd server模块通过channel从Raft模块获取Ready结构，通过HTTP协议的网络模块将追加日志条目消息广播给Follower，并同时将待持久化的日志持久化到WAL文件中，最后将日志条目追加到稳定的Raft日志存储中。Raft日志存储在内存中，即使丢失也可以通过WAL文件重建。 各个Follower收到追加日志条目消息，通过安全检查后，会持久化消息到WAL日志中，并将消息追加到Raft日志存储，随后向Leader回复一个应答追加日志条目的消息，告知Leader当前已复制的日志最大索引。 Leader收到应答追加日志条目消息后，将Follower回复的已复制日志最大索引更新到追踪Follower进展的MatchIndex字段，根据该字段信息，计算出一个位置，如果这个位置已经被一半以上节点持久化，那么这个位置之前的日志条目都可以被标记成已提交。Leader通过心跳信息告知已提交的日志索引位置给Follower。 各个节点的etcd server模块，通过channel从Raft模块获取已提交的日志条目，应用日志条目内容到存储状态机，返回给客户端。  安全性 Raft通过给选举和日志复制增加规则，保证当Leader crash后，能在众多Follower中选举出有最新日志条目的Follower成为新Leader。\n当节点收到选举投票时，需检查发出选举消息的节点的最后一条日志的任期号，若小于自己则拒绝投票，如果相同，日志比自己短，也拒绝投。节点需将投票信息持久化，防止异常重启后再投票给其他节点。\n在复制上，通过Leader完全特性、只附加原则和日志匹配保证Leader提交消息并广播给其他节点后crash，这条新消息不会被其他节点删除的问题和各个节点的同Raft日志位置含有相同的日志条目。\n 完全特性：某个日志条目在某个任期号中已被提交，那么这个条目必然出现在更大任期号的所有Leader中 只附加原则：Leader只能追加日志条目，不能删除 日志匹配：追加日志时会进行一致性检查，Leader发送追加日志的消息时，会把新的日志条目紧接之前的条目的索引位置和任期号包含在里面，Follower节点会检查相同索引位置的任期号是否与Leader一致，一致才追加。  当Follower日志和Leader冲突时，会导致两者的日志不一致，此时Leader会强制Follower直接复制自己的日志来解决，因此在Follower中冲突的日志条目会被Leader的日志覆盖，Leader会记录Follower复制进度的nextIndex，如果Follower在追加日志时一致性检查失败，就会拒绝请求，此时Leader会减小nextIndex值并重试，最终在某个位置让Follower跟Leader一致。尽管WAL日志模块只能追加，对于那些想要删除的持久化日志条目，WAL模块确实没有删除，当发现一个raft log index位置上有多个日志条目时，会通过覆盖的方式，将最后写入的日志条目追加到raft log中，通过覆盖实现删除。\n鉴权 etcd鉴权体系由控制面和数据面组成。\n控制面：通过etcdctl和鉴权API动态调整认证、鉴权规则，AuthServer收到请求后，为了确保各个节点间鉴权元数据一致性，会通过Raft模块进行数据同步。当对应的Raft日志条目被集群半数以上节点确认后，Apply模块通过鉴权存储AuthStore模块，执行日志条目内容，将规则存储到boltdb的鉴权表里。\n数据面：由认证和授权组成。目前有两种认证机制：密码认证和证书认证。通过认证后，在访问MVCC模块前，还需要进行授权，检查client是否有权限操作你请求的数据路径，使用的是RBAC机制。\n认证   密码认证：etcd鉴权模块使用bcrpt库的blowfish算法，基于明文密码、随机分配的salt、自定义的cost、迭代多次计算得到一个hash值，并将加密算法版本、salt值、cost、hash值组成一个字符串，作为加密后的密码。以用户名为key，用户名、加密后的密码作为value，存储到boltdb的authUsers bucket里。\n验证密码成功后，返回一个token给client，后续请求携带此token而无需进行密码校验了。默认的token过期时间是5分钟，仅在开发或测试环境中使用。正式环境一般使用JWT。\n  证书认证：类似HTTPS，证书认证在稳定性和性能上都优于密码认证。\n  授权 使用RBAC授权模型。\n租约 Lease etcd通过Lease实现活性检测，可以检测各个客户端的存活能力，业务client需要定期向etcd发送心跳请求汇报讲课状态，属于主动型上报，让etcd server保证在约定的有效期内，不删除client关联到此lease上的key-value，若未在有效期内续租，就会删除Lease和其关联的key-value。\n基于Lease的TTL特性，可以解决类似Leader选举、Kubernetes Event自动淘汰、服务发现场景中故障节点自动剔除等问题。\n检查Lease是否过期、维护最小堆、针对过期Lease发起revoke操作，都由Leader节点负责。\n创建Lease时，etcd会保存Lease信息到boltdb的lease bucket中，与该Lease关联的节点需要定期发送KeepAlive请求给etcd server续约Lease。\netcd在启动时，会创建Lessor模块，通过两个异步任务管理Lease：\n 一个RevokeExpiredLease任务定时检测是否有过期的Lease，使用最小堆管理Lease，每隔500ms进行检查，发起撤销过期Lease的操作，获取到LeaseId后通知整个集群删除Lease和关联的数据；过期默认淘汰限速是每秒1000个。 另一个是CheckpointScheduledLease，定时(默认5min)触发更新Lease的剩余到期时间的操作，定期批量将Lease剩余的TTL基于Raft Log同步给Follower节点，更新其LeaseMap中剩余的TTL信息；另外，Leader节点收到KeepAlive请求后，重置TTL，并同步给Follower节点进行更新。  Lease续约是一个高频率的操作，当完成Lease的创建和节点数据的关联，在正常情况下，节点存活时，需要定时发送KeepAlive请求给etcd续期健康状态的Lease。TTL时间过长会导致节点异常无法从etcd中删除，过短会导致client频繁发送续约请求。另外，Lease的数目可能会很大。为了解决这个问题，etcd在v3版本上，一个是采用grpc解决连接复用问题，减少连接数，另一个是当有不同的key的TTL相同，会复用同一个Lease，减少Lease数目。\nLease最小的TTL时间是 比选举的时间长，默认是2s\nMVCC MVCC特性由treeIndex、Backend/boltdb组成，实现对key-value的增删查改功能。MVCC模块将请求划分为两个类别，分别是读事务（ReadTxn）和写事务（WriteTxn）。读事务负责处理range请求，写事务负责put/delete操作。\nTreeIndex中key的版本号与boltdb中的value关联。\nTreeIndex模块 基于内存版本的B-tree实现Key索引管理，保存用户key与版本号revision的映射关系。之所以使用B-tree，是因为etcd支持范围查询，B树每个节点可以容纳比较多的数据，树高度低，查找次数少，so不用哈希表或平衡二叉树。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  type keyIndex struct { key []byte // key值 \tmodified revision // 最后一次修改key时的etcd版本号 \tgenerations []generation // 保存一个key若干代版本号信息，每代包含对key的多次修改版本号列表 } type generation struct { ver int64 // key的修改次数  created revision // generation结构创建时的版本号  revs []revision // 每次修改key时的revision追加到此数组 } type revision struct { main int64 // 全局递增主版本号，随put/txn/delete事务递增，一个事务内的key main版本号一致，空集群时启动时默认为1  sub int64 // 事务内的子版本号，从0开始随事务内put/delete递增 }   Backend/boltdb模块 负责etcd的key-value持久化存储，主要由ReadTx、BatchTx、Buffer组成，ReadTx定义了抽象的读写事务接口，BatchTx在ReadTx之上定义了抽象的写事务接口，Buffer是数据缓存区。Backend支持多种实现，当前使用boltdb，基于B+ tree实现，支持事务的key-value嵌入式数据库。\nvalue的数据结构，版本号格式{main, sub}\n1 2 3 4 5 6  key：用户的key value：用户的value create_revision：key创建时的版本号，与treeIndex中generate的created对应 mod_revision：key最后一次修改时的版本号，put操作时的全局版本号+1作为该值 version：key的修改次数，每次修改时，与treeIndex中generate的ver值+1 lease：   一般情况下为了etcd的写性能，默认堆积的写事务数大于1万才在事务结束时同步持久化，由backend的一个goroutine完成，通过事务批量提交，定时将boltdb页缓存中的脏数据提交到持久化存储磁盘中。\n创建/更新操作  在treeIndex中获取key的KeyIndex信息 填充boltdb的value数据，写入新的key-value到blotdb和buffer中 创建/更新KeyIndex到treeIndex中 backend异步事务提交，将boltdb中的数据持久化到磁盘中  查询操作 创建一个读事务对象(TxnRead / ConcurrentReadTx)，全量拷贝当前写事务未提交的buffer数据，读不到则从boltdb中查询\n删除操作 删除操作是软删除，原理类似更新，会在被删除的key版本号追加删除标志t，对应的boltdb value也变成了只包含用户key的KeyValue结构，treeIndex模块也会给此key的KeyIndex结构追加一个空的generation对象，标识此索引对应的key被删除，当查询时发现其存在空的generation对象，并且查询的版本号大于等于被删除的版本号时，返回空。\n删除key时会生成events，Watch模块会根据key的删除标识，生成对应的Delete事件；或者当重启etcd，遍历boltdb中的key构建treeIndex内存树时，未这些key上次tombstone标识。\n真正删除treeIndex中的KeyIndex、boltdb的value是通过压缩组件异步完成，之所以要延迟删除，一个是为了watcher能够有相应的处理，另一个是减少B tree平衡影响读写性能。\nWatch 客户端订阅etcd的某个key，当key发生变化时，客户端能够感知。这也是Kubernetes控制器的工作基础。\nclient获取事件的方式 V2版本：使用轮询推送，每一个watcher对应一个TCP连接，client通过HTTP/1.1协议长连接定时轮询server，获取最新数据变化事件。但是大量的轮询会产生一定的QPS，server端会消耗大量的socket、内存等资源。\nV3版本：使用流式推送，因为使用的是基于HTTP/2的gRpc协议，实现了一个TCP连接支持多gRPC stream，一个gRPC stream又支持多个watcher，降低了系统资源的消耗。\n事件的存储和保留 V2版本：滑动窗口，使用环形数组存储历史事件版本，当key被修改后，相关的事件就会被添加到数组中来，若超过一定容量（默认1000），则会淘汰旧事件，容易导致事件丢失，当事件丢失时，client需要获取最新的版本号才能继续监听，查询成本比较大。\nV3版本：MVCC，将事件保存到boltdb中，持久化到磁盘中，通过配置压缩策略控制历史版本数。\n版本号是etcd的逻辑时钟，当client因网络等异常连接断开后，通过版本号可以从server的boltdb获取错过的历史事件，而无需全量同步，它是etcd Watch机制数据量增量同步的核心。\n事件推送机制  \nclient对每一个key发起的watch请求，etcd的gRPCWatchServer收到watch请求后，会创建一个serverWatchStream，它负责接收client的gRPC Stream的create/cancel watcher请求（recvLoop goroutine)，并将从MVCC模块接收的watch事件转发给client（sendLoop goroutine）\n当serverWatchStream收到create watcher请求后，serverWatchStream会调用MVCC模块的WatchStream子模块分配一个watcher id，并将watcher注册到MVCC的WatchableKV模块。\nwatchableStore将watcher划分为synced / unsynced / victim三类\n synced watcher：如果创建的watcher未指定版本号或版本号为0或指定版本号大于etcd server当前的最新版本号，那它就会保存在 synced watcherGroup中，表示此类watcher监听的数据都已经同步完毕，等待新的变更。 unsynced watcher：如果创建的watcher指定的版本号小于etcd server当前最新版本号，那它就会保存在 unsynced watcherGroup中，表示此类watcher监听的数据还未同步完成，落后于当前最新数据的变更，正在等待同步。 victim：当接收watch事件的channel的buffer满了，该watcher会从synced watcherGroup中删除，然后保存到victim的watcherBatch中，通过异步机制重试保证事件可靠性。  当etcd启动时，WatchableKV模块会运行syncWatcherLoop和syncVictimsLoop goroutine，分别负责不同场景下的事件推送。\n syncWatcherLoop：遍历unsynced watcherGroup中的每个watcher，获取key的所有历史版本，转成事件，推送给接收的channel，完成后将watcher从unsynced watcherGroup转移到synced watcherGroup。 syncVictimsLoop：遍历victim watcherBatch，尝试将堆积的事件再次推送到watcher的接收channel中，若推送失败则再次加入等待重试；若推送成功，watcher监听的最小版本号小于当前版本号，则加入unsynced watcherGroup中，大于则加入synced watcherGroup中。  高效的找到监听key的所有watcher 由于watcher可以监听key范围、key前缀，\n当收到创建watcher请求时，会把watcher监听的key的范围插入到区间树中，当产生一个事件时，etcd首先会从map中找到是否有watcher监听了该key，其次还要从区间树中找到与key相交的所有区间，得到所有watcher。\n事务 etcd事务API由IF、Then、Else语句组成。\netcd通过WAL日志 + consistent index + boltdb保证原子性；\nWAL日志+boltdb保证持久性；\n数据库和业务程序保证一致性；\n通过MVCC机制实现读写不阻塞，解决隔离性的脏读问题；MVCC快照读解决隔离性的不可重复读问题；MVCC版本号实现冲突检测机制，在串行提交事务时保证读写的数据都是最新的，未被他人修改。\nboltdb 磁盘布局 boltdb文件存放在etcd数据目录下的member/snap/db文件，etcd启动时，会通过mmap机制将db文件映射到内存，后续从内存中快速读取文件中的数据。\n![](https://github.com/Nixum/Java-Note/raw/master/picture/etcd boltdb文件布局.png)\n开头两个是固定的db元数据meta page；freeList page记录db中哪些页是空闲的，可使用的；\n写操作时，会先打开db文件并增加文件锁，防止其他进程以读写模式打开后操作meta和free page，导致db文件损坏；然后通过mmap机制将db文件映射到内存中，并读取两个meta page到db对象实例，校验meta page的magic version、checksum是否有效，若两个meta page都无效，说明db文件损坏，将异常退出。\n执行put请求前会先执行bucket请求，先根据meta page中记录root bucket的root page，按照B+树的查找算法，从root page递归搜索到对应叶子节点page面，返回key名称，leaf类型；如果leaf类型未bucketLeafFlag，且key相等，说明已经创建过，不允许bucket重复创建，否则往B+树种添加一个flag为bucketLeafFlag的key，key的名称为bucket name，value为bucket结构；\n执行完bucket请求，就会进行put请求，跟创建bucket类似，根据子bucket的root page，从root page递归搜索此key到leaf page，如果没有找到，则在返回的位置插入新key和value，插入位置的查找使用二分法。\n当执行完一个put请求时，值只是更新到boltdb的内存node数据结构里，此时还未持久化。当代码执行到tx.commit api时，才会将node内存数据结构中的数据持久化到boltdb中。一般是经过 删除节点后重平衡操作、分裂操作、持久化freelist、持久化dirty page、持久化meta page。\n压缩 由于更新和删除都会增加版本号，内存占用和db文件就会越来越大，当达到etcd OOM和db大小的最大配额时，最终不可写入，因此需要适合的压缩策略，避免db大小增长失控。\n压缩是使用Compact接口，可以设置自动也可通过业务服务手动调用，压缩时首先会检查请求的版本号rev是否被压缩过，然后更新当前server已压缩的版本号，并将耗时的压缩任务保存在FIFO队列种异步执行。压缩任务执行时，首先会压缩treeIndex模块中的keyIndex索引，其次会遍历boltdb中的key，删除已废弃的key，遍历boltdb时会控制删除的key数100个，每批间隔10ms，分批完成删除操作。\n压缩过程中，compact接口会持久化存储当前已调度的压缩版本号到boltdb，保证当发生crash后各个节点间的数据一致性。\n压缩时会保留keyIndex中的最大版本号(为了保证key仍存在)，移除小于等于当前压缩的版本号，通过一个map记录treeIndex中有效的版本号返回给boltdb模块使用。\n遍历删除boltdb中的数据后，db文件不会变小，而是通过freelist page记录哪些页是空闲的，覆盖使用\n使用参数--auto-compaction-retention '[0|1]'0表示关闭自动压缩，1 表示开启自动压缩策略，使用参数--auto-compaction-mode '[periodic|revision]'，periodic表示周期性压缩，revision表示版本号压缩\n  时间周期性压缩：只保留最近一段时间写入的历史版本，periodic compactor会根据设置的压缩时间间隔，划分为10个区间，通过etcd MVCC模块获取当前的server版本号，追加到rev数组中，通过当前时间减去上一次执行compact操作的时间，如果间隔大于设置的压缩时间，则取出rev数组首元素，发起压缩。\n  版本号压缩：保留最近多少个历史版本，revision compactor会根据设置的保留版本号数，每隔5分钟定时获取当前server最大版本号，减去想保留的历史版本数，得到要压缩的历史版本，发起压缩。\n  参考 极客时间 - etcd实战课\n","date":"2020-09-23T00:00:00Z","permalink":"http://nixum.cc/p/etcd%E5%92%8Czookeeper/","title":"etcd和ZooKeeper"},{"content":"[TOC]\nBIO 特点\n BIO是同步阻塞的，以流的形式处理，基于字节流和字符流 每个请求都需要创建独立的线程，处理Read和Write 并发数较大时，就算是使用了线程池，也需要创建大量的线程来处理 连接建立后，如果处理线程被读操作阻塞了，那就阻塞了，只能等到读完才能进行其他操作  以基于TCP协议的Socket，编写服务端Demo\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55  package com.nixum.bio; import java.io.InputStream; import java.io.PrintWriter; import java.net.ServerSocket; import java.net.Socket; import java.util.Date; import java.util.concurrent.ExecutorService; import java.util.concurrent.Executors; public class BIOServer { public static void main(String[] args) throws Exception { ExecutorService newCachedThreadPool = Executors.newCachedThreadPool(); //创建ServerSocket  ServerSocket serverSocket = new ServerSocket(8080); while (true) { // 主线程负责处理监听  final Socket socket = serverSocket.accept(); // 创建线程处理请求  newCachedThreadPool.execute(() -\u0026gt; { handler(socket); }); } } public static void handler(Socket socket) { try { byte[] bytes = new byte[1024]; //通过socket获取输入流  InputStream inputStream = socket.getInputStream(); //循环的读取客户端发送的数据  while (true) { int read = inputStream.read(bytes); if(read != -1) { System.out.println(\u0026#34;接收到的请求是：\u0026#34; + new String(bytes, 0, read)); } else { break; } // 响应给客户端  PrintWriter out = new PrintWriter(socket.getOutputStream()); out.println(\u0026#34;服务端接收到请求了，响应时间：\u0026#34; + new Date()); out.flush(); } } catch (Exception e) { e.printStackTrace(); } finally { try { // 关闭连接  socket.close(); }catch (Exception e) { e.printStackTrace(); } } } }   客户端Demo：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  package com.atguigu.bio; import java.io.BufferedReader; import java.io.InputStreamReader; import java.io.PrintStream; import java.net.Socket; import java.util.Scanner; public class BIOClient { public static void main(String[] args) throws Exception { Socket client = new Socket(\u0026#34;127.0.0.1\u0026#34;, 8080); Scanner scan = new Scanner(System.in); PrintStream out = new PrintStream(client.getOutputStream()); BufferedReader bufferReader = new BufferedReader(new InputStreamReader(client.getInputStream())); while (true) { if (scan.hasNext()) { // 键盘输入，并发送请求  String str = scan.next(); if (\u0026#34;exit\u0026#34;.equalsIgnoreCase(str)) { break; } out.println(str); out.flush(); // 接收服务端的响应  System.out.println(bufferReader.readLine()); } } client.close(); } }   NIO 特点：\n  同步非阻塞，通过缓冲区进行缓冲增加处理的灵活性，当某一线程里没有数据可用时，则去处理其他事情，保证线程不阻塞\n  由三个部分组成：Channel、Buffer、Selector\n  数据总是从Channel读到Buffer中，或从Buffer写到Channel中，事件 + Selector监听Channel，实现一个线程处理多个操作。每一个Channel会对应一个Buffer、一个Selector对应多个Channel，Selector通过事件决定使用哪个Channel\n  Buffer  存储数据时使用，本质是一个数组 清除时本质只是把下列各个属性恢复到初始状态，数据没有被正常的擦除，而是由后面的数据覆盖 将数据读入Buffer后，需要调用flip方法进行反转后才能将Buffer里的数据写出来 可以put各种类型的数据进byteBuffer后，flip后，需要按顺序和类型进行get操作，否则会抛异常  重要属性 1 2 3 4 5 6 7 8  // 下一个要读或写的位置索引 private int position = 0; // 缓冲区的当前终点，\u0026lt;= limit，读写时位置索引不能超出limit private int limit; // 缓冲区容量 private int capacity; // 标记，标记后用于重新恢复到的位置 private int mark = -1;   常用子类 每个基本类型都有对应的Buffer，比如CharBuffer、IntBuffer、DoubleBuffer、ByteBuffer（最常用）\nChannel  作用类似流，但流是单线的，只能读或只能写，而Channel是双向的，可以同时进行读写 可异步 通常与Buffer配合使用，也可以使用Buffer数组，当一个buffer存满时会取下一个buffer取处理  常用子类   FileChannel专门处理文件相关的数据（从FileIn/OutputStream的getChannel()方法得到）\n  ServerSocketChannel和SocketChannel用于处理TCP连接的数据\n  DatagramChannel用于处理UDP连接的数据\n  读写文件的Demo，比如从一个文件里读出数据并写入另一个文件\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  package com.nixum.nio; import java.io.FileInputStream; import java.io.FileOutputStream; import java.nio.ByteBuffer; import java.nio.channels.FileChannel; public class CopyFileAToB { public static void main(String[] args) throws Exception { FileInputStream fileInputStream = new FileInputStream(\u0026#34;first.txt\u0026#34;); FileChannel readFileChannel = fileInputStream.getChannel(); FileOutputStream fileOutputStream = new FileOutputStream(\u0026#34;second.txt\u0026#34;); FileChannel writeFileChannel = fileOutputStream.getChannel(); // 只设置成1024的容量，通过循环 + clear覆盖重写  ByteBuffer byteBuffer = ByteBuffer.allocate(1024); while (true) { byteBuffer.clear(); int read = readFileChannel.read(byteBuffer); if (read == -1) { break; } // 写操作前需要先flip  byteBuffer.flip(); writeFileChannel.write(byteBuffer); } // close  fileInputStream.close(); fileOutputStream.close(); } }   Selector  一个线程处理多个连接，就是靠Selector，Channel需要事先注册到Selector上，Selector根据事件选择Channel进行处理。实际上是一个发布订阅模型，通过事件触发 只有真正有读写事件时才会进行读写，就不用为每个连接都创建一个线程了 避免多线程上下文切换的开销 selectionKey，可以理解为触发selector的事件，有4种，OP_ACCEPT：有新连接产生，一般用于服务端建立连接、OP_CONNECT：连接已建立，一般用于客户端建立连接、OP_READ：读操作、OP_WRITE：写操作  NIO基本使用Demo，服务端，也可以不使用Selector，但这样就跟BIO没什么差别了\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80  package com.nixum.nio; import java.net.InetSocketAddress; import java.nio.ByteBuffer; import java.nio.channels.*; import java.util.Iterator; import java.util.Set; public class NIOServer { private ServerSocketChannel serverSocketChannel; private Selector selector; public NIOServer(int port) { try { serverSocketChannel = ServerSocketChannel.open(); selector = Selector.open(); serverSocketChannel.socket().bind(new InetSocketAddress(port)); // 需要显式设置为非阻塞  serverSocketChannel.configureBlocking(false); // 注册serverSocketChannel，触发事件为 OP_ACCEPT，用于建立连接  serverSocketChannel.register(selector, SelectionKey.OP_ACCEPT); } catch (Exception e) { e.printStackTrace(); } } public void run() throws Exception { while (true) { // 等待1s获取事件，这样一次性可以获取多个事件进行处理，如果没有则继续  if(selector.select(1000) == 0) { System.out.println(\u0026#34;这1秒内没有收到数据\u0026#34;); continue; } // 监听到事件发生， 获取发生的事件集合  Set\u0026lt;SelectionKey\u0026gt; selectionKeys = selector.selectedKeys(); Iterator\u0026lt;SelectionKey\u0026gt; keyIterator = selectionKeys.iterator(); // 判断事件的类型  while (keyIterator.hasNext()) { SelectionKey key = keyIterator.next(); // 触发的是OP_ACCEPT事件  if (key.isAcceptable()) { connectionHandler(selector, serverSocketChannel); } // 触发OP_READ事件  if (key.isReadable()) { readHandler(key); } // 将处理完的事件集合移除，防止重复操作  keyIterator.remove(); } // 处理完事件集合  } } private void connectionHandler(Selector selector, ServerSocketChannel serverSocketChannel) throws Exception { // 建立连接  SocketChannel socketChannel = serverSocketChannel.accept(); // 将SocketChannel 设置为非阻塞  socketChannel.configureBlocking(false); // 建立连接后，将socketChannel注册到selector，初始化一个Buffer用来装数据，发送OP_READ给selector，当selector收到OP_READ事件后，就会使用该socketChannel处理该Buffer  socketChannel.register(selector, SelectionKey.OP_READ, ByteBuffer.allocate(1024)); } private void readHandler(SelectionKey key) throws Exception { // 获取处理该事件的channel  SocketChannel channel = (SocketChannel) key.channel(); // 获取该channel关联的Buffer  ByteBuffer buffer = (ByteBuffer) key.attachment(); // 将channel里的数据读到buffer里  channel.read(buffer); System.out.println(\u0026#34;接收到请求是：\u0026#34; + new String(buffer.array())); } public static final int PORT = 8080; public static void main(String[] args) throws Exception{ NIOServer server = new NIOServer(PORT); server.run(); } }   客户端，这里没有使用selector，直接使用socketChannel连接：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  package com.nixum.nio; import java.net.InetSocketAddress; import java.nio.ByteBuffer; import java.nio.channels.SocketChannel; public class NIOClient { public static void main(String[] args) throws Exception { SocketChannel socketChannel = SocketChannel.open(); socketChannel.configureBlocking(false); InetSocketAddress inetSocketAddress = new InetSocketAddress(\u0026#34;127.0.0.1\u0026#34;, 8080); if (!socketChannel.connect(inetSocketAddress)) { // 这里的例子是为了说明如果连接和处理是异步的，非阻塞的  while (!socketChannel.finishConnect()) { System.out.println(\u0026#34;当连接还没完成时，会循环打印，但此时也在执行连接，请稍等\u0026#34;); } } // 如果要使用selector，则在此处将socketChannel注册进selector，剩下处理跟上面类似  String str = \u0026#34;hello, world\u0026#34;; ByteBuffer buffer = ByteBuffer.wrap(str.getBytes()); // 根据str的大小wrap一个buffer  // 发送请求  socketChannel.write(buffer); } }   线程模型 传统阻塞IO模型 典型的BIO例子，有一个ServiceSocket在监听端口，一个线程处理一个连接，监听端口、建立连接，read操作、业务处理、write操作这一整个过程都是阻塞的\nReactor模型 reactor其实就是针对传统阻塞IO模型的缺点，将上述操作拆分出来异步处理，通过事件通知，由一个中心进行分发，本质就算IO复用 + 线程池，甚至单线程 + 消息队列也可以\n1. 单Reactor单线程  单Reactor单线程模型 \n Acceptor实际上也是一个Handler，只是处理的事件不同，当Reactor收到(select)连接事件时调用 当Reactor收到(select)非连接事件，比如读事件、写事件、处理其他业务的事件等，会起一个handler来处理 当Handler处理完当前事件后，将下一次要处理的事件和相关参数丢给Reactor进行select和dispatch 单线程模型，天然是线程安全的，但是当handler处理过慢时就会造成事件堆积，阻塞主线程(Reactor)，处理能力下降，因此要求handler处理尽可能的快。 异常处理要小心，否则会导致整个线程垮掉 比如上面NIO的例子就是这个模型，当业务复杂时，也可将handler抽出。不同的Handler类实现不同的业务处理，再配合对象池实现复用  2. 单Reactor多线程  单Reactor多线程模型 \n 在单Reactor单线程模型的基础上，因为Handler的处理流程相对固定，就将比较耗时的业务处理包装成任务交由线程池处理，加快Handler的处理速度 实际上如果只是在Handler处将业务逻辑交给线程池去做，在同步等待结果，只是一种伪异步，本质上Handler还是要等任务执行完才能执行send操作。优化的方法是先将Handler存起来，把业务处理提交给线程池后，就结束handler的执行了，这样就能把主线程释放出来，处理其他事件。当线程池里的任务执行完，只需将结果、handlerId、事件交由Reactor，Reactor根据事件和HandlerId找到对应的Handler去响应结果就可以了。 由于业务处理使用了多线程，需要注意共享数据的问题，处理起来会比较复杂，线程安全只存在于Reactor所在的线程 Reactor需要处理的事件变多，高并发下容易出现性能瓶颈  3. 主从Reactor多线程  多Reactor多线程模型 \n 在单Reactor多线程模型的基础上，将Handler下沉处理，通过子Reactor来提高并发处理能力。Acceptor处理连接事件后，将连接分配给SubReactor处理，例如一个连接对应一个SubReactor，SubReactor负责处理连接后的业务处理，可以把这层理解为单Reactor多线程模型的Reactor 由于又多了一层，线程处理更加复杂，同一Reactor下才能保证线程安全，不同Reactor间要注意数据共享问题  Netty 对NIO的包装，简化NIO的使用；实现客户端重连、闪断、半包读写、失败缓冲、网络拥塞和异常流处理；基于主从Reactor多线程模型，事件驱动\nServer端线程模型  Netty线程模型 \nDemo  BossGroup专门处理连接，WorkerGroup专门处理读写 NioEventLoop是一个无限循环的线程，不断的处理事件，每一个NioEventLoop有一个selector，用于监听事件 NioEventLoop内部串行化设计，负责消息的读取 -\u0026gt; 解码 -\u0026gt; 处理 -\u0026gt; 编码 -\u0026gt; 发送 一个NioEventLoopGroup包含多个NioEventLoop，每个NioEventLoop包含一个Selector，一个taskQueue Selector可以注册监听多个NioChannel，每个NioChannel只会绑定在唯一的NioEventLoop上，每个NioChannel都绑定有一个自己的ChannelPipeline 注意如果在一次连接中多次调用ChannelHandlerContext的writeAndFlush响应数据回去时，每次writeAndFlush写出去的数据会整合在一起后才响应回去，即TCP的粘包，接收端只会接收到合并后的数据包，需要特殊处理去拆包 netty中的I/O操作是异步的，如 bind、wirte、connect方法都是返回一个ChannelFeture，可以使用ChannelFeture的sync方法将异步改为同步，或者调用其他方法来判断其状态和结果  服务端\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65  package com.nixum.netty; import io.netty.bootstrap.ServerBootstrap; import io.netty.channel.ChannelFuture; import io.netty.channel.ChannelFutureListener; import io.netty.channel.ChannelInitializer; import io.netty.channel.ChannelOption; import io.netty.channel.EventLoopGroup; import io.netty.channel.nio.NioEventLoopGroup; import io.netty.channel.socket.SocketChannel; import io.netty.channel.socket.nio.NioServerSocketChannel; public class NettyServer { private static final int PORT = 8080; public static void main(String[] args) throws Exception { // 一个BossGroup线程池处理连接请求，设置了一个线程  EventLoopGroup bossGroup = new NioEventLoopGroup(1); // 一个WorkerGroup线程池进行业务处理，默认设置的线程数是 CPU核数 * 2  EventLoopGroup workerGroup = new NioEventLoopGroup(); try { // 创建服务端的启动对象，配置参数  ServerBootstrap bootstrap = new ServerBootstrap(); // 初始化设置  bootstrap.group(bossGroup, workerGroup) // 设置两个线程组  .channel(NioServerSocketChannel.class) // 指定NioSocketChannel为传输channel  // .localAddress(new InetSocketAddress(PORT)) // 可以在这里设置监听端口或者设置初始化配置后使用bind()方法设置  .option(ChannelOption.SO_BACKLOG, 128) // 设置线程队列得到连接个数  .childOption(ChannelOption.SO_KEEPALIVE, true) //设置保持活动连接状态  .handler(null) // 该handler对应bossGroup, childHandler对应 workerGroup  .childHandler(new ChannelInitializer\u0026lt;SocketChannel\u0026gt;() { // 当一个连接被接收时，给channelPipeline添加一个Handler  @Override protected void initChannel(SocketChannel ch) throws Exception { // 可以使用一个集合管理 SocketChannel，在推送消息时，可以将业务加入到各个channel 对应的 NIOEventLoop 的 taskQueue 或者 scheduleTaskQueue  // 给workGroup的eventLoop对应的channel设置handler  ch.pipeline().addLast(new NettyServerHandler()); } }); System.out.println(\u0026#34;服务器 is ready...\u0026#34;); // 同步阻塞等待直到绑定完成，并监听端口  ChannelFuture cf = bootstrap.bind(PORT).sync(); // 设置监听器，监听连接事件  cf.addListener(new ChannelFutureListener() { @Override public void operationComplete(ChannelFuture future) throws Exception { if (cf.isSuccess()) { System.out.println(\u0026#34;监听端口 \u0026#34; + PORT + \u0026#34; 成功\u0026#34;); } else { System.out.println(\u0026#34;监听端口 \u0026#34; + PORT + \u0026#34; 成功\u0026#34;); } } }); // 获取cfChannel的CloseFuture，并阻塞当前线程直到其完成  cf.channel().closeFuture().sync(); } finally { bossGroup.shutdownGracefully(); workerGroup.shutdownGracefully(); } } }   服务端业务处理器NettyServerHandler\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63  package com.nixum.netty; import io.netty.buffer.ByteBuf; import io.netty.buffer.Unpooled; import io.netty.channel.Channel; import io.netty.channel.ChannelHandlerContext; import io.netty.channel.ChannelInboundHandlerAdapter; import io.netty.channel.ChannelPipeline; import io.netty.util.CharsetUtil; // netty通过一系列的Handler来实现业务处理，用户自定义Handler需要继承ChannelInboundHandlerAdapter才能使用 public class NettyServerHandler extends ChannelInboundHandlerAdapter { /* 读取客户端发送的请求 ChannelHandlerContext ctx: 上下文对象, 含有管道pipeline, 通道channel, 地址 Object msg: 就是客户端发送的数据 默认Object */ @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception { // 这里假设有需要执行一个非常耗时的任务 \t/* // 解决方案1: 用户程序自定义的普通任务 ctx.channel().eventLoop().execute(() -\u0026gt; { try { Thread.sleep(5 * 1000); ctx.writeAndFlush(Unpooled.copiedBuffer(\u0026#34;hello, 客户端2\u0026#34;, CharsetUtil.UTF_8)); } catch (Exception e) { System.out.println(\u0026#34;发生异常\u0026#34; + e.getMessage()); } }); // 解决方案2: 用户自定义定时任务 -\u0026gt; 该任务是提交到 scheduleTaskQueue中 ctx.channel().eventLoop().schedule(() -\u0026gt; { try { Thread.sleep(5 * 1000); ctx.writeAndFlush(Unpooled.copiedBuffer(\u0026#34;hello, 客户端~(\u0026gt;^ω^\u0026lt;)喵4\u0026#34;, CharsetUtil.UTF_8)); } catch (Exception e) { System.out.println(\u0026#34;发生异常\u0026#34; + e.getMessage()); } }, 5, TimeUnit.SECONDS); */ Channel channel = ctx.channel(); // 将 msg对象转成一个ByteBuf，这里ByteBuf是Netty提供的，不是NIO的ByteBuffer.  ByteBuf buf = (ByteBuf) msg; System.out.println(\u0026#34;客户端发送消息是: \u0026#34; + buf.toString(CharsetUtil.UTF_8)); System.out.println(\u0026#34;客户端地址: \u0026#34; + channel.remoteAddress()); } //数据读取完毕，一般在这里需要对要发送的数据进行编码  @Override public void channelReadComplete(ChannelHandlerContext ctx) throws Exception { // 将数据写入到缓存，并刷新, 响应给客户端  ctx.writeAndFlush(Unpooled.copiedBuffer(\u0026#34;hello, 客户端\u0026#34;, CharsetUtil.UTF_8)); } //处理异常, 一般是需要关闭通道  @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception { ctx.close(); } }   客户端：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39  package com.nixum.netty; import io.netty.bootstrap.Bootstrap; import io.netty.channel.ChannelFuture; import io.netty.channel.ChannelInitializer; import io.netty.channel.EventLoopGroup; import io.netty.channel.nio.NioEventLoopGroup; import io.netty.channel.socket.SocketChannel; import io.netty.channel.socket.nio.NioSocketChannel; public class NettyClient { public static void main(String[] args) throws Exception { EventLoopGroup group = new NioEventLoopGroup(); try { // 创建客户端启动对象，这里使用的是BootStrap  Bootstrap bootstrap = new Bootstrap(); // 初始化设置  bootstrap.group(group) //设置线程组  .channel(NioSocketChannel.class) // 设置客户端通道的实现类  .handler(new ChannelInitializer\u0026lt;SocketChannel\u0026gt;() { @Override protected void initChannel(SocketChannel ch) throws Exception { // 添加Handler  ch.pipeline().addLast(new NettyClientHandler()); } }); // 启动客户端去连接服务器端阻塞直至成功  ChannelFuture channelFuture = bootstrap.connect(\u0026#34;127.0.0.1\u0026#34;, 8080).sync(); // 同上  channelFuture.channel().closeFuture().sync(); } finally { group.shutdownGracefully(); } } }   客户端业务处理Handler\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  package com.nixum.netty; import io.netty.buffer.ByteBuf; import io.netty.buffer.Unpooled; import io.netty.channel.ChannelHandlerContext; import io.netty.channel.ChannelInboundHandlerAdapter; import io.netty.util.CharsetUtil; public class NettyClientHandler extends ChannelInboundHandlerAdapter { // 当通道就绪就会触发该方法  @Override public void channelActive(ChannelHandlerContext ctx) throws Exception { ctx.writeAndFlush(Unpooled.copiedBuffer(\u0026#34;hello, server\u0026#34;, CharsetUtil.UTF_8)); } //当通道有读取事件时，会触发  @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception { ByteBuf buf = (ByteBuf) msg; System.out.println(\u0026#34;服务器回复的消息: \u0026#34; + buf.toString(CharsetUtil.UTF_8)); System.out.println(\u0026#34;服务器的地址: \u0026#34; + ctx.channel().remoteAddress()); } // 异常时的处理  @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception { cause.printStackTrace(); ctx.close(); } }   核心组件 ServerBootstrap和Bootstrap 服务引导类，通过它设置配置（链式调用）和启动服务，ServerBootstrap用于服务端，Bootstap用于客户端\nChannel、ChannelPipeline、ChannelHandlerContext和ChannelHandler  Channel：网络通信的组件，提供异步网络I/O操作，操作都是异步的，会返回一个ChannelFuture实例，通过注册在ChannelFuture上的监听器进行回调操作。NioServerSocketChannel用于TCP服务端、NioSocketChannel用于TCP客户端，NioDatagramChannel用于UDP连接 ChannelHandler：是一个接口，通过实现该接口来注册到ChannelPipeline上进行使用。一般使用其出站和入站的两个适配器如ChannelOutboundHandlerAdapter和ChannelInboundHandlerAdapter或者SimpleChannelInboundHandler和SimpleChannelOutboundHandler ChannelPipeline：保存ChannelHandler的队列，像是责任链模式  每一个Channel对应一个ChannelPipeline，一个ChannelPipeline维护了一个由ChannelHandlerContext组成的双向链表，每一个ChannelHandlerContext关联一个ChannelHandler 入站事件会从链表的head往后传递到最后一个入站handler，出站事件会从链表tail往前传递到最前一个出战handler，两种类型的handler互不干扰   ChannelHandlerContext：上下文，包含一个ChannelHandler，绑定ChannelPipeline和Channel的信息  EventLoopGroup 包含一组EventLoop，默认设置的EventLoop线程数是 CPU核数 * 2，每个EventLoop维护一个Selector\n一般流程  BossGrop本质上是一个NioEventGroup，只包含一个NioEventLoop事件循环的线程。WorkGroup本质上也是一个NioEventGroup，但它包含了 CPU*2 个NioEventEventLoop来处理连接后的业务逻辑。 NioEventLoop是一个死循环，不断的处理事件和消息队列的任务。 初始化时将BossGrop和WorkGroup注册到ServerBootstrap并进行相应的配置(如Channel、ChannelHandler)，之后通过bind()方法绑定端口和ServerSocketChannel后启动。 BossGroup轮询Accept事件，获取事件后接受连接，创建一个新的NioSocketChannel，绑定ChannelPipeline，为ChannelPipeline添加ChannelHandler，注册到WorkGroup上，发送Read事件。 WorkGroup中一个EventLoop轮询Read事件，调用Channel的ChannelPipeline进行处理。 ChannelPipeline中每个节点是一个Context，用Context包装Handler，由Context组成双向链表，节点间通过AbstractChannelHandlerContext 类内部的 fire 系列方法 进行传递，入站方法叫inbound，从head节点开始，出站方法叫outbound，由tail节点开始。 对于耗时的方法，一般丢给线程池处理，如上面Demo中的例子  参考： Java NIO 的前生今世 之四 NIO Selector 详解\n深入浅出NIO之Channel、Buffer\nJava NIO：IO与NIO的区别\n尚硅谷Netty教程\nnetty in action\n","date":"2020-09-22T00:00:00Z","permalink":"http://nixum.cc/p/java-io/","title":"Java IO"},{"content":"[TOC]\nQuartz   分为三个部分：e\n Job\u0026amp;Detial(任务)：定时任务的执行方法，与Trigger配套的 Trigger(触发器)：规定什么时候触发，与Job\u0026amp;Detail配套的 Scheduler(调度器)：单例，把Trigger丢里面由调度器调度，只需要一个Scheduler，配置不同的Trigger；可以理解成类似线程池的东西    原理：ScheduledThreadPoolExecutor线程池 + 通过Object类的wait()和notify()或者Condition类的await()\\signal()进行等待和唤醒、锁保证线程安全 来进行调度\nScheduler有两个调度线程：regular Scheduler Thread（执行常规调度）和Misfire Scheduler Thread（执行错失的任务），Regular Thread 轮询所有Trigger，如果有将要触发的Trigger（用wait和notifyAll实现），则从任务线程池中获取一个空闲线程，然后执行与改Trigger关联的job；Misfire Thraed则是扫描所有的trigger，查看是否有错失的，如果有的话，根据一定的策略进行处理\n  默认是并发的，即如果当前任务没有完成，会自动开一个任务执行\n  注意在分布式集群的情况下，多台机子有相同的定时任务，会出错，此时通过共享数据库的方式实现\nQuartz的解决方案：\nquartz集群分为水平集群和垂直集群，水平集群即将定时任务节点部署在不同的服务器，其最大的问题就是时钟同步问题，若时钟不能同步，则会导致集群中各个节点状态紊乱，造成不可预知的后果；垂直集群则是集群各节点部署在同一台服务器，时钟同步自然不是问题，但存在单点故障问题，服务器宕机会严重影响服务的可用性\n在各个节点会上报任务，存到数据库中，执行时会从数据库中取出触发器来执行，如果触发器的名称和执行时间相同，则只有一个节点去执行此任务。\n如果此节点执行失败，则此任务则会被分派到另一节点执行，中途也会自动检查失效的定时调度，发现不成功的，其他节点立马接过来继续完成定时任务。Quartz有11个定时任务调度表\n  参考\nQuartz原理解密\n深入解读Quartz的原理\nQuartz 2.2 的实现原理和运行过程\n其他定时器  Timer：这是java自带的java.util.Timer类，这个类允许你调度一个java.util.TimerTask任务。使用这种方式可以让你的程序按照某一个频度执行，但不能在指定时间运行。一般用的较少。单线程，任务一多会阻塞；一个任务出异常其他任务都受影响；受系统时间影响 ScheduledExecutorService：也jdk自带的一个类；是基于线程池设计的定时任务类,每个调度任务都会分配到线程池中的一个线程去执行,也就是说,任务是并发执行,互不影响。线程池+延时队列DelayedQueue(数组、最小堆, 最近要执行的任务放在堆顶) 实现，如果堆顶任务时间未到就阻塞（通过自旋+condition.await\\signal实现）。不受系统时间影响 Spring 中的 @Schedule 注解  参考：Java 定时任务实现原理详解\nJava优先级队列DelayedWorkQueue原理分析\nCORS 浏览器的同源政策的同源指的是：协议相同、域名相同、端口相同，如果非同源，有三种行为会受到限制：Cookie、LocalStorage和IndexDB无法读取；DOM无法获得、AJAX请求不能发送。\n前后端分离的场景下，由于浏览器的同源策略，导致浏览器内的请求不同的源的后端是会失败，常见的解决跨域方法是使用CORS，现在常见的web框架都支持CORS，开启即可。\n解决跨域的方法除了CORS，还有jsonp，不过已经很少使用了，jsonp本质是利用浏览器允许加载不同源的js文件即标签等，将跨域请求标签里，返回一段可执行的js代码，其中包含了请求结果，通常是json格式，前端通过返回的js代码执行回调获取结果。\n详情见 跨域资源共享 CORS 详解\n对于跨域产生的问题，如CSRF跨域请求攻击的解决方案，可参考：美团:如何防止csrf\nsession和cookie   首先Http是无状态的，因此需要通过session、cookie来达到记录用户状态的目的。\n  传统的session、cookie：session存用户信息，保存在服务端中，cookie里存session对应的sessionId，保存在客户端中，用于找到对应的session，每次请求都会带上该cookie来表示此用户。\n  由于现在实例的部署不可能只部署一个，一般都是集群部署，因此session不可以只存在一个实例的内存中，因此引入Redis来存用户的登录信息\n  现在一般使用 token + Redis来实现 cookie - session 机制，本质上差不多，前端的cookie更多的是存token的信息而已，token也可以存在LocalStorage或sessionStorage中，发送请求时一般是把token的值放在请求头中，而不会把cookie发给后端，这样可以避免当用户禁用cookie导致功能不可用，还有CSRF问题。\n  JWT JWT = JSON WEB TOKEN\n原理 JWT实际上是一个token(令牌)，分为三部分：Header(头部)、Payload(负载)、Signature(签名)。\nHeader(头部) ：两部分组成，记录令牌类型和JWT的签名算法，一般是HMACSHA256。\nPayload(负载)： 记录用户登录信息(官方规范默认是不加密的，分为官方字段和私有字段）。\nSignature(签名) ：记录将 Header、Payload和服务端的密钥组合起来，使用Header(头部)里规定的方式加密。\n比如header里保存的加密方式是HMACSHA256，签名 Signature = HMACSHA256(base64URL(header) + \u0026quot;.\u0026quot; + base64URL(payload) + \u0026quot;.\u0026quot; + 保存在后端的密钥)\n最后的JWT = base64URL(Header) + \u0026quot;.\u0026quot; + base64URL(Payload) + \u0026quot;.\u0026quot; + Signature，后端收到该JWT后验证该签名是否正确，来判断JWT里的用户信息是否可靠。\nbase64：64指的是A-Z,a-z，0-9，+，/，将待转换的字符串转成二进制流，每3个8位转成4个6位，6位的二进制数转成十进制，根据码表找到对应的字符，以=号做后缀，凑齐位数\n一般是为了解决一些字符编码的问题，将非ASCII字符转化为ASCII字符，还有就是可以对数据做简单加密，base64URL在base64的基础上增加对一些符号的编解码，比如把\u0026quot;-\u0026ldquo;替换成\u0026rdquo;+\u0026quot;，使得它可以出现在url中。\nHMACSHA256：摘要算法，一般用于验证签名是否一致\n使用 可以存储在浏览器的本地缓存localStorage或者cookie中，发送请求的时候放在cookie里，或者放在请求头中\n JWT的目的是让服务器不保存任何session数据，让后端变成无状态的，因此没办法主动废弃某个token，一旦签发了JWT，在到期之前就会始终有效，如果想要实现这种功能，必然需要在后端保存JWT，就违背了JWT的设计初衷了。 要让JWT实现 续签 和 主动过期功能，必定需要在后端保存JWT  jwt主动过期问题，使用黑名单即可；分成两点，客户端要求失效，服务端记录token到黑名单；用户重置密码，服务端记录uid-time键值对，在此之前的token全部失效；客户端把保存的jwt删掉是没用的，此时的jwt依然有效，只是客户端没记录而已 jwt续签问题，一种解决方式是jwt中存储过期时间，服务端设置刷新时间，请求时判断是否在过期时间或刷新时间，在刷新时间内进行token刷新，失效token记入黑名单； 而黑名单过大问题，可以采用记录UID-刷新时间方式解决，判断jwt签发时间，jwt签发时间小于UID-刷新时间的记为失效   个人认为JWT的生成方式本身是有一套规范的，在实际使用过程中也可以对他进行改动，本质上还是一个签名校验而已，一般会对JWT进行魔改，比如使用Header(头部)里的加密方式加密Signature(签名)，Signature(签名)加密Header(头部) 和Payload(负载) 这两部分，服务器里的私钥解密Payload(负载)，得到需要的登录信息，不通过简单的base64URL编码，不对外暴露，签名算法或者签名里的密钥的方式可以改成其他等。  JWT参考：JWT 超详细分析\nCAS模型 - SSO(单点登录) 可参考：CAS实现单点登录SSO执行原理探究，讲得算是比较明白，这里是总结基于CAS模式改的单点登录模式\n 第一次访问时，由于没有访问的token，会引导至登录   第一次访问 \n  再次访问Web-1时，由于前端已存了token，直接使用token进行请求即可\n  已登录Web-1时去访问Web-2，会通过后端认证中心实现单点登录\n   第二次访问 \n这里在总结一下关于GrantTicket和ServiceTicket，跟CAS模型中提到的TGT、ST、PGT这些东西是类似的，本质是作为验证的票据，图中的GrantTicket、ServiceTicket、token含义如下\nGrantTicket：全局会话票据，保存在登录页，通过GrantTicket才能换取ServiceTicket；\nServiceTicket表示访问资源的一次性票据，根据ServiceTicket换取token，换取后失效；\ntoken：登录凭证\nGT、ST和token都是保存在Redis中的，他们在Redis中的存储结构如下\nkey：TOKEN_${Token的值} value: { \u0026quot;createTime\u0026quot;: 1565961654807, \u0026quot;accountId\u0026quot;: \u0026quot;123\u0026quot;, // 用户其他信息 \u0026quot;grantTicket\u0026quot;: ${GrantTicket的值} // token关联GT，用于注销时实现全局注销 } key：GRANT_TICKET_${GrantTicket的值} value: { \u0026quot;createTime\u0026quot;: 1565961654807, \u0026quot;accountId\u0026quot;: \u0026quot;123\u0026quot;, } key：SERVICE_TICKET_${ServiceTicket的值} value: { \u0026quot;createTime\u0026quot;: 1565961654807, \u0026quot;grantTicket\u0026quot;: ${GrantTicket的值} // ST关联GT，用于判断该ST是否有效，换取token后删除 } // token与grantTicket的记录，注销时，根据token中关联的GT，找到所有与之关联的token，进行删除，这里推荐使用Redis的scan命令进行分段查询，原因是Redis是单线程的，如果数据量太大使用keys命令遍历太久，阻塞Redis接收其他命令 key：{grantTicket}-{token} value：无 基于OAuth2.0的第三方登录 可参考：理解OAuth 2.0，这样基本就入门了，这里是总结项目中如何接入，一般在集成facebook和google的第三方登录也是类似的流程机制，这里只用到了access_token，对于refresh_token，是用来延长access_token的过期时间的，减少短时间内的重复登录，这里就没有涉及到了\n 基于OAuth2的第三方登录 \n为什么要后端要根据code + clientId + secret换成access_token，再根据access_token换用户个人信息？\n为什么后端不直接code + clientId + secret换用户个人信息呢？\n主要还是为了安全，防止中间人攻击\n  重定向的参数是带在url里的，是直接暴露在客户端的，如果直接返回access_token就不安全，因此才多了code这一层，为了降低code被拦截泄漏后的风险，code的过期时间一般都很短，且是一次性的；\n  另外就是后端对于外部的请求都是不信任的，因此接收到的参数(code)首先还要配合凭证去验证其合法性，对于验证通过后获得的access_token也有更多的操作空间，由后端持有，不会暴露出去\n像上图那种登录方案，后端只需要用户个人信息换完token就算完事了，所以看起来好像直接使用code + clientId + secret换用户个人信息就行，但是如果此时需要再获取用户的其他信息，就没有没办法再用code去换了，只能要求用户再次登录，此时如果有access_token就显得多么重要了\n  压测 总结一下做过的压测，压测工具jmetter，利用jmette可以多线程并发请求和可以实时查看简易报告的能力\n  先对被压测服务的接口针对不同场景编写压测用例，设定好TPS的起始和目标值，作为压测计划\n  画压测机器部署关系图，部署压测环境\n  对于被压测的服务，一般会mock掉与该服务相关关联的服务，比如该服务还连了数据库，该接口请求依赖一些独立部署的中间件，或者依赖其他服务，则会对这些相关的依赖用桩来代替，用于维持通信，以减少这些额外服务的影响。\n  一般一台机器只部署一个服务，特别是被压测服务，此外还要注意被压测服务所在的机器上网络设置相关的参数，比如TCP最大连接数、回收策略之类的设置\n    编写压测脚本，压测脚本越简单越好，尽量让压测工具不影响被压测服务，脚本最重要的几个设置： 发起请求时的并发线程数、响应的断言、TPS数，其他那些花里胡哨的输出树状图，饼图啊那些都不用配了，用最简单的报告输出即可\n  部署完后，将脚本配置放到jmeter的机器上，启动压测\nnohup java -jar bin/ApacheJMeter.jar -n -t jmetter脚本路径/config.jmx \u0026gt; test.out \u0026amp; 输出到当前目录下的test.out文件里，这里启动是使用默认参数启动，如果对jmetter的JVM设置有要求，也可以在启动时指定JVM参数，如\nnohup java -server -XX:+HeapDumpOnOutOfMemoryError -Xms512m -Xmx512m -XX:+UseG1GC -XX:MaxGCPauseMillis=250 -XX:G1ReservePercent=20 -Djava.security.egd=file:/dev/urandom -jar bin/ApacheJMeter.jar -n -t jmetter脚本路径/config.jmx \u0026gt; test.out \u0026amp; 压测开启后可以打开test.out文件查看压测报告\n  一般是按照TPS从小往大压，小的TPS压，在正常延时的情况下可以先判断程序是否有问题，比如内存泄漏，内存溢出，没问题了再逐步往大了压。如果先从大往小压，延时又上不去，此时判断不了是程序内部问题还是过大的TPS导致。压测时间一般最少压一天\n  输出压测报告\n  一般有如下几个点要注意，这些点到时也要输出到压测报告上\n   监控点 说明     jmetter端的TPS、延时、错误率 观察TPS是否符合预期、延时是否达到预期且稳定、错误率要为0。当程序正常时降低RT的手段：减少不必要的日志输出、业务逻辑算法是否还有优化空间，是否有IO占用或者频繁序列化反序列化、内部队列是否阻塞   被压测服务的gc fgc，ygc不要太频繁，一般来说fgc 一小时要小于3~4次；ygc一分钟要小于3~4次为佳。   jmetter端的CPU、内存使用率等 注意jmetter端的CPU是否过高或波动很大，避免影响压测结论   被压测服务端的CPU、磁盘、内存使用率等 如果cpu过高，如果连续达到90以上，基本上是内存泄漏导致了频繁的fgc；磁盘的占用情况，注意生成的日志是否把磁盘占满了    使用 jstat -gcutil [pid] [时间间隔，每几秒打印] [打印次数]查看GC情况\n当被压测端的gc不正常时，应尽量保存事发环境\n​\t1、收集内存使用基本情况统计：jmap -heap [pid] \u0026gt; [文件名，如heap.log]\n​\t2、收集线程堆栈运行信息：jstack [pid] \u0026gt; [文件名，如stack.log]\n​\t3、收集内存详细使用信息，生成dump内存快照：jmap -dump:format=b,file=[文件名，如heap.dump] [pid]\n一般使用eclipse mat工具进行内存快照的分析，排查出内存泄漏的问题。\nmat的使用参见：Eclipse MAT内存分析工具\n一般压测脚本的模板：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75  \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;jmeterTestPlan version=\u0026#34;1.2\u0026#34; properties=\u0026#34;3.2\u0026#34; jmeter=\u0026#34;3.2 r1790748\u0026#34;\u0026gt; \u0026lt;hashTree\u0026gt; \u0026lt;TestPlan guiclass=\u0026#34;TestPlanGui\u0026#34; testclass=\u0026#34;TestPlan\u0026#34; testname=\u0026#34;测试计划\u0026#34; enabled=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;!-- 一般写压测计划中的序号+名称 --\u0026gt; \u0026lt;stringProp name=\u0026#34;TestPlan.comments\u0026#34;\u0026gt;\u0026lt;/stringProp\u0026gt; \u0026lt;boolProp name=\u0026#34;TestPlan.functional_mode\u0026#34;\u0026gt;false\u0026lt;/boolProp\u0026gt; \u0026lt;boolProp name=\u0026#34;TestPlan.serialize_threadgroups\u0026#34;\u0026gt;false\u0026lt;/boolProp\u0026gt; \u0026lt;elementProp name=\u0026#34;TestPlan.user_defined_variables\u0026#34; elementType=\u0026#34;Arguments\u0026#34; guiclass=\u0026#34;ArgumentsPanel\u0026#34; testclass=\u0026#34;Arguments\u0026#34; testname=\u0026#34;用户定义的变量\u0026#34; enabled=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;collectionProp name=\u0026#34;Arguments.arguments\u0026#34;/\u0026gt; \u0026lt;/elementProp\u0026gt; \u0026lt;stringProp name=\u0026#34;TestPlan.user_define_classpath\u0026#34;\u0026gt;\u0026lt;/stringProp\u0026gt; \u0026lt;/TestPlan\u0026gt; \u0026lt;hashTree\u0026gt; \u0026lt;ThreadGroup guiclass=\u0026#34;ThreadGroupGui\u0026#34; testclass=\u0026#34;ThreadGroup\u0026#34; testname=\u0026#34;Thread Group\u0026#34; enabled=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;stringProp name=\u0026#34;ThreadGroup.on_sample_error\u0026#34;\u0026gt;continue\u0026lt;/stringProp\u0026gt; \u0026lt;elementProp name=\u0026#34;ThreadGroup.main_controller\u0026#34; elementType=\u0026#34;LoopController\u0026#34; guiclass=\u0026#34;LoopControlPanel\u0026#34; testclass=\u0026#34;LoopController\u0026#34; testname=\u0026#34;循环控制器\u0026#34; enabled=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;boolProp name=\u0026#34;LoopController.continue_forever\u0026#34;\u0026gt;false\u0026lt;/boolProp\u0026gt; \u0026lt;intProp name=\u0026#34;LoopController.loops\u0026#34;\u0026gt;-1\u0026lt;/intProp\u0026gt; \u0026lt;/elementProp\u0026gt; \u0026lt;stringProp name=\u0026#34;ThreadGroup.num_threads\u0026#34;\u0026gt;500\u0026lt;/stringProp\u0026gt; \u0026lt;!-- 发起请求时的并发线程数，这里设置为500个并发线程，表示使用这么多的线程数来达到下面设置的TPS数 --\u0026gt; \u0026lt;stringProp name=\u0026#34;ThreadGroup.ramp_time\u0026#34;\u0026gt;8\u0026lt;/stringProp\u0026gt; \u0026lt;longProp name=\u0026#34;ThreadGroup.start_time\u0026#34;\u0026gt;1509332694000\u0026lt;/longProp\u0026gt; \u0026lt;longProp name=\u0026#34;ThreadGroup.end_time\u0026#34;\u0026gt;1509332694000\u0026lt;/longProp\u0026gt; \u0026lt;boolProp name=\u0026#34;ThreadGroup.scheduler\u0026#34;\u0026gt;false\u0026lt;/boolProp\u0026gt; \u0026lt;stringProp name=\u0026#34;ThreadGroup.duration\u0026#34;\u0026gt;\u0026lt;/stringProp\u0026gt; \u0026lt;stringProp name=\u0026#34;ThreadGroup.delay\u0026#34;\u0026gt;\u0026lt;/stringProp\u0026gt; \u0026lt;/ThreadGroup\u0026gt; \u0026lt;hashTree\u0026gt; \u0026lt;HTTPSamplerProxy guiclass=\u0026#34;HttpTestSampleGui\u0026#34; testclass=\u0026#34;HTTPSamplerProxy\u0026#34; testname=\u0026#34;click http request\u0026#34; enabled=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;elementProp name=\u0026#34;HTTPsampler.Arguments\u0026#34; elementType=\u0026#34;Arguments\u0026#34; guiclass=\u0026#34;HTTPArgumentsPanel\u0026#34; testclass=\u0026#34;Arguments\u0026#34; testname=\u0026#34;用户定义的变量\u0026#34; enabled=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;collectionProp name=\u0026#34;Arguments.arguments\u0026#34;/\u0026gt; \u0026lt;/elementProp\u0026gt; \u0026lt;stringProp name=\u0026#34;HTTPSampler.domain\u0026#34;\u0026gt;192.168.1.123\u0026lt;/stringProp\u0026gt; \u0026lt;!-- 此处为被压测服务的host --\u0026gt; \u0026lt;stringProp name=\u0026#34;HTTPSampler.port\u0026#34;\u0026gt;12345\u0026lt;/stringProp\u0026gt; \u0026lt;!-- 此处为被压测服务的port --\u0026gt; \u0026lt;stringProp name=\u0026#34;HTTPSampler.protocol\u0026#34;\u0026gt;http\u0026lt;/stringProp\u0026gt; \u0026lt;stringProp name=\u0026#34;HTTPSampler.contentEncoding\u0026#34;\u0026gt;\u0026lt;/stringProp\u0026gt; \u0026lt;stringProp name=\u0026#34;HTTPSampler.path\u0026#34;\u0026gt;${__StringFromFile(/home/urls.log,,,)}\u0026lt;/stringProp\u0026gt; \u0026lt;!-- 发起的http请求uri从文件读取，文件路径 --\u0026gt; \u0026lt;stringProp name=\u0026#34;HTTPSampler.method\u0026#34;\u0026gt;GET\u0026lt;/stringProp\u0026gt; \u0026lt;boolProp name=\u0026#34;HTTPSampler.follow_redirects\u0026#34;\u0026gt;false\u0026lt;/boolProp\u0026gt; \u0026lt;boolProp name=\u0026#34;HTTPSampler.auto_redirects\u0026#34;\u0026gt;false\u0026lt;/boolProp\u0026gt; \u0026lt;boolProp name=\u0026#34;HTTPSampler.use_keepalive\u0026#34;\u0026gt;true\u0026lt;/boolProp\u0026gt; \u0026lt;boolProp name=\u0026#34;HTTPSampler.DO_MULTIPART_POST\u0026#34;\u0026gt;false\u0026lt;/boolProp\u0026gt; \u0026lt;stringProp name=\u0026#34;HTTPSampler.embedded_url_re\u0026#34;\u0026gt;\u0026lt;/stringProp\u0026gt; \u0026lt;stringProp name=\u0026#34;HTTPSampler.implementation\u0026#34;\u0026gt;Java\u0026lt;/stringProp\u0026gt; \u0026lt;stringProp name=\u0026#34;HTTPSampler.connect_timeout\u0026#34;\u0026gt;\u0026lt;/stringProp\u0026gt; \u0026lt;stringProp name=\u0026#34;HTTPSampler.response_timeout\u0026#34;\u0026gt;\u0026lt;/stringProp\u0026gt; \u0026lt;/HTTPSamplerProxy\u0026gt; \u0026lt;hashTree/\u0026gt; \u0026lt;ResponseAssertion guiclass=\u0026#34;AssertionGui\u0026#34; testclass=\u0026#34;ResponseAssertion\u0026#34; testname=\u0026#34;Response Assertion\u0026#34; enabled=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;collectionProp name=\u0026#34;Asserion.test_strings\u0026#34;\u0026gt; \u0026lt;stringProp name=\u0026#34;49586\u0026#34;\u0026gt;200\u0026lt;/stringProp\u0026gt; \u0026lt;!-- http请求的响应断言，要求返回的http code为200才判定为成功 --\u0026gt; \u0026lt;/collectionProp\u0026gt; \u0026lt;stringProp name=\u0026#34;Assertion.test_field\u0026#34;\u0026gt;Assertion.response_code\u0026lt;/stringProp\u0026gt; \u0026lt;boolProp name=\u0026#34;Assertion.assume_success\u0026#34;\u0026gt;false\u0026lt;/boolProp\u0026gt; \u0026lt;intProp name=\u0026#34;Assertion.test_type\u0026#34;\u0026gt;8\u0026lt;/intProp\u0026gt; \u0026lt;/ResponseAssertion\u0026gt; \u0026lt;hashTree/\u0026gt; \u0026lt;ConstantThroughputTimer guiclass=\u0026#34;TestBeanGUI\u0026#34; testclass=\u0026#34;ConstantThroughputTimer\u0026#34; testname=\u0026#34;Constant Throughput Timer\u0026#34; enabled=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;intProp name=\u0026#34;calcMode\u0026#34;\u0026gt;1\u0026lt;/intProp\u0026gt; \u0026lt;doubleProp\u0026gt; \u0026lt;name\u0026gt;throughput\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;30000.0\u0026lt;/value\u0026gt; \u0026lt;!-- 1分钟内发起的请求数，换算为tps为500 --\u0026gt; \u0026lt;savedValue\u0026gt;0.0\u0026lt;/savedValue\u0026gt; \u0026lt;/doubleProp\u0026gt; \u0026lt;/ConstantThroughputTimer\u0026gt; \u0026lt;hashTree/\u0026gt; \u0026lt;/hashTree\u0026gt; \u0026lt;/hashTree\u0026gt; \u0026lt;WorkBench guiclass=\u0026#34;WorkBenchGui\u0026#34; testclass=\u0026#34;WorkBench\u0026#34; testname=\u0026#34;工作台\u0026#34; enabled=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;boolProp name=\u0026#34;WorkBench.save\u0026#34;\u0026gt;true\u0026lt;/boolProp\u0026gt; \u0026lt;/WorkBench\u0026gt; \u0026lt;hashTree/\u0026gt; \u0026lt;/hashTree\u0026gt; \u0026lt;/jmeterTestPlan\u0026gt;   调优 参考：https://tech.meituan.com/2016/12/02/performance-tunning.html\n业务相关 防止表单重复提交 场景：用户点击下单页面，跳转至下单页面，提交订单，此时有可能网络原因或者用户点击多次，导致订单重复提交。\n解决：用户跳转至下单页前，会先获取订单号(也作为订单表主键)，将订单号绑定在下单页，利用数据库主键唯一的特性，让创建订单的操作变成幂等性。\n解决ABA问题 **场景：**类似MySQL的丢失更新，比如有操作1，操作2先后对记录A进行更新，操作1的响应丢失导致重试，此时操作2已经更新成功，操作1重试时会覆盖操作2的更新。\n**解决：**通过版本号解决，订单表增加一列作版本号，版本号可以使用递增序列、时间戳等，通过比较版本号来确定操作的先后顺序，更新成功时也需要更新版本号。\n流量大、数据量大的商品详情页数据存储 **场景：**一般商品详情页都是访问量最大的页面，比如用户做商品对比、查看商品详情都需要，另外就是商品详情页一般涉及很多数据，如下，且后端存储的sku量也是巨大的，直接分多张表去存虽然可以实现，但是性能就一般了。\n商品 ├── 基本信息 │ ├── 标题、副标题 │ ├── 价格：原价、促销价 │ └── 颜色、规格等 ├── 商品参数 ├── 商品介绍 ├── 图片视频 来自其他系统的 ├── 促销信息 ├── 推荐商品 ├── 评论、评价 ├── 配送信息 └── 店铺信息 **解决：**分析不同的数据特性，比如有些数据是热点的、相对固定的、不常被修改的、需求变化不大的等各种维度去划分，进行不同存储。动态数据、实时数据还是照旧，该怎么处理怎么处理，其他的可以：\n  套一层缓存在数据库外面，查询数据先缓存后数据库\n  针对每个不同的spu有不同的商品属性，则可以使用NoSQL来解决\n  针对图片、视频等数据，使用对象存储、CDN解决，比如AWS S3，直接通过其提供的API进行访问，将这部分的压力转移到云服务厂商\n  将相对固定的数据静态化，比如商品介绍，其包含了大量的文字、图片、视频等，可直接将这一部分保存成HTML文件中，访问时直接返回HTML文件，保存成HTML还可以配合CDN进行加速\n  针对SQL方面的优化  可以起一个SQL检查脚本，检查执行时间过长的SQL，如果超过指定时间的，进行记录和kill，再进行优化，把慢SQL解决掉，避免多个执行时间过长的SQL拖垮整个数据库。 主从分离，读写分离，服务降级 分析SQL执行和访问量间的关系，数据库CPU利用率变化 MySQL单次查询扫描的数据量控制在千万级别内，单次扫描的数据量在百万级别是可以接受，理论上查询都应该使用索引，避免全表扫描  对象存储原理  本质是一个规模很大的分布式Key-value集群，外加一个保存集群节点信息、文件信息和映射关系(统称为元数据)的节点集群，在最外层再加上一个Gateway来对外提供服务即可。 针对图片、视频等大文件，在存储时会将其拆分成多个大小相等的块Block，一般是几十KB到几MB，便于管理，也可以分散到不同节点，提升并行读写性能。 由于分成的块太小，数量多，一般也不是直接进行管理的，而是将一些块进行聚合，放到容器里，类似分片的概念，主从复制时，也是直接复制这些块即可，不用再复制多日志  跨系统数据实时同步  采用Bin Log + MQ的方式，将上游数据实时同步到下游其他系统的数据库中，为了确保数据一致性，必须顺序读取Bin Log，因此MQ的主题也必须设置为只有一个分区，才能保证Bin Log有序。 当下游系统想要扩展消费能力时，不可盲目增加同步线程数和MQ主题分区，由于Bin Log的顺序性，要确保多线程消费时，不会对数据产生影响，所以可以将具有因果一致性的Bin Log发布给同一主题分区，才可以多线程同步消费。具体可参考MySQL 5.6版本后多线程处理Bin Log的做法。  不停机情况下更换数据库  利用Bin Log或者复制状态机理论，增加一个新库和同步服务。先将旧库上的数据快照同步到新库，对于旧库的新数据，使用同步服务进行同步复制 改造旧服务，增加双写新旧两个库的功能，添加功能开关，使其能够只写旧库、只写新库、同步双写的功能 开关打至只写旧服务，利用同步服务同步数据，等改造后的旧服务能稳定运行，验证新旧两个库的数据是否一致；一致之后将改造后的旧服务的开关打至同步双写，关闭同步服务，此时仍然以数据写至旧库为主，写新库失败则进行人工干预，此外，双写时可能会存在数据不一致，此时需要针对这一小段时期上线数据对比与补偿服务，验证和补充新旧数据不一致问题；待最终稳定后，才将开关打至只写新服务，实现数据库替换的平滑过渡。  海量数据处理 针对的是埋点数据、日志数据、访问数据、点击数据、监控数据等，一般采用先存储后计算的方式\n 使用Kafka存储，上游系统将海量数据采集后发给KafKa，利用Kafka无限消息堆积和超高吞吐，存储数据，再由下游系统进行订阅消费即可。这种方案适合短时间的海量数据处理。关键词：分布式流数据存储。 HDFS存储 + Hive查询 或者 ES查询 针对监控数据，可以使用时序数据库，例如Prometheus  API协议设计 其实分成了API和协议两部分\n 一般API会符合Restful规范，由行为 + 资源组合而成； 协议一般就包含了请求/响应头和响应/请求体的内容，参数结构化，比如参数类型是Hash，就不要存成String，值是Hash的序列化后的字符串； 响应结果要统一，尽量不要因为参数的不同而返回不同类型的响应结构 需要考虑认证和安全相关，比如是否需要签名、票据、token等 多服务之间，保证风格一致 考虑幂等； 加入版本控制，加在URL上，或者请求头有个字段标识；  参考 极客时间 - 后端存储实战\n","date":"2020-09-22T00:00:00Z","permalink":"http://nixum.cc/p/%E5%85%B6%E4%BB%96/","title":"其他"},{"content":"[TOC]\n使用场景  秒杀系统，一般秒杀系统处理包含几个步骤：风险控制、库存锁定、生成订单、短信通知、更新统计数据灯，而决定秒杀是否成功只在前两个步骤，后续的操作就可以通过消息队列异步处理完成，加快整个流程的处理，减少等待时间，提升并发量。 隔离网关和后端服务，实现流量控制，保护后端服务，但会增加系统调用链，导致总体响应变长，异步增加系统复杂性。 令牌桶，目的也是进行流量控制。 服务解耦，数据同步，比如订单系统在订单状态发生变化时发出消息通知，其他服务订阅后做相应处理。 连接流计算任务和数据，比如集群日志处理，大数据统计 将消息广播给其他接收者  好处 流量削峰和流量控制、异步处理、解耦、广播、最终一致性\n缺点 可用性降低、复杂度提高、一致性问题、消息延迟\n常见消息队列    特性 ActiveMQ RabbitMQ RocketMQ Kafaka     单机吞吐量 万级，吞吐量比RocketMQ和Kafka要低了一个数量级 万级，吞吐量比RocketMQ和Kafka要低了一个数量级 10万级，RocketMQ也是可以支撑高吞吐的一种MQ 10万级别，这是kafka最大的优点，就是吞吐量高。一般配合大数据类的系统来进行实时数据计算、日志采集等场景   topic数量对吞吐量的影响  使用队列模型，通过Exchange模块实现发布-订阅模型，Exchange位于生产者和队列之间，由Exchange决定将详细投递到哪个队列。 topic可以达到几百，几千个的级别，吞吐量会有较小幅度的下降这是RocketMQ的一大优势，在同等机器下，可以支撑大量的topic topic从几十个到几百个的时候，吞吐量会大幅度下降。所以在同等机器下，kafka尽量保证topic数量不要过多。如果要支撑大规模topic，需要增加更多的机器资源   可用性 高，基于主从架构实现高可用性 高，基于主从架构实现高可用性 非常高，分布式架构 非常高，kafka是分布式的，一个数据多个副本，少数机器宕机，不会丢失数据，不会导致不可用   消息可靠性 有较低的概率丢失数据  经过参数优化配置，可以做到0丢失 经过参数优化配置，消息可以做到0丢失   时效性 ms级 微秒级，这是rabbitmq的一大特点，延迟是最低的 ms级 延迟在ms级以内   功能支持 MQ领域的功能极其完备 基于erlang开发，所以并发能力很强，性能极其好，延时很低 MQ功能较为完善，还是分布式的，扩展性好 功能较为简单，主要支持简单的MQ功能，在大数据领域的实时计算以及日志采集被大规模使用，是事实上的标准   优劣势总结 非常成熟，功能强大，在业内大量的公司以及项目中都有应用。偶尔会有较低概率丢失消息，而且现在社区以及国内应用都越来越少，官方社区现在对ActiveMQ 5.x维护越来越少，几个月才发布一个版本而且确实主要是基于解耦和异步来用的，较少在大规模吞吐的场景中使用 erlang语言开发，性能极其好，延时很低；吞吐量到万级；拥有灵活的路由配置；MQ功能比较完备而且开源提供的管理界面非常棒，用起来很好用；社区相对比较活跃；RabbitMQ确实吞吐量与其他几个相比会低一些，这是因为他做的实现机制比较重；对消息的堆积支持不是很好，当有大量消息积压时，会导致RabbitMQ性能急剧下降；erlang开发，比较小众，很难读源码，很难定制和掌控。 接口简单易用，日处理消息上百亿之多，可以做到大规模吞吐，性能也非常好，分布式扩展也很方便，社区维护还可以，可靠性和可用性都是ok的，还可以支撑大规模的topic数量，支持复杂MQ业务场景。java实现，源码易读；中文社区活跃度，文档相对来说简单，接口这块不是按照标准JMS规范走的有些系统要迁移需要修改大量代码。 kafka的特点其实很明显，就是仅仅提供较少的核心功能，但是提供超高的吞吐量，ms级的延迟，极高的可用性以及可靠性，而且分布式可以任意扩展。同时kafka最好是支撑较少的topic数量即可，保证其超高吞吐量。而且kafka唯一的一点劣势是有可能消息重复消费，那么对数据准确性会造成极其轻微的影响，在大数据领域中以及日志采集中，这点轻微影响可以忽略这个特性天然适合大数据实时计算以及日志收集。异步批量操作性能极佳，但是同步收发信息因为要攒一批消息才进行处理会导致响应延时比较高。    消息模型 1.点对点：队列、监听器\n2.发布订阅：监听器、监听器、观察者模式\n一般消息队列架构  主要是设计中间的消息转发，将一次RPC转化成两次RPC； 选择通信协议； 消息的可靠性确认； 消息持久化； 消息模型； 事务特性； 分布式集群特性； 消息队列一般有两种模式，pull模式（消费方主动向队列拉取数据），比如kafka，push模式（由队列向消费方推送数据）   消息队列架构 \n一般来说，一个主题可以分配给多个Broker，每个Broker可以有多个队列；\n多个生产者对于同一个主题的消息，可以发送到不同的队列（轮询发，随机挑一个发，只往某个一个发）；\n消费组订阅的是主题，消费主题里的所有队列的消息，消费消息时，可以只是去读队列里的消息，不一定会删掉；\n多个消费组在消费同一主题时，消费组之间互不影响，即消息可以同时被多个消费组消费；\n消费组内部可以包含多个消费者，同一消费组内，每个队列只能被一个消费者占用，一个消息只会被组内一个消费者消费；\n每个消费组内部维护自己的消费位置，记录消费某一队列中消息的位置，消费位置与消费者无关\n关于消息的序列化和反序列化 除了常见的json、XML、protobuf、Kryo、Hession等方案，还能自主设计\n客户端和服务端维护一份字段顺序，在序列化时只存对应对象的值，从而减少序列化对象的长度，比如\n03 | 08 7a 68 61 6e 67 73 61 6e | 17 | 01 User | z h a n g s a n | 23 | true 03表示对象类型，08表示名字的长度，然后字段值以名字、年龄、是否yi'wei 关于消息的断句 数据传输过程中，收到的分段不一定是发出去的分段，因此需要合理的分段，来让数据语义明确。比如在每个分段前加数据的长度\n03下雨天 03留客天 02天留 03我不留 关于消息的存储 消息队列的存储一般比较简单，每个主题包含若干分区，每个分区(队列)都是WAL，写入时尾部追加，不允许修改，读取的时候，根据一个索引序号进行查询，连续顺序往下读。\nKafka 以分区为单位，每个分区包含一组消息文件和一组索引文件，消息文件和索引文件一一对应，具有相同的文件名，但文件扩展名不一样，文件名就是这个文件中第一条消息的索引序号；每个索引中保存索引序号（表示这条消息是这个分区中的第几条消息），和 对应的消息在消息文件中的绝对位置。\n以分区为单位，粒度更细，比较灵活，容易进行数据迁移和扩容。\nKafka为了节省存储空间，采用稀疏索引，每隔几条消息创建一条索引。\n写入消息时，在文件尾部追加写入，一个文件满了再写下一个文件；查找消息时，先根据文件名找到所在索引的索引文件，然后二分法遍历索引文件内的索引，找到离目标最近的索引，再去消息文件中找到这条索引对应的位置，顺序遍历消息文件，找到目标消息。\nRocketMQ 以Broker为单位，每个Broker只包含一组消息文件和索引文件，在该Broker上的所有主题的消息都存放在这一组消息文件中，索引文件则按照主题和队列分别建立，每个队列对应一组索引文件。\n以Broker为单位，粒度较粗，但在写入时文件更少，有更好的批量和顺序写入。\n索引采用定长稠密索引，为每条消息都创建索引，每个索引的长度固定20个字节。\n写入消息时，Broker上所有主题、所有队列的消息都按照自然顺序追加写入同一个消息文件中，一个文件满了再写下一个；查找消息时，直接根据队列的消息序号，计算出索引的全局位置（索引序号 乘 固定长度20），直接读取索引，再根据索引在消息文件中找到对应的消息，查找速度比Kafka快，但耗存储。\n案例一 - 异步消息的处理  \n  收到请求后，我们在Handler中不做过多的处理，执行必要的检查后，将请求放到一个内存队列中，也就是图中的RequestsQueue。请求被放入队列后，Handler的方法就结束了。可以看到，在Handler中只是把请求放到了队列中，没有太多的业务逻辑，这个执行过程是非常快的，所以即使是处理海量的请求，也不会过多的占用IO线程。 由于要保证消息的有序性，整个流程的大部分过程是不能并发的，只能单线程执行。所以，接下来我们使用一个线程WriteThread从请求队列中按照顺序来获取请求，依次进行解析请求等其他的处理逻辑，最后将消息序列化并写入存储。序列化后的消息会被写入到一个内存缓存中，就是图中的JournalCache，等待后续的处理。 执行到这里，一条一条的消息已经被转换成一个连续的字节流，每一条消息都在这个字节流中有一个全局唯一起止位置，也就是这条消息的Offset。后续的处理就不用关心字节流中的内容了，只要确保这个字节流能快速正确的被保存和复制就可以了。 这里面还有一个工作需要完成，就是给生产者回响应，但在这一步，消息既没有落盘，也没有完成复制，还不能给客户端返回响应，所以我们把待返回的响应按照顺序放到一个内存的链表PendingCallbacks中，并记录每个请求中的消息对应的Offset。 然后，我们有2个线程，FlushThread和ReplicationThread，这两个线程是并行执行的，分别负责批量异步进行刷盘和复制，刷盘和复制又分别是2个比较复杂的流程，我们暂时不展开讲。刷盘线程不停地将新写入JournalCache的字节流写到磁盘上，完成一批数据的刷盘，它就会更新一个刷盘位置的内存变量，确保这个刷盘位置之前数据都已经安全的写入磁盘中。复制线程的逻辑也是类似的，同样维护了一个复制位置的内存变量。 最后，我们设计了一组专门用于发送响应的线程ReponseThreads，在刷盘位置或者复制位置更新后，去检查待返回的响应链表PendingCallbacks，根据QOS级别的设置（因为不同QOS基本对发送成功的定义不一样，有的设置需要消息写入磁盘才算成功，有的需要复制完成才算成功），将刷盘位置或者复制位置之前所有响应，以及已经超时的响应，利用这组线程ReponseThreads异步并行的发送给各个客户端。 这样就完成了消息生产这个流程。整个流程中，除了JournalCache的加载和卸载需要对文件加锁以外，没有用到其他的锁。每个小流程都不会等待其他流程的共享资源，也就不用互相等待资源（没有数据需要处理时等待上游流程提供数据的情况除外），并且只要有数据就能第一时间处理。 这个流程中，最核心的部分在于WriteThread执行处理的这个步骤，对每条消息进行处理的这些业务逻辑，都只能在WriteThread中单线程执行，虽然这里面干了很多的事儿，但是我们确保这些逻辑中，没有缓慢的磁盘和网络IO，也没有使用任何的锁来等待资源，全部都是内存操作，这样即使单线程可以非常快速地执行所有的业务逻辑。  这个里面有很重要的几点优化：\n第一是使用异步设计，把刷盘和复制这两部分比较慢的操作从这个流程中分离出去异步执行；\n第二是使用了一个写缓存JournalCache将一个写磁盘的操作，转换成了一个写内存的操作，来提升数据写入的性能，关于如何使用缓存，后面我会专门用一节课来讲；\n第三是这个处理的全流程是近乎无锁的设计，避免了线程因为等待锁导致的阻塞；\n第四是把回复响应这个需要等待资源的操作，也异步放到其他的线程中去执行。\n 案例二 - Kafka高性能IO  Kafka的SDK，生产者调用SDK发送消息时，看起来只发了一条，但是SDK处理时不是一条一条发的，而是先把消息放到内存中缓存起来，攒一批，异步一块发给broker，broker会把这一批消息当成一条处理，消费者pull消息，也是一次性拉取这一批消息进行处理，从而减少Broker处理请求的次数，减轻压力。 顺序读写，Broker收到消息后，顺序写入对应的log文件，一个文件写满就开启新文件顺序写下去，消费时也是从log文件的某一位置开始，顺序的读出来 利用PageCache加速读写，数据写入文件时，先写入PageCache，再一批一批写入磁盘，读取文件时，也是先从PageCache中读取数据，当PageCache中没有数据，会引发os的缺页中断，读取先从会被阻塞，直到数据从文件中复制到PageCache，再从PageCache中读取数据。 零拷贝提升消费性能：一般来说，Broker处理消费时，会先从文件复制数据到PageCache，从PageCache复制数据到应用内存，从应用内存复制到Socket的缓冲区，发送数据。如果文件中的数据无需其他处理，可以使用零拷贝，直接将数据从PageCache复制到socket缓冲区，减少复制次数  常见问题 以下问题都是需要分具体的MQ的，这里简单说下通用方法。一般异常会出现的场景：\n 生产者：网络或内部问题导致消息发送失败，没发出去；或者是发出去了，但是没有接受成功响应导致重复发送。 消费者：消费者获取了消息，处理消息的过程中宕机，恢复时消息如何处理。 消息队列：宕机重启时会丢数据  如何利用事务消息实现分布式事务 以RocketMQ为例，生产者先发送一个半消息给MQ，此时的半消息对消费者不可见，完成之后执行本地事务，根据本地事务的执行结果，对刚刚发送的半消息进行commit或rollback，只有commit之后，消费者才能消费此消息。如果生产者本地事务提交成功后宕机，MQ会调用生产者提供的反查方法确认半消息的之后的执行状态，反查方法会有三个结果：提交、回滚、不确定，如果查询结果是不确定，MQ则会进行重试，直到查到确定的结果或者超重试次数。这种场景其实是默认消费者一定会消费成功的，如果要让消费者消费不成功时，生产者也回滚，那就不能用这种方案了。\n如何保证高可用性 集群 + zookeeper + 负载均衡\n以ActiveMQ为例（因为是主从架构）\n使用ZooKeeper（集群）注册所有的ActiveMQ Broker。只有其中的一个Broker可以提供服务，被视为 Master，其他的 Broker 处于待机状态，被视为Slave。如果Master因故障而不能提供服务，Zookeeper会从Slave中选举出一个Broker充当Master。 Slave连接Master并同步他们的存储状态，Slave不接受客户端连接。所有的存储操作都将被复制到 连接至 Master的Slaves。如果Master挂了，得到了最新更新的Slave会成为 Master。故障节点在恢复后会重新加入到集群中并连接Master进入Slave模式\n消息数据可以持久化或非持久化，在集群内共享，当有节点挂掉后，其他节点也可以通过这些共享的数据顶上。另外，也可通过定时任务定时对失败的消息进行补偿。\n数据的持久化存储可以存储在文件系统 \u0026gt; 分布式KV \u0026gt; 分布式文件系统 \u0026gt; 数据库（速度上的排列，但可靠性就要反过来了）\n如何解决消息重复问题，保证消息幂等性 消息幂等，可以保证即使消息被重复消费也无所谓，一般来讲，重复发送总是存在的，要避免的是即使重复消费也能保证业务正确。要保证消息重复发送，除非允许消息丢失\n常见实现幂等性方法：\n 保证消息幂等一般是在消费者做，消费者处理消息的时候将消息入库记录，利用数据库的唯一键约束实现，比如通过消息id + redis/MySQL/Mongo来判重。 更新数据时设置前置条件，如果满足该条件则更新数据，否则拒绝更新数据，在更新数据的同时，变更前置条件中需要判断的数据，这样当重复执行这个操作时，由于第一次更新已经修改了前置条件，不满足前置条件时，则不会重复执行更新数据的操作，比如给数据增加一个版本号，每次更新数据前，比较当前数据的版本号是否和消息中的版本号一致，如果不一致就拒绝更新数据，更新数据的同时，版本号+1，实现幂等更新。 在发送消息时，给消息指定一个全局唯一的ID，消费时，先根据这个ID检查这条消息是否被消费过，如果没消费过，才更新数据，然后标记消息为已消费。但该方案在分布式系统中比较复杂，高可用，高性能都有一定的影响，需要使用分布式锁或者分布式事务 对于无法处理的消息导致MQ重复发送，可以设置重复次数，过了重复次数将消息持久化到其他地方，比如死信队列，以后处理。  如何保证消息顺序 100%的顺序消息的条件比较苛刻，需要允许消息丢失且生成者到消费者到接收者都是单点单线程，一般是保证在不丢消息的前提下，尽量减少重复消息，不保证投递顺序。。\n允许重复消息时，可以通过版本号或者状态机来解决消息幂等处理和消息顺序的问题\n 版本号：消费者接收消息时，只接收比最新版本号大的消息，重复消息因为版本号比当前版本号小，所以可以抛弃，比如消息顺序是123，消费者当前也收到123了，再收到123其中一个就可以不进行处理了；如果消息顺序不一致，比如消息顺序是123，但先接收到的是3，1和2还没收到，那就只能先把3存起来，等待12的到来，具体可以参考TCP协议的通信机制。 状态机：将业务流程设定成一系列的状态扭转，不同的状态只能处理不同的消息，就可以依靠状态流的扭转来实现顺序消息  减少重复消息的处理\n消息队列收到消费者的确认信号后，将消息id清除或进行标记；\n发送方要进行重发前对消费者进行询问请求\n如何保证消息的可靠性传输 消息传输有3个阶段：\n 消息的传输包括生产者发送消息给消息队列 消息在Broker端存储，如果是集群，消息会复制到其他副本 消费者通过pull模式拉取数据 或者 消息队列通过push模式向消费者发送消息  保证消息可靠，一般是使用 持久化机制 或 事务 + ack机制\n持久化不一定带有事务特性，比如直接日志落地，如果持久化要实现事务特性，可以使用分布式事务或者本地数据库事务。\n消息的确认机制（ack机制），消息发送完成后，需要收到接收方的确认信号，确认信号的返回可以是收到后消息后就立即返回，比如默认auto ack机制，或者是接收方法接收到消息，处理完该消息后才发送确认信号。另外，当接收方无法处理消息，比如消费能力不够，网络不佳等情况，接收方也可以直接拒绝消息，等待发送方重新发送，所以这里就涉及到消息的重复发送了，通过直接拒绝消息来减少业务负担。\n ACK模式描述了Consumer与broker确认消息的方式(时机),比如当消息被Consumer接收之后,Consumer将在何时确认消息。对于broker而言，只有接收到ACK指令,才会认为消息被正确的接收或者处理成功了,通过ACK，可以在consumer（producer）与Broker之间建立一种简单的“担保”机制.\n   消息发送阶段：生产者往Broker发送消息前，可以做一次消息持久化，收到Broker的ack响应后，要正确处理，以此保证消息在生产阶段不会丢失。在此阶段，Broker并不关心生产者是否收到ack响应，因为在生产者的角度，消息已经持久化后成功发出了，如果没有收到ack，最多就重复发送，那就会收到重复的ack，保证消息一定落到了Broker，当然前提是Broker能幂等处理。\n发送方只有在消息入库成功，事务提交后，才会发送消息，如果发送失败，可以靠定时任务重试。注意本地事务做的是业务落地和消息落地的事务。\n  消息到达Broker阶段：对于单节点Broker，在收到消息后，将消息持久化到磁盘，再给生产者返回ack响应；对于集群Broker，在收到消息后，至少要把消息发送到两个以上节点，再给生产者返回ack响应。\n  消息消费阶段：对于pull模式，消费者拉取消息后，先成功执行完业务逻辑后，才给Broker发送消费ack响应，如果Broker没有收到消费者的确认响应，下次拉取的仍然是同一条消息。\n支持广播的消息队列需要对每个待发送的endpoint，持久化一个发送状态，直到所有endpoint状态都OK才可删除消息。\n无论是pull模式还是push模式，在允许重复消息的情况下，还可通过定时任务轮询未消费消息发送给消费者处理来保证最终一致性。\n  保证消息从生产者到MQ或者MQ到消费者的过程在同一个会话中，保证原子性；在事务性会话中，当一个事务被提交的时候，确认自动发生；事务回滚，消息再次传送；一个事务提交才能进行下个事务，效率较差。\n在非事务性会话中，消息何时被确认取决于创建会话时的应答模式ACK模式，分为自动确认(onMessage方法成功返回，如果抛异常会交由异常消息监听器，或者重复次数发送)、手动确认、不必须确认,批量(重复有标记)。\n检测消息丢失的方法\n通过消息队列的有序性来验证是否有消息丢失。生产者发出消息时附加一个连续递增的序号，由消费者来检查这个序号的连续性。在分布式系统上，在发送消息时必须指定分区，消费者在每个分区单独检查消息序号的连续性。\n消息的丢失处理\n生产者消息丢失处理：发送消息时产生一个id，MQ接收到消息后回传id，超过一定时间没收到则重发\nMQ消息丢失处理：开启消息队列持久化 和 消息持久化，持久化后才回传id给生产者\n消费者消息丢失处理：取消自动ack，在方法处理完之后调用方法，发送确认ack给MQ，如果消息处理的时间太长，但可能导致重复发送\n关于pull模式和push模式  push模式的弊端，如果消费者消费能力不够，就会导致消息在消息队列中堆积，消息队列也需要保存这些消息，记录这些消息的状态；而pull模式是消费者按能力消费，所以没有这样问题。 push模式下要保证顺序消息也比较麻烦，需要等待消费者确认一个消息后才能发送下一个，吞吐量就不太行了 pull模式的弊端，因为消费者拉取消息的时间间隔比较难把握，间隔时间不合理就会导致消息消费存在延迟和忙等，常见的作法是消费者建立连接后hold住一段时间，保存一个长连接，设置等待时间进行断开，在这段时间内进行消息消费  如何解决消息队列的延时以及过期失效问题？ 手动查询丢失消息，重新导入\n对于activeMQ，可以设置死信队列，过期或者重复多次为被消费的消息会进入死信队列，activeMQ有提供方法处理死信队列\n如何解决消息积压问题？ 一般消息积压，要不就是发送快了，要不就是消费慢了，一般先查MQ监控，判断生产速度和消费速度是否异常。\n  生产者发送消息太慢也会导致积压，比如，生产者在一个事务内先发送半消息，处理业务逻辑，提交事务，事务成功后才会提交半消息，此时该消息才会被消费者所见，如果事务处理得太慢，也会造成消息堆积。\nso 生产者最好可以支持批量发送或者并行发送两种发送方式；对于实时性不强的业务，生产者可以积累一定量的消息才发送给MQ；对于批量发送的消息，消费时也要批量消费\n  修复消费者，检查消费者是不是对某一消息进行重复消费，恢复消费速度；\n  扩充原来的数量，消费之后再恢复原来架构。比如新键一个topic，建立比原先多n倍的队列，多n倍的消费者处理，每批消费者对应一个队列（分区），确保消费者的实例数和队列(分区)数相等。\n  如果可以持久化消息，耶可以先丢弃消息，之后再将持久化消息导入队列再处理\n  服务降级，减少生产者的发送消息的数量\n  延迟队列 几种延迟队列实现\n参考 消息队列常见问题和解决方案\n如何从0到1设计一个MQ消息队列\nActiveMQ消息传送机制以及ACK机制详解\n架构文摘：消息队列设计精要\n消息队列设计精要，这个讲的不错\n极客时间 - 消息队列高手课 - 李玥\n","date":"2020-09-22T00:00:00Z","permalink":"http://nixum.cc/p/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/","title":"消息队列"},{"content":"[TOC]\n只记录常用设计模式\n设计模式六大原则  单一职责原则(SRP)：一个类只负责一个功能领域中的相应职责，就一个类而言，应该只有一个引起它变化的原因。 开闭原则(OCP)：一个软件实体应当对扩展开放，对修改关闭。即软件实体应尽量在不修改原有代码的情况下进行扩展。 里氏代换原则(LSP)：所有引用基类（父类）的地方必须能透明地使用其子类的对象。 依赖倒转原则(DIP)：抽象不应该依赖于细节，细节应当依赖于抽象。换言之，要针对接口编程，而不是针对实现编程。如 控制反转和依赖注入 接口隔离原则(ISP)：使用多个专门的接口，而不使用单一的总接口，即客户端不应该依赖那些它不需要的接口。 迪米特法则(LoD)：一个软件实体应当尽可能少地与其他实体发生相互作用。  常见设计模式 创建型模式是将创建和使用代码解耦\n结构型模式是将不同功能代码解耦\n行为型模式是将不同的行为代码解耦\n创建型 单例模式 数据在应用上只保持一份，解决资源访问冲突的问题，就可以使用单例模式\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105  /** 1. * 懒汉模式,线程不安全，只有在调用方法的时候才实例化,好处是没用到该类时就不实例化，节约资源 */ class LazyInstance { private static LazyInstance singleton; private LazyInstance() { // if (singleton != null) // throw new RuntimeException();  } /** 1.1 * 想要线程安全只需在方法上加上synchronized关键字，缺点是，多线程访问时锁的操作耗时 */ public static LazyInstance getInstance() { if (singleton == null) { singleton = new LazyInstance(); } return singleton; } } /** 2. * 饿汉模式，直接实例化，线程安全，缺点是丢失了延迟实例化造成资源浪费 */ class HungryInstance { private static final HungryInstance singleton = new HungryInstance(); //加不加final都可以  public static HungryInstance getInstance() { return singleton; } } /** 3. * 双重锁,可在多线程下使用 */ class DoubleCheckedLocking { /** * 注意变量要声明volatile,也需要两次if判断,否则可能因为指令重排序导致在多线程情况下不安全,这个比较难测试 * singleton = new Singleton()不是原子操作，而分为了三个步骤 * 1. 给 singleton 分配内存 * 2. 调用 Singleton 的构造函数来初始化成员变量，形成实例 * 3. 将singleton对象指向分配的内存空间（执行完这步 singleton才是非 null了） * 由于有一个『instance已经不为null但是仍没有完成初始化』的中间状态，而这个时候， * 如果有其他线程刚好运行到第一层if (instance ==null)这里，这里读取到的instance已经不为null了， * 所以就直接把这个中间状态的instance拿去用了，就会产生问题。这里的关键在于线程T1对instance的写操作没有完成， * 线程T2就执行了读操作 **/ private volatile static DoubleCheckedLocking singleton; public static DoubleCheckedLocking getInstance(){ if (singleton == null) { synchronized (DoubleCheckedLocking.class) { if (singleton == null) { singleton = new DoubleCheckedLocking(); } } } return singleton; } } /**4. * 静态内部类模式，利用的是JVM对静态内部类的加载机制 * 因为静态内部类只有被调用的时候才会被初始化，相当于延时的机制，且JVM能保证只初始化一次 * 相当与结合了懒汉模式和饿汉模式的优点吧 */ class StaticInnerClassMode { private static class StaticInnerClassInstance { private static final StaticInnerClassMode SINGLETON = new StaticInnerClassMode(); } public static StaticInnerClassMode getInstance() { return StaticInnerClassInstance.SINGLETON; } } /**5. * 枚举类创建单例,利用JVM的机制,保证只实例化一次,同时可防止反射和反序列化操作破解 */ enum EnumMode { SINGLETON; public void method(){} } /** * 除了枚举类可防止反射和反序列化操作破解外，其他四种方法都会被反射和反序列化破解 * 1，阻止反射破解 * 在空构造方法里，判断singleton是否为空，如果不为空，则抛出RuntimeException， * 因为反射需要通过class.getInstance()调用空参构造方法实例化对象，如果此时抛出异常，则会终止程序， * 如果在懒汉模式里使用就会发现会抛出异常 * * 2.阻止反序列化破解 * 实现Serializable接口，定义readResolve()方法返回对象，具体原理不太清楚 * 在反序列化的时候用readResolve()中返回的对象直接替换在反序列化过程中创建的对象 * private Object readResolve() throws ObjectStreamException { * return instance; * } */   工厂模式   创建型模式\n  定义一个创建对象的接口，让其子类自己决定实例化哪一个工厂类，工厂模式使其创建过程延迟到子类进行，还有另一种工厂需要用到单例，通过在静态代码块里先初始化好对象，然后+key放到map里\n  util.Calendar、util.ResourceBundle、text.NumberFormat、nio.charset.Charset、util.EnumSet、DI容器\n  应用场景：当有一段需要通过if-else的代码来判断初始化哪些对象的时候，就可考虑\n   工厂模式例子类图 \n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49  // 接口 public interface Shape{ void draw(); } // 实体,工厂生产的产品 public class Circle implements Shape { @Override public void draw() { System.out.println(\u0026#34;circle draw\u0026#34;); } } public class Rectangle implements Shape { @Override public void draw() { System.out.println(\u0026#34;Rectangle draw\u0026#34;); } } // 工厂 public class ShapeFactory { //使用 getShape 方法获取形状类型的对象, 生产产品的方法  public Shape getShape(String shapeType){ if(shapeType == null){ return null; } if(shapeType.equalsIgnoreCase(\u0026#34;CIRCLE\u0026#34;)){ return new Circle(); } else if(shapeType.equalsIgnoreCase(\u0026#34;RECTANGLE\u0026#34;)){ return new Rectangle(); } return null; } } // 例子 public class FactoryPatternDemo { public static void main(String[] args) { // 创建工厂  ShapeFactory shapeFactory = new ShapeFactory(); //获取 Circle 的对象，并调用它的 draw 方法， 接口装载子类对象  Shape shape1 = shapeFactory.getShape(\u0026#34;CIRCLE\u0026#34;); shape1.draw();\t// circle draw  //获取 Rectangle 的对象，并调用它的 draw 方法  Shape shape2 = shapeFactory.getShape(\u0026#34;RECTANGLE\u0026#34;); shape2.draw();\t//Rectangle draw  } }   抽象工厂模式  与工厂模式类似，也创建型模式 工厂方法是一个工厂，根据传入参数生产不同实例，而抽象工厂则加多一层工厂获取。抽象工厂属于大工厂，根据传入参数产生工厂实例，在通过这个工厂，传入参数获取对象实例  建造者模式  其实就是链式调用，主要为了解决构造方法参数过多，且需要校验的情况下，如果参数过多且需要校验，使用构造方法或者set方法来实例化对象不太方便，同时，使用建造者模式还可以把对象处理成初始化后属性不可变得对象 工厂模式是用来创建不同但是相关类型的对象（继承同一父类或者接口的一组子类），由给定的参数来决定创建哪种类型的对象。建造者模式是用来创建一种类型的复杂对象，可以通过设置不同的可选参数，“定制化”地创建不同的对象  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81  public class ResourcePoolConfig { private String name; private int maxTotal; private int maxIdle; private int minIdle; private ResourcePoolConfig(Builder builder) { this.name = builder.name; this.maxTotal = builder.maxTotal; this.maxIdle = builder.maxIdle; this.minIdle = builder.minIdle; } //...省略getter方法...  // 建议此处把builder设置为内部类  public static class Builder { // default value  private static final int DEFAULT_MAX_TOTAL = 8; private static final int DEFAULT_MAX_IDLE = 8; private static final int DEFAULT_MIN_IDLE = 0; private String name; private int maxTotal = DEFAULT_MAX_TOTAL; private int maxIdle = DEFAULT_MAX_IDLE; private int minIdle = DEFAULT_MIN_IDLE; public ResourcePoolConfig build() { // 校验逻辑放到这里来做，包括必填项校验、依赖关系校验、约束条件校验等  if (StringUtils.isBlank(name)) { throw new IllegalArgumentException(\u0026#34;...\u0026#34;); } if (maxIdle \u0026gt; maxTotal) { throw new IllegalArgumentException(\u0026#34;...\u0026#34;); } if (minIdle \u0026gt; maxTotal || minIdle \u0026gt; maxIdle) { throw new IllegalArgumentException(\u0026#34;...\u0026#34;); } return new ResourcePoolConfig(this); } public Builder setName(String name) { if (StringUtils.isBlank(name)) { throw new IllegalArgumentException(\u0026#34;...\u0026#34;); } this.name = name; return this; } public Builder setMaxTotal(int maxTotal) { if (maxTotal \u0026lt;= 0) { throw new IllegalArgumentException(\u0026#34;...\u0026#34;); } this.maxTotal = maxTotal; return this; } public Builder setMaxIdle(int maxIdle) { if (maxIdle \u0026lt; 0) { throw new IllegalArgumentException(\u0026#34;...\u0026#34;); } this.maxIdle = maxIdle; return this; } public Builder setMinIdle(int minIdle) { if (minIdle \u0026lt; 0) { throw new IllegalArgumentException(\u0026#34;...\u0026#34;); } this.minIdle = minIdle; return this; } } } // 调用 public class FactoryPatternDemo { public static void main(String[] args) { // 这段代码会抛出IllegalArgumentException，因为minIdle\u0026gt;maxIdle ResourcePoolConfig config = new ResourcePoolConfig.Builder() .setName(\u0026#34;dbconnectionpool\u0026#34;) .setMaxTotal(16) .setMaxIdle(10) .setMinIdle(12) .build(); } }   原型模式  其实就是拷贝，把对于那些创建成本较大(比如创建要进行复杂计算、IO读取等)且同一类但不同对象得对象，利用原有对象进行拷贝，来达到创建新对象的目的 常见方式有序列化再反序列化，或者递归遍历对象里的字段进行创建和赋值，深拷贝或者浅拷贝，这里要注意浅拷贝和深拷贝问题  结构型 代理模式   结构型模式\n  通过代理类扩展被代理类的能力，代理类和被代理类实现同一接口，重写其中的方法，在代理类中传入被代理类的实例，在两者相同的方法中，调用被代理类的该方法，同时可以处理其他逻辑，达到扩展的能力\n  1、和适配器模式的区别：适配器模式主要是两个代表不同维度的接口，它们的实现通过组合的方式扩展原来的功能，而代理模式是代理类和被代理类实现相同的接口，通过代理类调用被代理类相同的方法来达到对代理类补充的作用。\n2、和装饰器模式的区别：装饰器模式为了增强功能，而代理模式是为了加以控制。\n  下面的例子属于静态代理，其他代理请看动态代理和CGLIB代理\n   代理模式例子 \n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52  // 代理类和被代理类的接口 public interface Image { void display(); } // 被代理类 public class RealImage implements Image { private String fileName; public RealImage(String fileName){ this.fileName = fileName; loadFromDisk(fileName); } @Override public void display() { System.out.println(\u0026#34;Displaying \u0026#34; + fileName); } private void loadFromDisk(String fileName){ System.out.println(\u0026#34;Loading \u0026#34; + fileName); } } // 代理类 public class ProxyImage implements Image{ private RealImage realImage; private String fileName; public ProxyImage(String fileName){ this.fileName = fileName; } public ProxyImage(RealImage realImage){ this.realImage = realImage; } @Override public void display() { if(realImage == null){ // 可以选择在这里延迟加载 或者 在构造方法的时候加载 被代理类  realImage = new RealImage(fileName); } // 在调用被代理类同名方法前后做其他操作  realImage.display(); } } // 使用时 public class ProxyPatternDemo { public static void main(String[] args) { Image image = new ProxyImage(\u0026#34;test_10mb.jpg\u0026#34;); // 图像将从磁盘加载  image.display(); // Loading test_10mb.jpg /n Displaying test_10mb.jpg  System.out.println(\u0026#34;\u0026#34;); // 图像不需要从磁盘加载  image.display(); // Displaying test_10mb.jpg  } }   装饰器模式   结构型模式\n  装饰类向被装饰类添加新功能，同时又不改变其结构，作为现被装饰类的包装，继承的一种代替，主要解决多层次继承的问题\n  装饰类和被装饰类可以独立发展，不会相互耦合\n  跟代理模式很像，本质上代码差别不大，只是意图不太一样。\n区别：装饰器可以一层一层装饰，每次装饰可以增强或扩展被装饰者的功能，功能是相关的，是对功能的增强。外部是知道具体的被装饰者的（对装饰器传入被装饰对象），然后不断通过装饰达到对原有功能的增强。\n而代理模式是一层，代理类控制被代理类，控制被代理对象的访问，外部并不关心被代理的对象是谁（被代理类是在代理类内部进行构造，不从外部传入），只知道通过代理对象可以实现对被代理对象的功能补充，是对代理对象的功能补充，直接加强。\n  最典型的例子就是IO类了，每个io类都继承了in/outputStream(带了默认实现)，同时又持有in/outputStream，调用的本质还是在调持有的in/outputStream方法的基础上进行增强\n题外话，为啥BufferedInputStream不直接继承InputStream，而是继承FileInputStream？\n原因是如果BufferInputStream继承并同时持有inputStream，由于BufferedInputStream只对部分方法增加buffer功能，对那些不需要增强的方法的调用就需要显式调用持有的inputStream，而如果先通过FileInputStream继承并组合InputStream，并调用持有的inputStream的默认实现，就不用写这部分多余的代码了\n   装饰模式例子 \n接口Shape表示形状，它的实现类是圆Circle类，统一的抽象装饰类ShapeDecorator，带有Shape类的引用，其 实现的装饰类RedShapeDecorator通过传入具体的形状实例，来对共同的方法做增强\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65  // 接口，作为被装饰类的接口 public interface Shape { void draw(); } // 实现类，作为被装饰类 public class Circle implements Shape { @Override public void draw() { System.out.println(\u0026#34;Shape: Circle\u0026#34;); } } // 接口的抽象装饰类，对非增强方法带有默认实现 public abstract class ShapeDecorator implements Shape { protected Shape decoratedShape; // 每一个装饰器都需要一个被装饰引用，因此需要一个抽象父类  public ShapeDecorator(Shape decoratedShape){ this.decoratedShape = decoratedShape; } // 装饰器类单独使用的默认方法  public void draw(){ decoratedShape.draw(); } } // 装饰类，装饰成红色 public class RedShapeDecorator extends ShapeDecorator { public RedShapeDecorator(Shape decoratedShape) { super(decoratedShape); } @Override public void draw() { decoratedShape.draw(); setRedBorder(decoratedShape); } private void setRedBorder(Shape decoratedShape){ System.out.println(\u0026#34;Border Color: Red\u0026#34;); } } // 装饰类，加深颜色 public class DarkRedShapeDecorator extends ShapeDecorator { public DarkRedShapeDecorator(Shape decoratedShape) { super(decoratedShape); } @Override public void draw() { decoratedShape.draw(); setDarkRedBorder(decoratedShape); } private void setDarkRedBorder(Shape decoratedShape){ System.out.println(\u0026#34;Border Color: DarkRed\u0026#34;); } } // 调用 public class DecoratorPatternDemo { public static void main(String[] args) { Shape circle = new Circle(); Shape redCircle = new RedShapeDecorator(circle); Shape darkRedCircle = new DarkRedShapeDecorator(redCircle); circle.draw();\t// Shape: Circle  redCircle.draw();\t// Border Color: Red  darkRedCircle.draw(); // Border Color: DarkRed  } }   适配器模式  结构型模式 接口适配器使得实现了不同接口的类可以通过适配器的选择而工作，主要是规避接口不兼容的问题，本质是使用一组类和接口充当适配器，包在被适配的类和接口上，具体又是实现或组合 典型例子：Arrays#asList()，Collections#list()，Collections#enumeration()   适配器模式 \n有两个接口AdvancedMediaPlayer和MediaPlayer，它们都有不同的作用，但它们的作用又很相似，AdvancedMediaPlayer可以播放vlc格式或者mp4格式，而MediaPlayer只能单纯的播放，多接口适配\n如果想要让MediaPlayer能播放不同格式的音乐，就需要适配了，适配器实现MediaPlayer接口，根据传入的参数来判断需要实例化哪种播放器，并在播放方法里执行相应播放器的播放方法\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95  // AdvancedMediaPlayer接口 public interface AdvancedMediaPlayer { public void playVlc(String fileName); public void playMp4(String fileName); } // AdvancedMediaPlayer实现类 VlcPlayer public class VlcPlayer implements AdvancedMediaPlayer{ @Override public void playVlc(String fileName) { System.out.println(\u0026#34;Playing vlc file. Name: \u0026#34;+ fileName); } @Override public void playMp4(String fileName) { //什么也不做  } } // AdvancedMediaPlayer实现类 Mp4Player public class Mp4Player implements AdvancedMediaPlayer{ @Override public void playVlc(String fileName) { //什么也不做  } @Override public void playMp4(String fileName) { System.out.println(\u0026#34;Playing mp4 file. Name: \u0026#34;+ fileName); } } // MediaPlayer接口 public interface MediaPlayer { public void play(String audioType, String fileName); } // MediaPlayer适配器，实现了从 AdvancedMediaPlayer接口 到 MediaPlayer接口 的转换 public class MediaAdapter implements MediaPlayer { AdvancedMediaPlayer advancedMusicPlayer; public MediaAdapter(String audioType){ if(audioType.equalsIgnoreCase(\u0026#34;vlc\u0026#34;) ){ advancedMusicPlayer = new VlcPlayer(); } else if (audioType.equalsIgnoreCase(\u0026#34;mp4\u0026#34;)){ advancedMusicPlayer = new Mp4Player(); } } @Override public void play(String audioType, String fileName) { if(audioType.equalsIgnoreCase(\u0026#34;vlc\u0026#34;)){ advancedMusicPlayer.playVlc(fileName); }else if(audioType.equalsIgnoreCase(\u0026#34;mp4\u0026#34;)){ advancedMusicPlayer.playMp4(fileName); } } } // 适配器使用类，从而不用关心 被适配接口 的具体实现类 public class AudioPlayer implements MediaPlayer { MediaAdapter mediaAdapter; @Override public void play(String audioType, String fileName) { //播放 mp3 音乐文件的内置支持  if(audioType.equalsIgnoreCase(\u0026#34;mp3\u0026#34;)){ System.out.println(\u0026#34;Playing mp3 file. Name: \u0026#34;+ fileName); } //mediaAdapter 提供了播放其他文件格式的支持  else if(audioType.equalsIgnoreCase(\u0026#34;vlc\u0026#34;) || audioType.equalsIgnoreCase(\u0026#34;mp4\u0026#34;)){ mediaAdapter = new MediaAdapter(audioType); mediaAdapter.play(audioType, fileName); } else{ System.out.println(\u0026#34;Invalid media. \u0026#34;+ audioType + \u0026#34; format not supported\u0026#34;); } } } // 调用 public class AdapterPatternDemo { public static void main(String[] args) { AudioPlayer audioPlayer = new AudioPlayer(); audioPlayer.play(\u0026#34;mp3\u0026#34;, \u0026#34;beyond the horizon.mp3\u0026#34;);\t// Playing mp3 file. Name: beyond the horizon.mp3  audioPlayer.play(\u0026#34;mp4\u0026#34;, \u0026#34;alone.mp4\u0026#34;); // Playing mp4 file. Name: alone.mp4  audioPlayer.play(\u0026#34;vlc\u0026#34;, \u0026#34;far far away.vlc\u0026#34;); // Playing vlc file. Name: far far away.vlc  audioPlayer.play(\u0026#34;avi\u0026#34;, \u0026#34;mind me.avi\u0026#34;); // Invalid media. avi format not supported  } }   其他例子，以下例子来自极客时间-设计模式之美，都是单接口适配\n1、封装外部sdk接口\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  public class CD { //这个类来自外部sdk，我们无权修改它的代码  public static void staticFunction1() { //... }  public void uglyNamingFunction2() { //... }  public void tooManyParamsFunction3(int paramA, int paramB, ...) { //... }  public void lowPerformanceFunction4() { //... } } / 使用适配器模式进行重构 public class ITarget { void function1(); void function2(); void fucntion3(ParamsWrapperDefinition paramsWrapper); void function4(); //... } // 注意：适配器类的命名不一定非得末尾带Adaptor public class CDAdaptor extends CD implements ITarget { //...  public void function1() { super.staticFunction1(); } public void function2() { super.uglyNamingFucntion2(); } public void function3(ParamsWrapperDefinition paramsWrapper) { super.tooManyParamsFunction3(paramsWrapper.getParamA(), ...); } public void function4() { //...reimplement it...  } }   2、统一多个类的接口\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59  public class ASensitiveWordsFilter { // A敏感词过滤系统提供的接口  //text是原始文本，函数输出用***替换敏感词之后的文本  public String filterSexyWords(String text) { // ...  } } public class BSensitiveWordsFilter { // B敏感词过滤系统提供的接口  public String filter(String text) { //...  } } public class CSensitiveWordsFilter { // C敏感词过滤系统提供的接口  public String filter(String text, String mask) { //...  } } // 未使用适配器模式之前的代码：代码的可测试性、扩展性不好 public class RiskManagement { private ASensitiveWordsFilter aFilter = new ASensitiveWordsFilter(); private BSensitiveWordsFilter bFilter = new BSensitiveWordsFilter(); private CSensitiveWordsFilter cFilter = new CSensitiveWordsFilter(); public String filterSensitiveWords(String text) { String maskedText = aFilter.filterSexyWords(text); maskedText = bFilter.filter(maskedText); maskedText = cFilter.filter(maskedText, \u0026#34;***\u0026#34;); return maskedText; } } // 使用适配器模式进行改造 public interface ISensitiveWordsFilter { // 统一接口定义  String filter(String text); } public class ASensitiveWordsFilterAdaptor implements ISensitiveWordsFilter { private ASensitiveWordsFilter aFilter; public String filter(String text) { String maskedText = aFilter.filterSexyWords(text); maskedText = aFilter.filterPoliticalWords(maskedText); return maskedText; } } //...省略BSensitiveWordsFilterAdaptor、CSensitiveWordsFilterAdaptor... // 扩展性更好，更加符合开闭原则，如果添加一个新的敏感词过滤系统， // 这个类完全不需要改动；而且基于接口而非实现编程，代码的可测试性更好。 public class RiskManagement { private List\u0026lt;ISensitiveWordsFilter\u0026gt; filters = new ArrayList\u0026lt;\u0026gt;(); public void addSensitiveWordsFilter(ISensitiveWordsFilter filter) { filters.add(filter); } public String filterSensitiveWords(String text) { String maskedText = text; for (ISensitiveWordsFilter filter : filters) { maskedText = filter.filter(maskedText); } return maskedText; } }   3、替换依赖的外部系统\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  / 外部系统A public interface IA { //...  void fa(); } public class A implements IA { //...  public void fa() { //... } } // 在我们的项目中，外部系统A的使用示例 public class Demo { private IA a; public Demo(IA a) { this.a = a; } //... } Demo d = new Demo(new A()); // 将外部系统A替换成外部系统B public class BAdaptor implemnts IA { private B b; public BAdaptor(B b) { this.b= b; } public void fa() { //...  b.fb(); } } // 借助BAdaptor，Demo的代码中，调用IA接口的地方都无需改动， // 只需要将BAdaptor如下注入到Demo即可。 Demo d = new Demo(new BAdaptor(new B()));   桥接模式  结构型模式 抽象与实现分离，桥接接口类和抽象类，实现解耦，这里其实并不一定是要抽象类，抽象类只是代表一组现实的抽象，即一个类存在两个（或多个）独立变化的维度，通过组合的方式，让这两个（或多个）维度可以独立进行扩展 典型例子：JDBC，仅修改Class.forName(\u0026ldquo;com.mysql.jdbc.Driver\u0026rdquo;)，即可把驱动换成别的数据库  JDBC的做法是提供Driver接口，数据库厂商实现该接口提供不同的数据库能力，并调用DriverManager进行注册，后续通过DriverManager获取connection并进行CRUD操作，都由DriverManager委派给具体的Driver做\n 桥接模式例子 \n桥接接口类和抽象类，接口类实现上色，抽象类实现形状，在抽象类中引入接口，通过不同的组合实现不同的功能\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42  // 桥接实现接口， 连接 画 和 上色 public interface DrawAPI { public void drawCircle(int radius, int x, int y); } // 不同实现类，红色 public class RedCircle implements DrawAPI { @Override public void drawCircle(int radius, int x, int y) { System.out.println(\u0026#34;Drawing Circle[ color: red, radius: \u0026#34; + radius +\u0026#34;, x: \u0026#34; + x +\u0026#34;, \u0026#34;+ y +\u0026#34;]\u0026#34;); } } // 使用 DrawAPI 接口创建抽象类 Shape public abstract class Shape { protected DrawAPI drawAPI; protected Shape(DrawAPI drawAPI){ this.drawAPI = drawAPI; } public abstract void draw(); } // Shape不同得实现类 public class Circle extends Shape { private int x, y, radius; public Circle(int x, int y, int radius, DrawAPI drawAPI) { super(drawAPI); this.x = x; this.y = y; this.radius = radius; } public void draw() { drawAPI.drawCircle(radius,x,y); } } // 调用 public class BridgePatternDemo { public static void main(String[] args) { Shape redCircle = new Circle(100,100, 10, new RedCircle()); redCircle.draw();\t// Drawing Circle[ color: red, radius: 10, x: 100, 100]  } }   代理模式、桥接模式、装饰器模式、适配器模式的区别  **代理模式：**代理模式在不改变原始类接口的条件下，为原始类定义一个代理类，主要目的是控制访问，而非加强功能，这是它跟装饰器模式最大的不同。\n**桥接模式：**桥接模式的目的是将接口部分和实现部分分离，从而让它们可以较为容易、也相对独立地加以改变。\n**装饰器模式：**装饰者模式在不改变原始类接口的情况下，对原始类功能进行增强，并且支持多个装饰器的嵌套使用。\n**适配器模式：**适配器模式是一种事后的补救策略。适配器提供跟原始类不同的接口，而代理模式、装饰器模式提供的都是跟原始类相同的接口。\n 其实这几种设计模式在代码层面上是很相似的，本质只是设计意图的不同，应对的场景不同\n门面(外观)模式  结构型模式 本质是对接口的组合，比如有子系统或者子模块提供了b、c、d接口，都是些职责比较单一的接口，可以在上层提供一个大而全的接口，使用到了b、c、d接口提供的功能，简化了调用者的调用关系处理  组合模式   结构型模式\n  本质是多叉树，根节点和子节点继承或实现同一个接口，以方便递归处理树形结构的数据\n  主要处理树形结构数据\n  享元模式  结构型模式 享元，即共享单元，一般是通过复用不可变对象达到节省内存的作用，通过map + 工厂模式来达到复用的目的，但是对于GC并不友好，如果该对象并不常用，也可以使用弱引用或软引用相关的hashMap来存储，方便垃圾回收 典型例子：java中的Integer类，对于多个-128~127的包装类型对象，底层的内存地址是同一个，Integer类里有个IntegerCache内部类，相当于享元对象的工厂，缓存着-128~127之间的数据，类似的，如 Long、 Short、 Byte包装类型也用到了这种方法；还有String类的字符串常量池，会缓存string字面量，String类也提供了intern方法方便我们将字符串存入常量池  行为型 责任链模式  行为型模式 每个接收者都包含对另一个接收者的引用。如果一个对象不能处理该请求，那么它会把相同的请求传给下一个接收者，依此类推，即：将所有接受者连成一条链，请求沿着这条链传递，直到有对象处理 典型例子：servlet filter、spring interceptor、logger   责任链模式例子 \n这是模拟日志级别打印的例子：日志抽象类规定每个结点的日志等级和需要重写的方法，参数传递处理的方法，可以看成一个链表上结点的抽象；具体的类实现该抽象类重写共同方法当成每一个结点，最后将这些结点连成链即可\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89  // 创建抽象的记录器类 public abstract class AbstractLogger { protected int level; // 责任链中的下一个元素  protected AbstractLogger nextLogger; public void setNextLogger(AbstractLogger nextLogger){ this.nextLogger = nextLogger; } // 判断是否由当前结点处理或者交由下个结点处理  public void logMessage(int level, String message){ // 根据leve判断  if(this.level \u0026lt;= level){ write(message); } if(nextLogger != null){ nextLogger.logMessage(level, message); } } abstract protected void write(String message); } // 日志等级 public class LoggerLevel { public static final int INFO = 1; public static final int DEBUG = 2; public static final int ERROR = 3; } // 记录类的实现类，链上的结点 public class ConsoleLogger extends AbstractLogger { public ConsoleLogger(int level){ this.level = level; } @Override protected void write(String message) { System.out.println(\u0026#34;Standard Console::Logger: \u0026#34; + message); } } public class FileLogger extends AbstractLogger { public FileLogger(int level){ this.level = level; } @Override protected void write(String message) { System.out.println(\u0026#34;File::Logger: \u0026#34; + message); } } public class ErrorLogger extends AbstractLogger { public ErrorLogger(int level){ this.level = level; } @Override protected void write(String message) { System.out.println(\u0026#34;Error Console::Logger: \u0026#34; + message); } } // 创建链 public class ChainPatternDemo { private static AbstractLogger getChainOfLoggers(){ // 创建每一个结点，设定每个结点的level  AbstractLogger errorLogger = new ErrorLogger(LoggerLevel.ERROR); AbstractLogger fileLogger = new FileLogger(LoggerLevel.DEBUG); AbstractLogger consoleLogger = new ConsoleLogger(LoggerLevel.INFO); // 形成链  errorLogger.setNextLogger(fileLogger); fileLogger.setNextLogger(consoleLogger); // 返回头结点  return errorLogger; } public static void main(String[] args) { AbstractLogger loggerChain = getChainOfLoggers(); // 每个结点根据创建来的参数来判断是否执行或者交由下一个结点  loggerChain.logMessage(LoggerLevel.INFO, \u0026#34;This is an information.\u0026#34;); // 输出： Standard Console::Logger: This is an information.  loggerChain.logMessage(LoggerLevel.DEBUG, \u0026#34;This is an debug level information\u0026#34;); // 输出：  // File::Logger: This is an debug level information. \t// Standard Console::Logger: This is an debug level information.  loggerChain.logMessage(LoggerLevel.ERROR, \u0026#34;This is an error information.\u0026#34;); // 输出： \t// Error Console::Logger: This is an error information. \t// File::Logger: This is an error information. \t// Standard Console::Logger: This is an error information.  } }   迭代器模式  行为型模式 用于顺序访问集合对象的元素，不需要知道集合对象的底层表示，不会暴露该对象的内部表示，迭代器内部使用游标记录当前位置信息，每个迭代器独享游标信息，这样当我们创建不同的迭代器对不同的容器进行遍历的时候就不会互相影响 针对复杂的数据结构，比如树、图的遍历，使用迭代器模式会更加有效 对于迭代过程中通过对容器增加或删除元素会有问题， java使用fail-fast机制，即遍历每个元素的时候会比较modCount的值和调用迭代器时的expectedModCount的值比较来实现fail-fast，达到报错的目的。此外，如果想要删除，需要通过迭代器来删除才可以，java中使用lastRet变量来记录上一个游标，以保证在删除当前元素后，游标能正确指向。   迭代器模式例子 \n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46  // 迭代器接口 public interface Iterator { public boolean hasNext(); public Object next(); } // 集合接口 public interface Container { public Iterator getIterator(); } // 包含有迭代器的集合 public class NameRepository implements Container { public String names[] = {\u0026#34;Robert\u0026#34; , \u0026#34;John\u0026#34; ,\u0026#34;Julie\u0026#34; , \u0026#34;Lora\u0026#34;}; @Override public Iterator getIterator() { return new NameIterator(); } // 内部类  private class NameIterator implements Iterator { int index; @Override public boolean hasNext() { if(index \u0026lt; names.length){ return true; } return false; } @Override public Object next() { if(this.hasNext()){ return names[index++]; } return null; } } } // 调用 public class IteratorPatternDemo { public static void main(String[] args) { NameRepository namesRepository = new NameRepository(); // 创建集合  for(Iterator iter = namesRepository.getIterator(); iter.hasNext();){ String name = (String)iter.next(); System.out.println(\u0026#34;Name : \u0026#34; + name); } } }   访问者模式  行为型 允许一个或者多个操作应用到一组对象上，解耦操作和对象本身，比如对于多种文件类型，可以使用多种不同的执行器作用在不同的文件上  备忘录模式（快照）   行为型\n  一般栈来保存副本，入栈的元素都会叠加上一个元素的内容，以此来实现顺序撤销和恢复功能\n  主要是用来防丢失、撤销、恢复\n  状态模式  行为型 状态 -\u0026gt; 事件 -\u0026gt; 动作 -\u0026gt; 状态改变 或者 状态 -\u0026gt; 事件 -\u0026gt; 状态改变 -\u0026gt; 动作 分支逻辑法 或者 查表法 或 通过将分支判断抽成类来实现上述 当状态接收到事件后进行业务逻辑动作后改变状态  观察者模式   行为型\n  在一对多关系中，当一个对象被修改时，则会自动通知它的依赖对象\n  典型例子：消息队列的发布/订阅模型\n  硬要说的话，观察者模式和发布订阅模式还是有一定差别的\n观察者是当被观察者有状态发生改变时，通知观察者；而发布订阅是发布者把消息丢到消息队列，消息队列根据消息发给对应的订阅者，区别就是有没有第三方存在，消息的双方知不知道彼此的存在\n发布订阅：\n   观察者模式例子 \n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71  // 被观察者，当状态发生改变时，通知观察者 public class Subject { // 观察者队列  private List\u0026lt;Observer\u0026gt; observers = new ArrayList\u0026lt;Observer\u0026gt;(); private int state;\t// 状态  public int getState() { return state; } // 状态改变时通知观察者  public void setState(int state) { this.state = state; notifyAllObservers(); } // 绑定观察者  public void attach(Observer observer){ observers.add(observer); } // 通知观察者，执行观察者方法  public void notifyAllObservers(){ for (Observer observer : observers) { observer.update(); } } } // 观察者抽象类 public abstract class Observer { protected Subject subject; public abstract void update(); } // 观察者实现类 public class BinaryObserver extends Observer{ // 绑定被观察者  public BinaryObserver(Subject subject){ this.subject = subject; this.subject.attach(this); } // 执行观察者类  @Override public void update() { System.out.println(\u0026#34;Binary String:\u0026#34; + Integer.toBinaryString(subject.getState() )); } } // 观察者实现类 public class OctalObserver extends Observer{ // 绑定被观察者  public OctalObserver(Subject subject){ this.subject = subject; this.subject.attach(this); } // 执行观察者类  @Override public void update() { System.out.println(\u0026#34;Octal String:\u0026#34; + Integer.toOctalString(subject.getState())); } } // 调用 public class ObserverPatternDemo { public static void main(String[] args) { Subject subject = new Subject(); new OctalObserver(subject); new BinaryObserver(subject); System.out.println(\u0026#34;First state change: 15\u0026#34;); subject.setState(15);\t// Octal String: 17 /n Binary String: 1111  System.out.println(\u0026#34;Second state change: 10\u0026#34;); subject.setState(10);\t//Octal String: 12 /n Binary String: 1010  } }   模板方法  行为型 一个抽象类抽取一些通用方法合并成新方法并用final修饰作为模板，它的子类继承此抽象类，通过重写通用方法，来实现不一样的模板方法 典型例子：JUC包里的AQS和其子类、util.Collections#sort()、InputStream#skip()、InputStream#read()、servlet、junit 模板方法与回调的区别：回调的作用与模板方法类似，都是通过自由替换某个方法来实现不同的功能，但回调基于组合关系，而模板方法基于继承关系，同步回调类似模板方法，异步回调类似观察者模式   模板方法例子 \n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42  // 抽象模板类 public abstract class Game { // 给子类重写  abstract void initialize(); abstract void startPlay(); abstract void endPlay(); //模板  public final void play(){ //初始化游戏  initialize(); //开始游戏  startPlay(); //结束游戏  endPlay(); } } // 子类继承并重写 public class Cricket extends Game { @Override void endPlay() { System.out.println(\u0026#34;Cricket Game Finished!\u0026#34;); } @Override void initialize() { System.out.println(\u0026#34;Cricket Game Initialized! Start playing.\u0026#34;); } @Override void startPlay() { System.out.println(\u0026#34;Cricket Game Started. Enjoy the game!\u0026#34;); } } // 调用 public class TemplatePatternDemo { public static void main(String[] args) { Game game = new Cricket(); game.play(); // Cricket Game Initialized! Start playing. \t// Cricket Game Started. Enjoy the game! \t// Cricket Game Finished!  } }   策略模式  行为型 通过定义一个通用策略接口 + 一组策略的实现，由调用者在运行时动态选择某一策略完成业务逻辑，常用搭配是 策略 + 工厂模式，根据不同类型选择不同策略 常用的场景是使用策略模式来避免膨胀的分支判断，缺点是策略类会变多，另外，使用查表法也可避免膨胀的分支判断  命令模式  行为型 将函数方法封装成对象，类似c语言的函数指针，数据驱动，将事件和数据封装成对象（命令），最后执行，有点像策略模式，但是命令模式更侧重将行为请求者和实现者解耦  解释器模式  行为型 针对某种语言制定对应的语法，在代码中的体现是，对某种模式的解析抽成方法，降低代码复杂度 典型例子：后缀表达式的解析、sql解析、正则表达式解析  中介模式  行为型 引入中间层，将原来多对多的关系转换为一对多的关系，解耦对象间的关系，类似观察者模式，区别在于观察者模式中，数据的流向、对象间的关系是单向的，观察者就是观察者、被观察者就是被观察者，不会轻易改变，而中介模式是由一个中间对象来处理各个对象间的关系，类似数据总线 典型例子：聊天室  对于EventBus，个人认为它不是中介模式，硬要说话，处理事件的中心类，也算是中介，但是主要是发送事件的对象和处理事件的对象并没有很直接的关系，事件处理者仅关心接收到的是什么事件，并不关心事件是由谁发出的\n领域驱动模型 DDD 贫血模型，面向过程，业务逻辑和实体分开，实体仅声明需要的属性，业务逻辑的处理发生在service类，实体只用来存数据，比如各种vo、bo、do\n领域驱动模型是一种面向对象的模型、充血模型，更多是用在微服务、业务复杂的场景\n具体表现在 业务逻辑的处理是发生在实体类里，而不是发生在service类里，service类只是对各种vo、bo、do转换成需要的领域对象，对它们进行调度，执行它们的方法来完成业务，service甚至可以不感知领域对象的属性变化，与controller和dao层打交道，解耦流程性代码和业务代码，负责非功能性或者与第三方系统交互的工作，比如分布式事务、邮件、消息、rpc调用等，使得领域对象方法和属性复用性提高，业务更加内聚\n其他 事件驱动模型 实际上是使用了观察者模式和状态模式来实现的，比较直观的例子就是android的EventBus，chrome的V8引擎都有用到此模型，这里仅总结并进行简单介绍\n \n这里的demo是项目中使用到的组件的一个简化，真实的组件要比这个复杂的多，这里只简单罗列出基本的原理和优缺点。\n原理：\n 事件驱动-状态机的异步模型，本质上是底层controller维护一个阻塞队列，将外部请求转化为事件，通过事件在内部传递。 controller接收到请求，从对象复用池中获取一个上下文context并init，然后将事件交由context处理。context内有一套状态的扭转的控制流程，在不同的状态接收事件对业务逻辑进行处理，最后将处理结果交由注册的回调函数异步或者同步返回。 每一个状态在处理完当前逻辑操作后将发送事件给阻塞队列，并扭转为下一个状态，等待下一个事件的到来。 由于controller是单线程的，各个状态在处理的时候要求速率尽可能的快，以至于不会阻塞主线程，因此在controller内部还维护了一个延迟队列，用来接收延迟事件，状态通常在进行业务处理前会起一个定时器，如果超时将发送延迟事件给到延迟队列，来避免当前操作过长导致阻塞主线程，定时器由下一个状态来取消。 一般会为每个请求分配id，每个id对应一个上下文context，上下文一般使用id + Map来实现同一个请求下的上下文切换、保存和恢复，使用对象复用池来避免上下文对象频繁初始化 这套模型一般应用在中间件的设计上，当然也可以抽成通用框架，在写的时候就会发现，其实变化最多的是状态流程那一块，所以完全可以把这块抽出来 + netty进行网络通信就能搭出一套web框架出来了  优点：\n  是一个单线程的模型，本身就是线程安全的。\n  理论上一套业务逻辑拆分成小逻辑，交由不同的状态操作，各个状态的操作时间要求尽可能的短，不然会阻塞主线程\n  各个状态在进行逻辑操作时，如果处理的时间过长，一般会使用线程池+回调+事件的方式处理\n  状态机模式对应业务逻辑流程有比较强的控制，各个状态对应不同的职责\n  对CPU的利用率比较高，吞吐量比较高，因为可以一次处理多种业务请求，每个业务请求都能进行拆分进行异步处理，速率比较快，因此性能会比常用的Spring全家桶好很多吧，至少在项目使用中的感受是这样\n  缺点：\n 处理请求时会初始化一个上下文context来处理，所有异步操作的结果会暂存在上下文中，对内存的占用会比较高，对上下文里的参数存储也需要一套规范 模型相对来讲还是比较复杂，运用多种设计模式，包含了一些回调，需要有一定的设计模式基础，容易劝退 对象复用池，对象回收，线程池的操作，一不小心会造成内存泄漏；还要注意不能阻塞主线程，因此需要配合定时器进行处理 异步处理导致debug会麻烦一些，需要完善的日志调用链来补充  参考 设计模式六大原则\n菜鸟|设计模式\n","date":"2020-09-22T00:00:00Z","permalink":"http://nixum.cc/p/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/","title":"设计模式"},{"content":"[TOC]\nRPC  \n一个RPC框架的基本实现，高性能网络传输、序列化和反序列化、服务注册和发现，如果是实现客户端级别的服务注册和发现，还可以在SDK中提供容错、负载均衡、熔断、降级等功能。\n客户端发起RPC调用，实际上是调用该RPC方法的桩，它和服务端提供的RPC方法有相同的方法签名，或者说实现了相同的接口，只是这个桩在客户端承担的是请求转发的功能，向客户端屏蔽调用细节（比如向发现与注册中心查询要请求的服务方的url），使其像在使用本地方法一样；服务端在收到请求后，由其RPC框架解析出服务名和请求参数，调用在RPC框架中注册的该接口的真正实现者，最后将结果返回给客户端。\n一个简单的RPC实现可以由三部分组成：规定远程的接口和其实现，服务端提供接口注册和IO连接，客户端IO连接和接口代理\n  首先是定义要提供的远程接口和其实现类\n  服务端使用线程池处理IO，实现多路复用，使用socket去循环accept()，每个请求建立一个线程\n线程里注册远程接口实例，使用InputStream接收客户端发送的参数，如接口的字节文件，判断是哪个接口，哪个方法，什么参数；接收后反射调用接口方法，将结果通过OutputStream发送回客户端\n客户端在发送参数可以做一个封装，加入id，服务端处理得到结果后也加入此id，返回回去，表示此次调用完成\n  客户端使用接口，动态代理的方式调用方法，在动态代理的实现里使用IO连接服务端，将远程接口字节码、方法参数这些东西做一个封装发送给服务端，等待返回结果，IO接收是阻塞的\n  参考【Java】java实现的远程调用例子 rpc原理\nRPC原理及RPC实例分析\n异步通信 优点：解耦，减少服务间的依赖，获得更大的吞吐量，削峰，把抖动的吞吐量变得均匀。\n缺点：业务处理变得复杂，比如引入新的中间件，意味着要维护多一套东西，有时可能还得保证消息顺序，失败重传，幂等等处理，比较麻烦；异步也导致了debug的时候比较麻烦；\n定时轮询 发送方请求接收方进行业务处理，接收方先直接返回，之后接收方在自己处理，最后将结果保存起来，发送方定时轮询接收方，获取处理结果。\n回调 发送方请求接收方进行业务处理时，带上发送方结果回调的url，接收方接收到请求后先立刻返回，之后接收方在自己处理，当处理结果出来时，调用发送方带过来的回调url，将处理结果发送给发送方。\n同理在于服务内部的异步回调，也是如此，只是把url换成了callback方法，比如Java中的Future类+Callable类。\n发布订阅 主要靠消息队列实现，不过比较适合发送方不太care处理结果的，如果care处理结果，可以再通过一条队列将结果传递下去，执行后面的处理。\n事件驱动 + 状态机 可以依靠消息队列，本质还是发布订阅那一套，只是将触发的条件换成事件，消费者根据不同的事件触发不同的逻辑，然后再通过状态机保证处理事件顺序。\n比较常见的场景是电商业务中围绕订单服务的一系列业务处理，比如订单创建完成后，订单服务发出订单创建的事件，对应库存服务，收到该事件，就会进行锁库操作等\n","date":"2020-09-09T00:00:00Z","permalink":"http://nixum.cc/p/rpc%E4%B8%8E%E5%BC%82%E6%AD%A5%E8%AE%BE%E8%AE%A1/","title":"RPC与异步设计"},{"content":"[TOC]\n分布式理论 一个分布式系统最多只能满足 C、A、P 这三项中的两项。\nCAP理论 CAP特性\n C：Consistency，一致性，数据状态转化一致，写操作完成后的读操作，可以获取到最新的值 A：Availability，可用性，指的是服务一直可用，可以正常响应 P：Partition tolerance，分区容错，指的是当有节点故障不连通时，就会分区，但仍然能对外提供服务  矛盾在于这三个特性不能同时满足，比如\n 当分布式集群内有两个主从服务发生网络故障，但此时服务仍然可以访问，此时具有分区容错性。\n当对主服务对数据进行修改时，由于网络问题，无法同步到从服务，当访问到从服务时，无法获取到最新的值，此时满足可用性，但是无法满足一致性。\n当主从服务间网络恢复，写操作的数据虽然能在服务间同步了，但还未同步完成，此时访问从服务无法获取最新值，此时满足了一致性，但是无法满足可用性。\n简单概括，只要满足分区容错，就会设置复制集，复制集同时也保证了可用，但是复制集又会有数据同步，此时又有一致性问题\n 所以，一般只会满足其中两个\n 1、满足CA舍弃P，也就是满足一致性和可用性，舍弃容错性。但是这也就意味着你的系统不是分布式的了，因为涉及分布式的想法就是把功能分开，部署到不同的机器上。\n2、满足CP舍弃A，也就是满足一致性和容错性，舍弃可用性。如果你的系统允许有段时间的访问失效等问题，这个是可以满足的。就好比多个人并发买票，后台网络出现故障，你买的时候系统就崩溃了。\n3、满足AP舍弃C，也就是满足可用性和容错性，舍弃一致性。这也就是意味着你的系统在并发访问的时候可能会出现数据不一致的情况。\n 所以为了分布式服务能正常使用，一般时会满足分区容错性和可用性，在一致性上不追求强一致性，而是一个逐渐一致的过程。\nBASE理论 BASE理论是对CAP三者均衡的结果，基于CAP理论演化而来，通过牺牲强一致性来获得高可用。\n Basically Available（基本可用）: 允许暂时不可用，比如访问时可以等待返回，服务降级，保证核心可用等。 Soft state（软状态）: 允许系统存在中间状态，而该中间状态不会影响系统整体可用性，比如允许复制集副本间的数据存在延时，数据库的数据同步过程。 Eventually consistent（最终一致性）: 系统中的所有数据副本经过一定时间后，最终能够达到一致的状态。  与数据库ACID类似，只是强度减弱了\n参考：CAP 定理的含义\n关于可靠性、可用性、稳定性：   可靠性Reliability：不出事故，故障率低，关注的是系统无故障地持续运行的概率，比如\n  MTBF（Mean Time Between Failure）：即平均无故障时间，是指从新的产品在规定的工作环境条件下开始工作到出现第一个故障的时间的平均值。MTBF越长表示可靠性越高，正确工作能力越强 。\n  MTTR（Mean Time To Repair）：即平均修复时间，是指可修复产品的平均修复时间，就是从出现故障到修复中间的这段时间。MTTR越短表示易恢复性越好。\n  MTTF（Mean Time To Failure）：即平均失效时间。系统平均能够正常运行多长时间，才发生一次故障。系统的可靠性越高，平均无故障时间越长。\n  与可用性的关系：Availability = UpTime/(UpTime+DownTime) = MTBF / (MTBF + MTTR)\n  可用性Availability：不出事故，如果出事故可以快速恢复，关注的是服务运行持续时间，比如\n   通俗叫法 可用性级别 年度宕机时间 周宕机时间 每天宕机时间     1个9 90% 36.5天 16.8小时 2.4小时   2个9 99% 87.6小时 1.68小时 14分钟   3个9 99.9% 8.76小时 10.1分钟 86秒   4个9 99.99% 52.6分钟 1.01分钟 8.6秒   5个9 99.999% 5.26分钟，315.36秒 6.05秒 0.86秒      稳定性Stability：服务性能稳定，不时快时慢，关注的是在一个运行周期内，一定压力下，持续操作时间内的出错概率，性能优劣等\n  分布式事务 一、2PC 需要有协调者和参与者，协调者负责调度，参与者负责执行，分两步完成，1：prepare阶段 2：commit阶段。\n2PC是强一致性的，保证原子性和隔离性。在执行阶段，节点是处于阻塞状态，直到commit阶段完成，本地事务才会释放资源，因此性能不佳，一般用在强一致性、并发量不大的场景。\n正常情况下 prepare阶段：协调者向参与者A、B发送请求执行操作，参与者A、B开启事务，执行操作，但不commit，操作完成后，告诉协调者已经完成。\n**commit阶段：**协调者收到参与者的完成响应，向参与者A、B发送commit请求，参与者A、B收到commit请求后，提交事务，完成操作；如果收到执行失败的响应，则发送回滚请求给参与者A、B，执行回滚。\n异常情况下 在协调者等待参与者的完成响应时，协调者或参与者可能宕机，最终会导致数据不一致或阻塞，例如\n  场景：prepare阶段，参与者没有收到询问请求或者参与者的回应没有到达协调者\n解决办法：协调者做超时处理，一旦超时，当作失败，也可以重试\n  场景：commit阶段，正式提交发出后，如果有的参与者没有收到，或者参与者提交/回滚的确认信息没有返回，即参与者的回应超时\n解决方法：协调者或者参与者进行重试，或者把参与者标记为问题节点移除集群\n  **场景：**当处于prepare阶段和commit阶段之间时，协调者挂掉或重启，会导致协调者收不到参与者的响应，此时协调者就不清楚接下来的commit要发送什么请求过去，或者就不发请求过去了，导致参与者一直阻塞\n**解决办法：**协调者维护一份事务日志，以方便宕机重启后恢复原来的状态，但无法为参与者设置超时自动操作，因为它并不知道commit阶段自己要进行commit还是回滚\n  **场景：**当处于prepare阶段和commit阶段之间时，参与者挂掉后，接收不到协调者的请求，不知道接下来要执行commit还是回滚，协调者也无法在参与者挂掉后进行回滚操作\n**解决办法：**3pc\n  二、3PC 3pc实际上就是将commit阶段拆成两步，preCommit相当于一次保险阶段，作用类似于2pc的二阶段，但是它不是正真的提交\n正常情况下 **canCommit阶段：**协调者向参与者发送请求，参与者开启事务执行操作，成功完成后响应Yes，否则响应No。\n**preCommit阶段：**协调者收到所有参与者的Yes响应，发送操作请求给所有参与者，告诉所有参与者进行预提交状态。如果参与者能收到PreCommit请求，意味着它知道大家都同意提交了，即使不commit阶段丢失，参与者也可以在超时之后进行事务提交。\n**commit阶段：**协调者收到所有参与者的应答响应，向所有参与者发送commit请求，参与者收到后提交事务。\n异常情况下 如果在preCommit阶段到commit阶段之间，协调者挂了，参与者会在超时后进行事务提交\n三、TCC 补偿事务，每一个操作都要有对应的确认和补偿，类似于2pc，但2pc在于DB层面，TCC在于业务层面，每个业务逻辑都需要实现try-confirm-cancel的操作\nTry阶段：对于操作的数据行，增加字段表示其状态，表示正在操作，预留必须的资源\nConfirm阶段：只操作预留资源，将try阶段中表示数据状态的字段修改为确认状态，表示已经完成操作，操作需要幂等\nCancel阶段：将try阶段进行的操作进行回滚\n通过不断重试，并发的时候还是需要分布式锁\n比如，在订单创建并减库存的场景中，\nTry阶段：订单创建时发出订单创建的事件，此时订单状态是创建中，库存服务接收到后，并不是直接减库存，而是先冻结库存，将要扣减的库存数冻结起来，订单创建完成；\nConfirm阶段：订单支付完成后，发出订单支付完成的事件，库存服务接收到后，才是真正的减库存\n[](https://github.com/Nixum/Java-Note/raw/master/picture/TCC Try-Confirm流程.png)\nCancel阶段：如果在Try阶段出错，则对已执行Try请求的服务执行回滚，对库存进行回滚\n[](https://github.com/Nixum/Java-Note/raw/master/picture/TCC Try-Cancel流程.png)\n四、本地消息表 适合解决分布式最终一致性问题。\n正常情况下 参与者A正常进行数据库事务并提交，将涉及到对参与者B的操作记录到本地消息表中，相当于一条日志，然后再用一个异步服务，读取该条日志控制参与者B进行相关操作，如果失败了直接重试即可，保证参与者A与B数据最终一致性。注意：参与者A的数据库操作与日志记录是一个原子性操作。\n异步消费操作可以利用RocketMQ事务\n事务性消息：本地事务和发送消息是原子性操作\nhttps://www.jianshu.com/p/53324ea2df92\nhttp://blog.itpub.net/31556438/viewspace-2649246/\n五、SAGA 利用状态机实现\n六、简单通过可靠消息和最终一致性方案实现 \n另外，在可靠消息服务中，更新数据库里的消息和将消息发送给MQ这一个步骤必须保证原子。\n一般可靠消息服务可以和MQ一起组合，待确认消息算半消息，其实这种实现就是RocketMQ\n几个可能出现的问题：\n 上游服务给可靠消息服务发送待确认消息的过程中出现问题，此时上游服务可以感知调用异常，就可以直接不执行之后的流程了。 上游服务操作完本地数据库后，通知可靠消息服务确认消息或删除消息时出现问题，此时该消息在可靠消息服务里是待确认状态，在可靠消息服务里定时轮询待确认状态的消息，调用上游服务的接口判断这个消息的状态即可；如果上游响应该消息成功，可靠消息服务将消息投递到MQ即可，否则，将该消息删除。 下游服务一直消费消息一直失败，有两种方案：1. 可以设置消息可被消费的次数，超过这个次数进死信队列，后面人工干预即可；2. 由于该条消息在可靠消息服务里一直是已发送状态，始终未完成，此时可靠消息服务在后台可以有定时任务，将这些消息重新丢到MQ里让下游消费即可，当然下游服务消息是需要保证幂等的。  参考： 2pc、3pc\nTCC\nTCC\n华为的servicecomb\n蚂蚁金服的seata分布式事务架构\n分布式事务最经典的七种解决方案\n一个go的分布式事务框架DTM\nhttp://www.ruanyifeng.com/blog/2018/07/cap.html\nhttps://www.cnblogs.com/jajian/p/10014145.html\n分布式算法 为了避免单点故障，常见的多副本复制方案有两种：\n主从复制，如MySQL、Redis  全同步复制：主节点收到一个写请求后，必须等到全部从节点确认返回后，才能返回给客户端成功，一个节点故障将导致整个系统不可用。保证一致性，但可用性不高。 异步复制：主节点收到一个写请求，立马返回给客户端，异步将请求转发给各个从节点，但主节点如果还未将请求进行转发就故障了，就会导致数据丢失。保证可用性，但一致性不高。 半同步复制：主节点收到一个写请求后，至少有一个从节点收到数据，就可以返回客户端，在一致性和可用性上比较平衡。  去中心化复制，如Gossip 一个n个复制集节点的集群中，任意节点都可以接收写请求，但一个成功的写请求需要w个节点确认，读操作也必须查询至少r个节点。可用性高，但容易造成写冲突。\n除了复制方案，另一种是共识算法如Paxos或Raft，保证集群节点高可用和数据一致性。\n分布式锁 利用数据库唯一约束 在数据库新建一个锁表，然后通过操作该表中的数据来实现，表的字段为客户端ID、加锁次数、资源的key，加锁次数 + 客户端ID可以判断是否可重入。\n当我们要锁住某个方法或资源的时候，我们就在该表中增加一条记录，想要释放锁的时候就删除这条记录。在insert前先query判断是否存在该记录。\n优点：简单，方便理解，且不需要维护额外的第三方中间件(比如Redis,Zk)。\n缺点：虽然容易理解但是实现起来较为繁琐，需要自己考虑锁超时，加事务等等。性能局限于数据库，一般对比缓存来说性能较低。对于高并发的场景并不是很适合。\netcd实现 etcd采用raft算法，一个写请求要经过集群多数节点确认，所以一旦分布式锁申请成功返回给client时，锁数据一定是持久化了集群多数节点上，不会出现Redis那种主备异步复制，当主机宕机，从机和主机数据不一致的情况。\n 事务功能：etcd事务由IF、THEN、ELSE语句组成，可以比较key的修改版本号mod_revision和创建版本号create_revision，因此可以通过创建版本号create_revision检查key是否已存在，当key不存在时，创建版本号为0，此时可以进行put操作。 Lease 功能：可以保证分布式锁的安全性，为锁对应的 key 配置租约，即使锁的持有者因故障而不能主动释放锁，锁也会因租约到期而自动释放，同时也支持续约。此特性解决了client出现crash故障，client与ectd集群网络出现隔离等场景下的死锁问题。一旦Lease TTL，key就会自动释放，确保其他client在TTL过期后能正常申请锁。 watch功能：在实现分布式锁时，如果抢锁失败，可通过 Prefix 机制返回的 KeyValue 列表获得 Revision 比自己小且相差最小的 key（称为 pre-key），对 pre-key 进行监听，因为只有它释放锁，自己才能获得锁，如果 Watch 到 pre-key 的 DELETE 事件，则说明pre-ke已经释放，自己已经持有锁。 prefix功能：例如，一个名为 /mylock 的锁，两个争抢它的客户端进行写操作，实际写入的 key 分别为：key1=”/mylock/UUID1″，key2=”/mylock/UUID2″，其中，UUID 表示全局唯一的 ID，确保两个 key 的唯一性。很显然，写操作都会成功，但返回的 Revision 不一样，通过前缀 /mylock 查询，返回包含两个 key-value 对的的 KeyValue 列表，同时也包含它们的 Revision，通过 Revision 大小，客户端可以判断自己是否获得锁，如果抢锁失败，则等待锁释放（对应的 key 被删除或者租约过期），然后再判断自己是否可以获得锁。并配合上一条的watch功能使用。 reversion功能：如上一条所述，每个 key 带有一个 Revision 号，每进行一次事务加一，因此它是全局唯一的，如初始值为 0，进行一次 put(key, value)，key 的 Revision 变为 1；同样的操作，再进行一次，Revision 变为 2；换成 key1 进行 put(key1, value) 操作，Revision 将变为 3。多线程获取锁时，通过比较reversion的大小即可知道获取的顺序，避免\u0026quot;惊群效应\u0026quot;（指的是当锁被释放，会导致所有监听该key的client都尝试发起事务获取锁，性能较差）。  实现的方案：通过多个client创建prefix相同，名称不同的key，哪个key的revision最小，谁就获得锁。\netcd自带了concurrency包，简化分布式锁、分布式选举、分布式事务的实现。该包的分布式锁实现：当事务发现createRevision的值为0时，会创建一个prefix为/my-lock的key，key的名称为/my-lock+LeaseId，并获取/my-lock prefix下面最早创建的一个key，即revision最小，分布式锁最终由写入该key的client获得，其他client进入等待模式。未获得锁的client通过Watch机制监听prefix相同，revision比自己小的key，只有当revision比自己小的key释放了锁，才有机会获得锁。\n优点：\n 可以通过lease功能和结点健康监测，确保客户端崩溃时，锁一定会被释放。 etcd客户端etcd有相应锁实现，是否满足需求需要进一步查看源码。  缺点：\n 强一致性带来的必定是写效率上的降低  ZooKeeper实现 基于ZooKeeper的临时节点和顺序的特性。\n临时节点具备数据自动删除功能，当client与ZooKeeper连接和session断掉时，相应的临时节点就会被删除。\n另外，ZooKeeper也提供Watch特性监听key的变化。\n 因为创建节点的唯一性，我们可以让多个客户端同时创建一个临时节点，创建成功的就说明获取到了锁 。然后没有获取到锁的客户端也像上面选主的非主节点创建一个 watcher 进行节点状态的监听，如果这个互斥锁被释放了（可能获取锁的客户端宕机了，或者那个客户端主动释放了锁）可以调用回调函数重新获得锁。\n共享锁：规定所有创建节点必须有序，当你是读请求（要获取共享锁）的话，如果 没有比自己更小的节点，或比自己小的节点都是读请求 ，则可以获取到读锁，然后就可以开始读了。若比自己小的节点中有写请求 ，则当前客户端无法获取到读锁，只能等待前面的写请求完成\n排他锁：如果你是写请求（获取独占锁），若 没有比自己更小的节点 ，则表示当前客户端可以直接获取到写锁，对数据进行修改。若发现 有比自己更小的节点，无论是读操作还是写操作，当前客户端都无法获取到写锁 ，等待所有前面的操作完成\n Redis实现 单机Redis实现 使用setnx命令，setnx命令表示如果key不存在，则可以set成功，返回1，否则返回0，根据返回值来判断是否加锁成功，注意setnx不支持设置key、value的同时还要设置过期时间，过期时间主要是保证资源占用时间过长后可以释放锁，避免死锁，所以如果要用来加锁，必须使用Lua脚本来保证原子性\n1 2 3 4 5  if redis.call(\u0026#39;setnx\u0026#39;, KEYS[1], KEYS[2]) == 1 then redis.call(\u0026#39;expire\u0026#39;, KEYS[1], KEYS[3]) end return ok   不过从2.6.12起，set涵盖了setex、setnx功能，并且set本身可以设置过期时间，因此可以使用以下命令进行加锁\n命令：SET [key: 资源代表的key] [value: 客户端事务Id] NX PX [时间，EX的单位是秒，PX的单位是毫秒] ，要注意旧版本的setnx命令不支持设置过期时间\n解锁，先判断锁是不是自己加的，如果是才可以解锁，即删除该key，需要保证这两个步骤的原子性，否则可能会出现因为查询时间过长导致删掉了别人的锁\n1 2 3 4 5  if redis.call(\u0026#34;get\u0026#34;,KEYS[1]) == ARGV[1] then return redis.call(\u0026#34;del\u0026#34;,KEYS[1]) else return 0 end   为了保证Redis的高可用，就会部署Redis集群，但在集群环境下还使用上述方案可能会出现锁偶尔失效。比如Redis设置主从节点，一般加锁解锁会在主节点上进行，但是当加锁后，主节点宕机，从节点还未进行同步，从节点提升为主节点，其他服务就有机会获取到锁了，此时就出现了多个服务对同一资源的操作问题；另外，当发生网络分区现象时，Redis可能出现脑裂，出现多个master，使得多个client可以获得锁。\n集群Redis实现 RedLock与Redission实现，有空再进行补充，先把RedLock的基本思想简单描述一下\n假设有5个Redis节点，\n 加锁时，顺序再5个节点上申请锁，使用同样的key、value、超时时间，申请锁的超时时间，因为需要同时请求多个节点，避免加锁时间花费过长 当在3个节点上成功申请到锁，且申请锁消耗的时间 小于 锁的有效时间，就算申请成功。申请锁消耗的时间采用获得锁的当下时间减去加锁请求时的时间戳得到 锁申请到后，锁的有效时间 = 锁的过期时间 - 申请锁获得的时间 如果申请锁失败了，申请成功锁的节点会执行解锁操作，进行重置  参考：\nRedLock算法的争议\n分布式自增id 数据库自增id 利用MySQL的自增id来实现，服务在需要用到自增id时，会向MySQL发送请求。通过事务的方式，先增长id再进行查询\n优点：简单方便\n缺点：性能不是很好，还有一个是可用性问题，如果数据库不可用了，会导致其他服务不可用，如果使用主从方式部署，虽然可靠性提升了，但如果主库挂掉后，从库数据没有及时同步，会出现ID重复\n解决：使用双主模式，两个主节点的自增序列错开，比如设置节点A的起始值为1，步长为2，节点B的起始值为2，不出为2。但是这种方案又有一个缺点，那就是扩展性不好，假如两个节点不够用了，第三个节点加入时的起始值和步长不好设置\n解决：客户端通过号段的方式来获取自增id，每次从数据库获取时不再是获取一个，而是获取一段范围内的一批id，缓存在本地，减少IO和竞争\n雪花算法 分布式ID固定是一个long类型的数字，占64位，通过一定的规则编排这64位来实现\n------------------------------------------------------------------- | bit | 66 | 65 ~ 24 | 23 ~ 13 | 12 ~ 1 | | length | 1 | 41 | 10 | 12 | | part |none| timestamp | machine | sequence | ------------------------------------------------------------------- 41位的时间戳可以使用69年，41位的时间戳是精确到毫秒级别的 12位的序列号可以让同一个节点一毫秒内生产4096个ID 一般可以自己调整时间戳、机器id、序列号的位数来控制ID的并发量、使用年限等，满足不同的场景，机器ID可以自己配置在节点上、配置中心、数据库\n优点：简单方便，整型易操作，有需要还可以将整型进行进制转化，转成字符串类型的id\n缺点：因为使用到了时间，如果节点上的时钟回拨，会导致ID重复；如果服务部署在docker内，获取时间和机器id要注意；由于机器不同，生成的id是自增的，但是不一定连续；而且还有个问题，前端js是没有long类型的，整型最多支持53位，所以如果返回给前端还要做一次转化。\n利用ETCD etcd 能满足不会丢失的，多副本，强一致的全部需求。两种方案：\n  利用ETCD实现分布式锁，对存储在ETCD中的自增ID加锁实现\n  利用ETCD的boltdb，boltdb是一个单机支持事务的KV存储，key是reversion，value是key-value，bolted会把每个版本都保存下来，实现多版本机制\n用etcdctl通过批量接口写入两条记录： etcdctl txn \u0026lt;\u0026lt;\u0026lt;' put key1 \u0026quot;v1\u0026quot; put key2 \u0026quot;v2\u0026quot; 再通过批量接口更新这两条记录： etcdctl txn \u0026lt;\u0026lt;\u0026lt;' put key1 \u0026quot;v12\u0026quot; put key2 \u0026quot;v22\u0026quot; boltdb中其实有了4条数据： rev={3 0}, key=key1, value=\u0026quot;v1\u0026quot; rev={3 1}, key=key2, value=\u0026quot;v2\u0026quot; rev={4 0}, key=key1, value=\u0026quot;v12\u0026quot; rev={4 1}, key=key2, value=\u0026quot;v22\u0026quot; 此时可以发现reversion由两部分组成，第一部分为main rev，每次事务进行会加一，第二部分sub rev，同一个事务中每次操作加一。另外ETCD提供了命令和选项来控制存储的空间问题   利用ZooKeeper 利用ZooKeeper的顺序一致性和原子性，当客户端每次需要自增id时，创建一个持节顺序节点，ZooKeeper为了保证有序，会给这些节点编号，同时ZooKeeper会保证并发时不会产生冲突，创建成功后会返回类似/root/generateid0000000001的结果，再进行截取即可，另外，为了保证不浪费空间，可以用完该znode后进行删除。\n缺点：ZooKeeper是CP，不保证完全高可用，另外，由于数据需要再ZooKeeper间进行过半数同步完才算写入成功，性能也一般\n利用Redis 使用Redis的incr命令来实现原子性的自增和返回\n优点：简单方便\n缺点：需要对id进行持久化，虽然Redis本身提供了RDB和AOF，但如果宕机了，仍然有可能出现重复ID，如果为Redis搭建集群，由于Redis主从复制时异步复制的，无法保证master宕机之前将最新的id同步给其他子节点，导致宕机恢复之后可能会产生重复的id，需要针对这种情况做特殊处理，比如当id重复时报特殊错误码，跳过这个错误的id区间\n分布一致性算法 Paxos算法 Raft算法 感觉像与ZAB协议类似，具体有时间再整理\n当Leader宕机之后，只有拥有最新日志的Follower才有资格称为Leader。\n关于日志检查，当Follower上的日志与leader不一致时，Leader先找到Follower同它日志一致的index，将其后边的日志一条条覆盖在Follower上。\n参考：Raft算法详解\nRaft动画演示\nGossip协议 简单来讲，就是一个节点会周期、随机的将事件广播给其他节点，其他节点收到后也会周期性的广播给其他节点，最终集群上所有节点都能收到消息，是一种去中心化，最终一致性的算法，压力不在来自主节点，而是所有节点都均衡负载，扩展性很好，即使有新节点加入，最终也会被广播到。\n当会有个拜占庭问题，就是如果有一个恶意传播节点，将会打乱原本的传播\n","date":"2020-08-14T00:00:00Z","permalink":"http://nixum.cc/p/%E5%88%86%E5%B8%83%E5%BC%8F%E7%9B%B8%E5%85%B3/","title":"分布式相关"},{"content":"[TOC]\n数据类型及结构 数据类型 String、List(一般当成队列，尽量少使用随机读写)、Hash、Set、ZSet\nString类型下还有一种扩展类型：Bitmap。原理：String类型会保存二进制字节数组，对于这个字节数组的每个bit来表示一个元素的二值状态。\n此外还有扩展类型：HyperLogLog、Geo\n底层数据结构  String：简单动态字符串(SDS) List：双向链表 + 压缩列表 Hash：哈希表 + 压缩列表 Set：整数数组+ 哈希表 ZSet：跳表 + 压缩列表  简单动态字符串(SDS) Redis的String类型底层有两种保存形式，当保存的是64位有符号整数时，String类型会保存为一个9字节的Long类型整数；当保存的数据包含字符时，String类型就会用简单动态字符串SDS。\n简单动态字符串SDS由三个部分组成：\n buf：是字节数组，保存实际数据，结束标志位是\u0026quot;/0\u0026quot;。 len：表示buf已用长度，占4字节 alloc：表示buf的实际分配长度，一般大于len  此外，对于每种数据类型，Redis会使用RedisObject来记录一些元数据，比如最后以此访问时间，引用次数等，RedisObject包含了8个字节的元数据和一个8字节指针，指针指向具体的数据类型的实际数据所在。\n对于String类型的RedisObject：\n 当保存的是Long类型整数时，RedisObject中的指针直接就是整数数据，不用额外的指针指向整数； 当保存的是字符串时，如果字符串\u0026lt;=44字节，RedisObject中元数据，指针和SDS是一块连续的内存区域，避免内存碎片 当保存的是字符串时，如果字符串\u0026gt;44字节，RedisObject会给SDS分配独立的空间，并用指针指向SDS  ![Redis String RedisObject: 来自极客时间Redis核心技术与实战](https://github.com/Nixum/Java-Note/raw/master/picture/Redis String RedisObject.png)\n当使用String类型时，且value的类型是String时，如果value的长度太小，可能会出现元数据的大小比数据本身的大小还大，造成额外的内存开销。如果能替换成Long类型，实际存储的大小会大大降低。\n哈希表 但无论值是什么类型的，所有的键值对会保存在全局哈希表中，便于快速找到对应的Key，哈希桶只会保存键值对的指针。全局哈希表中的桶每个元素entry由三个8字节指针组成，分别为key、value、next，但实际会占32字节，因为内存分配库jemalloc会分配最接近24的2的幂次数，所以是32，以减少频繁的分配次数。\n因此，即使Redis里存在大量数据，也不影响查找的速度，毕竟都是根据Key进行hash就能找到对应的Value，真正有影响的是哈希表的在解决哈希冲突和rehash时带来的阻塞。\nRedis的哈希表使用拉链法解决哈希冲突。通过两个全局哈希表加快rehash的操作。\n处理全局哈希表有这种操作，Hash的数据结构也是这样的操作，本质是一样的。\n当Redis生产RDB和AOF重写时，哈希表不会进行rehash。\nrehash触发条件 装载因子：哈希表中所有entry的个数除以哈希表的哈希桶个数。\n  当装载因子\u0026gt;= 1，且哈希表被允许rehash，即此时没有进行RDB和AOF重写\n  当装载因子\u0026gt;= 5，因为此时数据量已远远大于哈希桶的个数了，此时会立马进行rehash\n  rehash过程   默认使用哈希表1，此时哈希表2还没有被分配空间\n  当数据增多至需要rehash时，为哈希表2分配空间，大小会比哈希表1大，比如大两倍\n  把哈希表1中的数据重新映射并拷贝到哈希表2中\n渐进式rehash：解决大量数据在哈希表1和2之间拷贝，会导致Redis线程阻塞（因为单线程)。\n3.1 拷贝数据时，Redis仍然正常处理客户端请求，每处理一个请求时，从哈希表1中的第一个索引位置开始，顺便将该索引位置上的所有entries拷贝到哈希表2中；\n3.2 等待处理下一个请求时，再顺带拷贝哈希表1中该索引下一个索引位置的entries到哈希表2中；\n通过这两种方式，将一次性的大量拷贝分散到每次请求和等待间隙中。\n3.3 此外Redis本身也有一个定时任务在执行rehash，发生在空闲时间\n  释放哈希表1的空间，此时哈希表1的空间被回收，原来的哈希表2变成哈希表1，哈希表1变成哈希表2\n  列表\n对于Hash数据类型，底层有压缩列表和哈希表两种实现，当Hash 集合中写入的元素个数超过了 hash-max-ziplist-entries，或者写入的单个元素大小超过了 hash-max-ziplist-value，Redis 就会自动把 Hash 类型的实现结构由压缩列表转为哈希表，且之后不可逆。使用压缩列表时，底层是无法利用index查找，只能遍历查找。\n压缩列表 本质上是一个数组，数组中每一个元素保存一个数据。但压缩列表在\n 表头有三个字段：zlbytes记录占用的内存字节数，可算出列表长度；zltail记录列表尾的偏移量，可算出尾节点到列表起始地址的字节数；zllen记录列表中的entry个数； 每个节点元素entry有四个字段：previous_entry_length记录前一个节点的长度；encoding记录content的数据类型和长度；content保存元素的值；len表示自身长度。 表尾有一字段：zlend用于标记列表末端。  数据类型List、Hash、ZSet都有使用到压缩列表。\n压缩列表的优势在于存储结构，普通数组要求数组的每个元素的大小相同，但是当我们需要在每个元素中存储大小不同的字符串时，就会浪费存储空间，压缩列表就是会把每个元素多余的空间进行压缩，让每个元素紧密相连，再为每个元素增加一个长度，用于计算下一个元素在内存中的位置。\n另外，在内存的地址查找时，在查找第一个和最后一个的时候有优势，可以利用这三个字段查到，是O(1)，但是因为存储紧凑的缘故，查找其他元素只能遍历，是O(n)。\n压缩列表或者数组主要因为其数据结构紧凑，节省空间，避免内存碎片，提升内存利用率，线性顺序存储，对CPU高速缓存支持友好。对于查找的时间复杂度的优势提升不大。但是，由于压缩列表比较紧凑，在新增更新删除操作时可能会引发连锁更新，此时最坏为O(n^2)，但触发概率相对较低，利大于弊。\n跳表 本质是为链表增加索引，建立多层索引，查找时从顶层的索引开始逐步往下层找，最终定位到元素，适用于范围查询的场景。查找的时间复杂度为O(logN)\n时间复杂度 对各种数据类型操作的时间复杂度取决于底层的数据结构，对于Set，虽然名字看起来是集合，但由于底层是哈希表 + 数组，因此在SREM、SADD、SRANDMENBER命令时，时间复杂度都是O(1)\n  数据类型的范围查询，都需要进行遍历操作，一般都是比较耗时的。比如List的LRANGE、ZSet的ZRANGE、Set的SMEMBERS等\n  数据类型的统计查询，比如查看某数据类型的元素个数，时间复杂度是O(1)，因为数据结构本身就有记录了。\n  与Memcached的区别   Redis支持存储多种数据类型：string、list、hash、set、zset；而Memcached只支持string\n  Redis支持持久化：RDB快照和AOF日志；Memcached不支持持久化\n  Redis支持事务，使用MULTI 和 EXEC命令，支持流水线式发送命令 ；Memcahced不支持事务，命令只能一条一条的发，当然这里的事务并不满足传统意义上的ACID\n  Redis-Cluster 支持分布式存储，可以多台Redis服务器存储同样的数据；Memcached是以一致性哈希算法实现分布式存储，即多台Memcached服务器，Memcached根据查找的key计算出该数据在哪台服务器上\n  在 Redis 中，并不是所有数据都一直存储在内存中，可以将一些很久没用的 value 交换到磁盘；\nMemcached 的数据则会一直在内存中，Memcached使用固定空间分配，将内存分为一组大小不同的slab_class，每个slab_class又分为一组大小相同的slab，每个slab又包含一组大小相同的chunk，根据数据大小，放到不同的chunk里，这种管理方式避免内存碎片管理问题，但是会带来内存浪费，即每个chunk内会放小于这个chunk大小的数据，chunk里有些空间没利用到\n  一致性哈希算法，主要解决传统哈希下节点扩容或缩容导致所有key要重新计算迁移的问题\n 构造一个长度为2的32次方的整数环，即0 ~ (2^32)-1的数字空间； 根据节点的唯一名称或者ip作为关键字进行哈希，确定节点在这个Hash环上的位置； 根据数据的key值计算Hash值，在Hash环上顺时针查找距离这个Key的Hash值最近的节点，完成key到节点的Hash映射查找； 当一个节点扩容或缩容时，只会影响到其中一个节点上的key，此时需要将该节点上的key重新计算，迁移到离它最近的一个节点，其他节点上的key则不会受影响，避免大量数据迁移，减少服务器压力。 为了解决负载不均衡问题（比如节点的数量很少，大量的数据就有可能都集中在上面），可以在此基础上增加一个虚拟层，对每个节点的关键字多计算几次哈希，每次计算的结果作为在环中的位置；key算完哈希值后，先在环上找到虚拟节点，再找到物理节点，将数据分散到各个节点，一般一个物理节点对应150个虚拟节点  一致性哈希参考\nRedis的单线程 Redis的单线程，指的是网络IO和键值对读写由一个线程来完成，其他的如持久化、异步删除、集群数据同步都有额外的线程完成。\n原理 Redis单线程之所以能处理得很快，得益于高效的数据结构，且也采用了多路复用机制，在网络IO操作中能并发处理大量客户端请求。监听 + 事件驱动 + 回调 的方式，处理易阻塞的accept和recv事件。\n一文揭秘单线程的Redis为什么这么快?\n总结一下就是：高效的数据结构和数据压缩、纯内存操作、非阻塞IO多路复用，避免频繁的上下文切换。\n影响性能的场景   对big key的操作，比如创建和删除，big key在分配内存和释放内存会比较耗时。big key意味着value很大，当进行全量查询返回、聚合操作、全量删除(因为要释放内存)时可能会阻塞主线程。\n一般可以配合scan命令以及对应数据类型的scan命令来增量获取或删除。\n  一些命令对应的操作时间复杂度高，比如范围查询、keys命令。\n一般使用scan代替keys命令，scan命令时因为使用高位进位法遍历，所以即使在扩容情况下也不会漏key，但在缩容时可能得到重复的key。\n  大量Key集中过期，Redis过期机制也在主线程中执行，大量Key集中过期会导致耗时过长，所以在设置过期时间时要加入随机数。\n  淘汰策略也是在主线程执行的，当内存超过Redis内存上限后，每次写入都需要淘汰Key，也会产生耗时。\n  主从全量同步生产RDB快照，虽然采用fork子进程生成数据快照，但fork瞬间也会阻塞线程，另外，从库在接收RDB快照后，会清空所有键值对，如果量大的话也很耗时，如果RDB文件太大，加载也耗时。\n  AOF日志同步写，因为是需要写磁盘的，比如设置为everysec可能会阻塞主线程。\n  切片集群，向其他实例传输哈希槽信息，bigkey数据的迁移。\n  改进的话，就只有删除操作和AOF日志同步写可以放在异步线程去做，Redis4.0以后才提供删除和AOF同步写的异步命令。\n过期时间和数据淘汰策略 key过期删除原理   定期删除策略：Redis起定时器扫描key（默认100ms），判断key是否过期，过期则删除。虽然可以保证过期的key会被删除，但是每次都要扫描会非常消耗CPU资源，且定时器有间距，有可能出现key过期，但是此时定时器还没起，key仍保存在内存中\n  惰性删除策略：每次获取key的时候才判断key是否过期，过期则删除，但如果key一直未被使用，则会一直留在内存里，浪费空间。\n注意在Redis3.2以前，主库惰性删除后，从库不会触发数据删除，此时还能读到，3.2以后的版本才改正，返回空值，4.0后从库才会定时校验过期key。另外，expire命令在主库设置key的过期时间，到了从库可能有延迟导致过期时间比实际的长，expire命令表示执行完多久会过期，expireat/pexpireat命令表示在某一时间点会过期。\n  所以Redis会将这两种策略整合在一起，定期删除策略不在是每次都扫描全部key，而是随机抽取一部分key进行检查，在配合惰性删除策略，正好可以弥补惰性删除策略的缺点\n  淘汰策略 当内存使用量超出所设置的maxmemory值时，才会执行淘汰策略。默认的淘汰策略是当内存满了之后，新写入操作会报错。maxmemory值一般设置为总数据量的15%-30%。\n其他淘汰策略，分为两种，一种是在所有数据范围内，一种是在设置了过期时间的范围内。\n allkeys-random：在所有key中，随机移除某个key allkeys-Lru：在所有key中，移除最近最少使用的key（优先使用） allkeys-Lfu：在所有key中，移除访问次数最少的某个key volatile-random：在所有有设置过期时间的key中，随机移除某个key volatile-Lru：在所有有设置过期时间的key中，移除最近最少使用的key volatile-ttl：在所有有设置过期时间的key中，有更早过期时间的key优先移除 volatile-Lfu：在所有有设置过期时间的key中，移除访问次数最少的某个key  淘汰策略中的LRU算法 对于LRU算法，如果一些元素被频繁使用，会导致频繁的移动，带来了额外的开销。\nRedis的LRU算法做了简化，其只会在RedisObject中记录每个数据最近以此访问的时间戳，当出现数据淘汰时，第一次会随机选出N个数据，作为一个候选集合，比较N个数据的lru字段，把lru字段最小的元素淘汰。\n再次淘汰时，Redis会挑选那些LRU的值小于候选集合中最小LRU值的数据，进入第一次淘汰时创建的候选集合，可能会把里面最大的换出去或者淘汰里面最小的。准备候选集和淘汰数据是两个解耦操作。\nN的值由maxmemory-samples决定。\n淘汰策略中的LFU算法 Redis的LRU算法有个问题是对于非热点数据，其访问次数很少，直到触发淘汰策略才会被删除，在被删除之前都会一直留存，造成缓存污染。LFU算法就是为了解决这个问题。\nLFU缓存策略是在LRU策略基础上，为每个数据增加一个计数器，统计数据访问次数。当使用LFU淘汰策略筛选淘汰数据时，首先会根据访问次数进行筛选，把访问次数最低的数据淘汰出缓存，当淘汰次数相同时，才会比较时间。\nLFU在LRU的基础上，将RedisObject中的访问时间戳拆成两半，16bit存时间，8bit存计数，但计数不是线性递增，而是采用一种计算规则，让其增长不那么快到达2^8=255次，同时也有衰减机制，减少次数对时间的影响。\n存储与持久化 RDB快照（Redis DataBase） RDB快照由于保存的是数据，恢复起来会比AOF快（AOF保存的是命令），而且AOF是文本文件，RDB是二进制文件，所以RDB快照在网络传输、IO效率都比AOF好。\n原理 将Redis中的数据全量保存的文件中，一般会使用子进程进行刷盘操作，不阻塞主线程，此时主线程仍然能处理命令。（先全量）\n此外，会使用Copy on Write机制（写时复制），解决创建快照的过程中，原有数据被修改对RDB快照的影响。当主线程对原有数据进行修改前，这块数据会被复制一份（复制引用，由bgsave子进程操作），形成副本（此时会消耗两倍内存），由子进程将该副本写入RDB文件中，由于写的是引用，主线程修改后会同步到RDB中。（后增量）\n相关命令 bgsave，会调子进程创建快照写入磁盘，主线程继续处理其他命令请求\nsave，主线程创建快照写入磁盘，会阻塞其他命令请求\nredis.conf配置里，save [时间] [次数] 表示在[时间]内有[次数]写入，就会触发bgsave命令\n另外，在进行主从复制，主redis发生sync命令给从redis时，如果刚刚没有执行完bgsave，也会进行一次bgsave操作。\n潜在风险  风险在于快照的创建频率，如果频繁创建快照，多快照写盘会影响磁盘IO，因此每次都进行全量快照并不可取。 如果频率过低，则会导致宕机时丢失的数据过多。解决方式是RDB + AOF一起使用，在两次快照期间用AOF代替。 当使用copy on write机制时，主线程会为其申请额外的空间，当进行频繁的写操作时，会导致内存很快被耗光。当实例系统开启了Swap机制时，超过内存使用量部分会转移到磁盘，访问磁盘的那部分就会很慢，如果没有开启Swap机制，则会触发OOM，Redis进程可能被kill或宕机 另外，当出现频繁的写操作时，由于生成RDB的子进程需要CPU核运行，主线程、多个线程或后台进程会竞争使用CPU，导致性能降低。  AOF（Append Only File） AOF配置项   no：每次命令只写内存(日志缓冲区)，刷盘记日志的操作由操作系统决定\n  everysec（默认）：写命令记录到文件中，默认是每秒同步一次，所以如果发生故障，最多会丢失一秒的数据，但使用AOF保存的数据文件比RDB快照要大。\n  always：此外AOF还能选择每接收一个写命令就追加写入到AOF文件中，虽然能避免不丢数据，但每个写命令都是在主线程上完成，且后面都跟着一个刷盘操作，对机器的负担较大，影响服务性能。\n  原理 不同与MySQL的WAL机制，AOF是先执行命令将数据写入内存，再写入日志。因为AOF会记录Redis收到的每一条命令，并以文本的形式保存，如果先写日志，并不知道命令是否是正确的，因此先写内存，让系统执行成功后，才会记录到日志中，避免错误命令。\n潜在风险  执行完命令，还没来得及记录日志就宕机，此时会丢失该命令的记录和相应数据。如果使用Redis当缓存，且要保证Redis宕机时不直接读库，利用Redis的AOF机制时就要注意了。 AOF是在命令执行后才记录日志，所以不会阻塞当前的写操作，但由于日志的写操作也是在主线程中的，虽然避免阻塞当前操作，但可能会阻塞下一个命令操作，比如刷盘时磁盘IO过慢。  重写机制 一般用于避免AOF日志文件过大，毕竟如果文件太大会影响磁盘IO、重放会太耗时。\n原理 AOF重写机制在重写时，Redis根据Redis现状创建一个新的AOF文件，读取Redis中所有键值对后进行写入，但在重写时不是原样copy，而是会对命令进行合并，以此减小文件大小。\n重写时，主线程会fork后台的bgrewriteiaof子进程，把主线程的内存拷贝一份给bgrewriteiaof子进程，其中也包括了Redis的最新数据，bgrewriteiaof子进程逐一写入重写日志，避免阻塞主线程。\n因为重写是在子进程中处理的，此时主线程仍然能处理客户端的命令，当接收到客户端的写命令时，会记录到AOF日志中，同时也会写进AOF重写日志的缓冲区，等子进程将拷贝数据写完后，再把缓冲区的数据刷入，完成后即可删除AOF文件，留下AOF重写文件。\n潜在风险  主线程fork创建bgrewriteaof子进程时，内核会把主线程的PCB内容拷贝给子进程，此时会阻塞主线程，当要拷贝的内容特别大时，fork执行的时间就会变长，阻塞主线程的时间也会变长。 bgrewriteaof子进程会和主线程共享内存，当主线程收到新增或修改操作时，主线程会申请新的内存空间用来保存，但是如果是bigkey，主线程就会面临申请空间过大导致耗时。  高可用集群 RDB和AOF保证了数据少丢失，集群部署保证服务少中断，实现高可用。\n主从复制 - 保证数据一致性 一般采用主从读写分离，主服务器进行写操作，然后再同步给从服务；而读操作发生在主从服务器均可。最好还是读写都在主库，从库只保证高可用，原因是主从数据同步是异步的，总有可能出现网络波动等问题导致主从库数据不一致，或者主库设置过期时间，但是到从库上有延迟导致读到过期数据，主库的过期删除无法同步到从库（4.0才解决）。\n如果写操作可以发生在主从服务器上，会导致主从服务器上的数据不一致，取数据时可能会取到旧值，而如果要保持一致，则需要加锁，加锁的话效率旧太差了。\n一般用在从节点初始化加入时使用，先进行全量同步(通过快照)，再进行增量同步(通过命令缓存同步)。\n相关命令 在从库上使用命令：Redis 5.0以前使用slaveof、之后使用replicaof [主库IP] [主库端口]\n过程   主服务器创建RDB快照文件，发送给从服务器，并在发送期间使用缓冲区(replication buffer)记录之后收到的写命令。\n快照文件发送完毕之后，开始向从服务器发送存储在缓冲区中的写命令；整个过程中主服务器不会被阻塞。\n  从服务器丢弃所有旧数据，载入主服务器发来的快照文件，之后从服务器开始接受主服务器发来的写命令；\n  主服务器每执行一次写命令，就向从服务器发送相同的写命令\n  一般只设置一个主节点，当负载上升时，主服务器可能无法很快地更新所有从服务器，或者重新连接和重新同步从服务器将导致系统超载。\n解决办法：通过主从级联模式解决，可以设置主节点为根节点，向下延申，从节点再设置从节点的方式，形成树状的主从链，让从节点帮忙同步给其子节点的方式，降低主节点的压力。\n同样使用命令slaveof或replicaof，只是ip换成从库的ip，这样就形成了主-从-从的模式了\n主从库命令传播时网络中断 Redis2.8之前，如果主从库发生网络中断，重写连接时会进行全量复制，开销巨大。\n2.8之后，会使用增量复制。断连期间，主库会把收到的写命令写入缓冲区(replication buffer和repl_backlog_buffer)。repl_backlog_buffer是一个环形缓冲区，主库会记录自己写到的偏移量(master_repl_offset)，从库会记录自己读到的偏移量(slave_repl_offset)。\n断连时，会记录从库的偏移量，待重新连接后即可根据偏移量进行同步。由于repl_backlog_buffer是环形缓冲区，如果从库同步太慢，因此可能会出现新命令覆盖到未读取的命令，只能通过调整其大小解决，配置repl_backlog_size，否则，从库将进行全量复制。\n主从模式下，从库宕机影响不大，但主库宕机就会影响从库的同步了，此时需要哨兵机制重新选举主库，保证高可用。\nreplication buffer 是主从库在进行全量复制时，主库上用于和从库连接的客户端的 buffer，非共享，每个从库对应一个\nrepl_backlog_buffer 是为了支持从库增量复制，主库上用于持续保存写操作的一块专用 buffer，从库共享，只是每个从库都会有复制进度标志(slave_repl_offset)记录在上面\n哨兵机制 哨兵机制会解决三个问题：1.判断主库是否宕机 2.选择哪个从库为主库 3.如何把新主库的信息通知给从库和客户端。\n使用哨兵模式（sentinel）来监听和管理Redis集群，存储集群配置，作用类似ZooKeeper，哨兵节点是一台特殊的Redis，功能有限，主要用来支持哨兵机制，哨兵节点有三个任务：监控、选主、通知。\n原理 哨兵节点一般设置3个及以上的奇数个，哨兵节点间是平级的，会互相监控。\n 所有哨兵节点都会周期性的给所有主从库发送Ping命令检测Redis的主从节点是否正常运行，当有Redis节点出现问题时，进行通知。 如果发生问题的节点是主节点，会从从节点中选出主节点，代替失效的主节点。 之后会把新主库的连接信息发给其他从库，让他们执行replicaof命令，和主库建立连接，并进行数据复制。同时把新主库的连接信息通知给客户端，让客户端把请求发送到新主库上。  如何监控，如何判断主库不可用 主观下线：每个哨兵节点每隔1s对主从节点发生心跳，当有节点再超过x秒后没有进行回复，此时该节点为主观下线，还需要进一步判断。\n客观下线：当主观下线的节点是主节点时，该哨兵节点会通过sentinel is-master-down-by-addr命令，向其它n-1个哨兵节点询问对主节点的判断，当超过 n / 2 + 1 个哨兵节点认为主节点有问题时，该节点为客观下线。多这一步是为了防止误判。\n客观下线后，会从从节点中选举出主节点，前主节点重新上线后会被设置为从节点\nRedis没有使用什么一致性算法，仅依据Gossip协议在有效时间范围内收到其它Sentinel节点的确认。\n另外，如果不使用哨兵模式，只使用Redis集群，也可以实现高可用，只不过是把监控和选择转移到各个节点中。\n如何选举 哨兵节点会周期性的发送心跳给主从库，在此过程中会对各个节点进行打分。之后按照一定的筛选条件和规则，选出得分最高的从库为新主库。\n 筛选条件：从库的在线状态，之前的网络连接状态。通过设置主从库断连的最大连接超时时间（down-after-milliseconds）、断连次数（n），当超过阈值时则说明从库的网络状况不好。 打分规则：从库的优先级（比如不同从库的配置不一样，优先级也就不一样）、从库的复制进度（根据从库在repl_backlog_buffer中的偏移量，从库间比较）、从库的ID号（ID号小的分高）  如何通知 客户端在访问主从库时，不能写死主库地址，而是从哨兵节点中获取主库地址；当哨兵选出新的主库时，会更新自己存的新主库地址。哨兵节点通过 发布/订阅 机制，让客户端进行订阅和修改。从而也能让客户端了解整个主从切换过程。\n主从切换时，可能产生脑裂 如果主库因为某些原因进入假死状态（但还能处理命令），被哨兵判定为客观下线，哨兵执行主从切换，但此时主库恢复正常，但此期间写操作的数据还未同步到从库，待哨兵完成主从切换后，Redis集群会短暂出现两个主库，导致客户端的写操作会发往不同的主库，或者原主库降级成从库，会清空本地数据重新载入新主库的RDB快照，导致数据不一致或丢失。\n解决方法：在主库上配置min-slaves-to-write表示主库能进行数据同步的最少从库数量；min-slaves-max-lag表示主从数据复制时，从库给主库发送ACK消息的最大延迟(秒)。这样当主库假死时，无法响应哨兵心跳，也不能和从库同步，确认从库的ACK命令，原主库就无法再接收客户端的写操作。\n另外一种脑裂是，主库和客户端，哨兵和从库被分割成两个网络，此时哨兵与从库的网络里，哨兵会重新选出主库，但旧主库仍然能处理客户端写操作，网络恢复后，主库降级，数据丢失。\n根本原因：Redis主从集群没有使用共识算法，每个写操作没有在大多数节点写成功后才认为成功导致的。不像ZooKeeper，客户端的操作都会经过ZooKeeper的主节点，当发生脑裂时，ZooKeeper主节点无法写入大多数节点，写请求直接失败，保证数据一致性。\n哨兵集群的高可用 由于哨兵需要进行客观下线的判断，因此需要多个哨兵组成集群，集群就会涉及到高可用。\n哨兵节点间、与主从库间的相互发现机制 哨兵节点间的相互发现：正常情况下，每个哨兵都是平级的。每个哨兵节点在设置时的命令是sentinel monitor \u0026lt;master-name\u0026gt; \u0026lt;ip\u0026gt; \u0026lt;redis-port\u0026gt; \u0026lt;quorum\u0026gt; ，并不感知其他哨兵的存在。哨兵节点间的相互发现，以来Redis的 发布/订阅 机制。哨兵节点一旦和主库建立连接，就会把自己的连接信息(如IP、端口)发布到主库上，同时它也会订阅，从而发现其他哨兵节点。\n**哨兵节点发现从库：**哨兵节点连接主库后，发送INFO命令，主库就会把从库连接信息列表发给哨兵节点，从而实现哨兵节点对从库的监控。\n哨兵节点Leader选举原理 正常情况下哨兵集群内的每个哨兵节点是平级的，但是当触发客观下线时，需要选出一个哨兵节点Leader来执行主从库切换。重新选举主库只能由一个哨兵节点来做，如果不是，可能会出现主从库集群脑裂。另外，哨兵节点越多，选举速度越慢，风险也会增加。\n哨兵节点Leader选举分为两个阶段\n  各个哨兵节点判断主/客观下线阶段：各个哨兵在判断主库是主观下线后，首先会给自己投Yes票，之后会发送is-master-down-by-addr命令给其他哨兵节点，其他哨兵节点会根据自己的判断情况，回复Yes / No回去。该哨兵节点收集得到的Yes票，当超过设置的quorum值时，标记主库为客观下线。\n  每个哨兵在一次选举节点只有一次投票机会，当有哨兵节点得出客观下线结论后，该哨兵再发起投票，进行Leader选举，当收集到的票数超过一半，则该哨兵节点成为Leader节点，如果没有选举成功，则等待一般是故障转移超时时间failover_timeout的2倍时间后会从新举行选举。\n   Redis哨兵下线主库和Leader选举 \n高可扩展集群 一般一个Redis保存几个G内比较合适，当单个Redis实例要保存的键值对太多时，会影响Redis的主从复制、RDB快照和AOF日志的大小、影响从库重放速度、fork子进程的速度(太慢会导致阻塞主线程)，因此需要进行对单Redis实例扩展，常见的方式是对单个Redis实例进行扩展，一般分为纵向扩展和横向扩展。\n扩展  纵向扩展：升级单Redis实例的配置，如内存容量、磁盘容量、CPU等，但会影响RDB快照和AOF日志大小，网络传输等，一般用在不使用Redis持久化功能的场景。 横向扩展：根据key，对Redis实例进行分片，增加Redis实例个数。  分片 原理   将数据分散到集群的多个机器上，Redis里使用的概念是槽Slot，每个集群的槽数固定为16 * 1024 = 16384个，使用哈希分片算法对Key进行取模，计算方法：HASH_SLOT = CRC16(Key) mod 16384，余数为Key所在的槽。\n之所以会使用槽，是因为要把数据和节点解耦，如果不使用槽，而是使用key与节点的映射表，当key的数量非常庞大时，映射表也会非常大，映射表的修改和迁移的性能不高。\n  集群内每台机器会存放一些槽，在集群初始化的时候会对集群内的机器平均分配这16384个槽，使用查表法进行分配。因此，当需要扩容时，会重新计算槽的位置和迁移key，可以使用官方提供的redis-trib.rb脚本实现。\n  客户端连接分片集群后，即可获得槽与各个Redis分片节点的映射。访问时可以访问集群内的任意节点，先根据key算出在哪个槽，在查询槽和节点间的关系，找到对应的节点进行查询。\n另外，要注意分片后，对于Keys、scan这样的扫描命令的性能就会更加差了。\n  当分片集群有增删时，槽与节点的映射也会随之修改，为了负载均衡，Redis需要把槽重新分布到各个分片上。但是客户端却不感知，当客户端发送命令时，如果节点上该槽已迁往别处，客户端会收到MOVED 新槽编号 新槽所在的host的错误信息，客户端将重新请求，同时修改槽与节点的映射关系。\n  如果客户端请求发生在Redis迁移槽的过程中，则会先收到ASK 新槽编号 新槽所在的host的错误消息，让客户端进行重试，直到Redis完成槽的迁移，重试成功。\n  HashTag，如果key的格式是 user:order:{3214}，由{}括起来的部分为HashTag，CRC算法在计算key在哪个槽时只会计算{}里的值，HashTag主要是让多个类型的key可以映射到同一个槽，方便范围查询。\n但使用HashTag可能会导致数据倾斜，使得请求无法平均分到各个分片。\n  一般的分配规模是几十个以内，不适合构建超大规模的集群，原因是去中心化设计，选举算法使用Gossip，规模越大时，插播速度越慢。如果要构建超大规模的集群，则需要增加一层代理，进行集群间的转发，例如twemproxy或者Codis这类基于代理的集群架构。\n事务 使用 MULTI 和 EXEC 命令将多个写操作包围起来进行发送，Redis收到后会先将命令暂存到一个队列里，当收到EXEC命令时才会一起顺序执行。\nRedis中的原子性  更多的是为了减少客户端与服务端的通信次数。Redis本身不提供回滚机制，如果一次性传输多个命令时，当有一个命令执行失败了，剩下的命令还是会继续往下执行，无法实现事务的原子性。 如果命令本身有错，只有到了EXEC命令时才会保错，整个MULTI期间的命令都不会执行，此时才保证了原子性。 RDB不会在事务执行的时候执行，所以可以保证原子性。 事务执行过程中的命令，AOF日志则会记录，所以如果事务执行过程中宕机了，重启前需要使用redis-check-aof工具检查AOF日志，去除执行到一半的事务命令，才能保证原子性。  Redis中的原子操作  Redis的 incr / decr 命令，本质上是一个读取 -》修改 -》写入的过程 为Redis定义新的数据结构实现原子操作，毕竟Redis本身是单线程的，本身不会有其他线程的影响。 Lua脚本，才能实现复杂逻辑的原子操作，使用时会先通过script load命令把脚本加载到Redis中，得到唯一摘要，再通过命令evalsha + 摘要的方式执行脚本，避免每次执行脚本都要先传输，减少网络IO；另外Lua脚本的时间复杂度不易过长，否则会阻塞主线。  Demo：限制客户端每分钟访问次数不能超过20次 的Lua脚本，该脚本包含了计数、访问次数判断和过期时间设置。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  // 客户端先获取ip对应的访问次数 current = redis.call(\u0026#34;get\u0026#34;, KEYS[1]) //如果超过访问次数超过20次，则报错，这两步不涉及临界值的修改，因此可以不放在脚本中原子执行。 IF current != NULL AND current \u0026gt; 20 THEN ERROR \u0026#34;exceed 20 accesses per second\u0026#34; Lua脚本名称：visit_restrict.script 脚本内容 //如果访问次数不足20次，增加一次访问计数 value = redis.call(\u0026#34;incr\u0026#34;, KEYS[1]) //如果是第一次访问，将键值对的过期时间设置为60s后 IF value == 1 THEN redis.call(\u0026#34;expire\u0026#34;, KEYS[1], 60) END //执行其他操作 DO THINGS END 执行命令： redis-cli --eval visit_restrict.script keys , args   Redis中的隔离性  需要依赖Watch机制，因为Redis流水线命令是在Exec命令执行后开始的，在Exec命令还未执行前，如果要保证隔离性，需要使用Watch监控某个key，当Exec命令执行时，Watch机制就会触发，如果监控的数据被修改了，就放弃此次事务的执行。 如果是在Exec命令执行后的，由于Redis是单线程的，所以并发情况下不会破坏事务隔离性。  Redis中的一致性 在命令执行错误或 Redis 发生故障的情况下，Redis 事务机制对一致性属性是有保证的。\nRedis中的持久性 尽管有RDB和AOF日志的支持，但是仍然有丢数据的可能。比如使用RDB模式，在一个事务执行后，而下一次的 RDB 快照还未执行前，如果发生了实例宕机，此时，事务修改的数据也是不能保证持久化的。\n应用场景 聚合统计 - 统计每日新增用户  使用Set类型 一个Set记录所有登录过的用户ID：[key：user:id，value：Set]，另一个Set记录每日登录过的用户ID[key：user:id :日期，value：Set] 统计每日新增用户，计算每日用户 Set 和累计用户 Set 的差集，使用命令SDIFFSTORE [结果key] [user:id :日期] [user:id] 弊端：Set的差集、并集、交集计算复杂度较高，当数据量较大时计算太慢会阻塞主线程，一般这种计算会在从库上做。  排序统计 - 统计最新评论、排行榜、时间段内在线数等  使用List，当有新元素LPush或RPush插入时，原先所有元素都会后移一位，使用Lrange读取元素时可能会读到旧数据。 使用ZSet，为每个元素设置权重，按权重排序，使用Zrangebyscore则不会读到旧数据。  二值状态统计 - 统计是否签到、存不存在、有没有问题  针对值只有0 或1的场景，使用bitmap，是Redis的扩展数据类型，本身是String类型的一种特殊应用。 bitmap利用的是存储的String value的位。并且bitmap支持Bitop命令对多个bitmap进行按位与、或、异或操作，bitcount统计位中1的个数。  基数统计 - 统计一个集合中不重复的元素  使用Set或Hash，但是可能会比较耗内存 使用HyperLogLog，专门用于基数统计，优势在于所需空间总是固定，使用PFAdd key v1 v2 v3命令做添加，PFCount key统计key的value数量，但是存在误差，误差率约为0.81%  GEO - LBS应用  底层实现是ZSet，权重值是经纬度，但是由于ZSet的权重值只能是浮点类型，因此一般会对经纬度做GeoHash编码。 GeoHash编码的基本原理是：二分区间，区间编码。  二分区间：比如把经度范围[-180,180]会被分成两个子区间：[-180,0) 和[0,180]，如果要编码的经度值落在左区间，则用0表示，右区间用1表示，通过不断的对区间进行分区，经过N次之后，得到一个N位数的01组合。纬度同理。 区间编码：把经纬度编码进行组合，规则：偶数位是经度编码值，奇数位是纬度编码值。   通过GeoHash编码，地图上的每个方格都能用数字进行表示，分区越多越精准。通过范围查询 + 方格周围方格的编码，即可实现搜索附近的人功能。 Redis也提供了Geo的数据类型  保存时间序列数据  时间序列数据的特点是插入速度要快，数据一旦插入就不变、查询模式较多。 可以基于Hash和ZSet实现。Hash实现插入快速和点查询，ZSet实现范围查询，但由于插入一条数据的时候需要同时操作Hash和ZSet，因此要求这个操作是原子的。 ZSet只能提供范围查询，聚合查询只能在让客户端查回去之后自己做，但是大量数据查询传输比较依赖网络资源，可能会导致其他操作响应速度变慢，如果想要Redis实现的话，就得依靠RedisTimeSeries。  消息队列   消息队列一般要解决三个问题：顺序消费、幂等消费、消息可靠性保证\n  使用List\n List的每个元素除了消息的内容外还要保存消息的唯一id，用于解决重复性消费； List类型有提供BRPOP供消费者阻塞读取，避免循环读取带来的消耗； List类型有提供BRPOPLPUSH命令，让消费者读取时会把数据存入另一备份List中，用于避免消费者从List中移除消息读取时宕机，重启后可重新读取消息。 List不支持消费组实现，无法避免生产速度大于消费速度的场景。    使用Stream类型，解决多端消费问题。\n Stream的XADD命令，可以为其生成全局唯一ID。    读取时使用命令XRead，可支持指定ID读取，也支持超时阻塞读取。\n 命令XGroup创建消费组，XREADGROUP指定组内哪个消费者进行消费。    Streams 会自动使用内部队列（也称为 PENDING List）留存消费组里每个消费者读取的消息，直到消费者使用 XACK 命令进行回复。如果消费不成功，它就不会给 Streams 发送 XACK 命令，消息仍然会留存。当消费者重启后，用 XPENDING 命令查看已读取、但尚未确认处理完成的消息。\n Stream是Redis5.0以后才有的专门用来处理消息队列的。    缓存可能引发的问题以及应对方法 使用Redis来构建缓存有两种模式\n 只读缓存：加速读请求。应用读取数据，先向Redis查询，查不到再查库，然后保存到缓存中。应用写数据，先改库，删掉旧缓存数据。这种方式可以很好的保证了一致性。 读写缓存：同时加速读写请求。读写都会在缓存里发生，最新的数据是在Redis中，但需要严重依赖Redis的持久化。这种方式对一致性的保证就差了点，特别是在高并发下。  同步直写：优先保证数据可靠。写请求会同时修改缓存和库，等到都完成了才返回，需要保证原子性。 异步写回：优先保证快速响应。写请求会先在缓存中处理，等到这些增改数据要从缓存中淘汰，才会写回数据库。    缓存雪崩 现象：缓存大面积失效导致请求到达数据库\n应对方案：\n 缓存过期时间设置均匀，不能让一大片缓存在某一时间全部失效，比如设置过期时间时加入随机数，让过期时间在一个范围内波动。 请求时加锁(比如利用redission的rlock)，后续请求只能等到前面的查完数据库，进行缓存后，才能继续，但会造成吞吐量降低，响应时间变长，或者可以使用semaphore设置一定的信号量，不至于只有一个请求去回源数据库。 不设置key的过期时间，另开一个定时任务定期全量更新缓存；或者定时任务定期扫描，将快要过期的key延迟过期时间；设置多级缓存。 灰度发布，对缓存进行预热。 服务降级，当访问的数据是非核心数据，直接返回预定义数据或空值；当访问的数据是核心数据，仍允许查缓存，查库。 缓存标记，比如，给数据设置缓存过期时间为60分钟，标记缓存过期时间为30分钟，当时间超过缓存标记时间后，触发缓存更新，但此时实际缓存还能返回旧数据，直到缓存更新完成，才会返回新缓存。 如果是Redis实例发生宕机，只能在应用层中实现服务熔断或请求限流。  缓存击穿 现象：大量请求访问热点数据时，但热点数据刚好过期，此时请求击穿缓存层，直接到达数据库\n应对方案：\n 不给热点数据的缓存设置过期时间，一直保留。 使用互斥锁，当有大量请求访问一个过期的热点数据时，只有一个请求会到达数据库查询数据并缓存，其他请求等待缓存加载即可。  缓存穿透 现象：查询一个一定不存在的数据，导致请求一直到达数据库，数据也不在数据库中。\n应对方案：\n 使用布隆过滤器，将可能出现查询的值哈希到一个bitMap中，进行拦截，虽然布隆过滤有一定的误报几率，但也能一定程度的减少穿透的影响，常见的方案是配合2一起降低穿透带来的影响。 如果查询结果为空，也加入缓存中（可以直接设置为空，或者使用特殊标识来表示），并设置过期时间。 通过异步更新服务 + 消息队列的方式进行全量缓存的更新。缓存的设置还是照旧，只是当有数据更新时，只是触发消息交给消息队列，再由异步更新服务消费消息，实现缓存更新。 利用数据库的Bin Log，当数据库执行更新操作时，从数据库接收到Bin Log之后根据Bin Log更新Redis缓存，道理跟消息队列类似，只是不用担心消息发生失败问题。 前端预防，对请求进行检测。  缓存无底洞 现象：增加缓存节点，性能不升反降，原因是客户端要维护大量的连接，如果key分布在不同机器，需要查多次\n应对方案：\n 减少网络请求，能批量查尽量批量查 将key进行分类，存到指定节点，查询同类的key时只需要特定的节点去查 并发查询  缓存污染 现象：对于那些访问次数很少的数据，一直留存在缓存中，占用缓存空间。\n应对方案：Redis的淘汰策略，一般会使用LRU、LFU、TTL的淘汰策略。\n主动更新缓存要注意的点   不推荐先更新缓存再更新数据库（也称 Write Behind Caching 模式，更新数据时，只更新缓存，异步 / 同步 更新数据库）原因是数据库操作可能失败，导致缓存与数据库不一致；\n这个模式也有点像Linux的PageCache算法，好处是因为直接操作内存，异步，write back可以合并同一个数据的多次操作，IO性能极高，但因为要保证数据一致性，write back的实现就会很复杂，它需要知道哪些数据被更新了，然后还要持久化到磁盘上，比如操作系统的write back仅会在当这个缓存需要失效、内存不够、进程退出时，才会真正持久化。\n  不推荐先更新数据库再更新缓存（也称 Write Through 模式），原因是两者更新数据的顺序可能不一致，更新到缓存的数据也不一定被访问，即并发写导致脏数据；\n  不推荐先删缓存再更新数据库，访问时再进行加载，原因是并发情况下，删除缓存后来不及更新数据库，但旧值已经被其他线程读到了，更新到缓存了；或者数据库操作失败了，但缓存已经没了，导致其他请求还要再读一次数据库；应对的方案是延迟双删，先删缓存 -》更新数据库 -》sleep -》再删除缓存\n  推荐先更新数据库再删除缓存（也称为Cache Aside 模式），访问时再进行加载，虽然也可能出现3中的情况，比如读缓存时缓存失效，紧接着一个并发写操作，就有可能出现读操作覆盖了写操作后的数据更新，导致数据不一致，但实际上发生的概率不大，带来的影响会相对小一些，因为写操作通常会比读操作慢，再加上要锁表，而读操作必须在写操作前进入数据库操作，而又要晚于写操作更新缓存，条件就很苛刻了。\n如果删除缓存失败了，可以延迟任务进行删除重试，因为删除操作一般是幂等的，所以即使重复删除也没关系，另外，相比Read/Write Through模式（更新数据库后更新缓存操作），不会因为并发读写产生脏数据。还有由于会删除缓存，所以要注意缓存穿透问题。\n   Redis缓存方案操作 - 极客时间Redis核心技术与实战 \n使用规范  String类型的数据不要超过10KB，避免BigKey Key的长度尽量短 key的过期时间加上随机值，避免key的集中过期 集合类型的元素个数不超过一万个 Redis实例容量控制在10GB内 禁用Keys、FlushAll、FlushDB命令，慎用全量操作命令、Monitor命令、复杂度过高的命令（如Sort、Sinter、SinterStore、ZUnionStore、ZInterStore）  参考 极客时间 - Redis核心技术与实战：讲得很不错，推荐。\n还有其他看过的别人博客，但是当时忘记把链接贴下来。\n","date":"2020-08-09T00:00:00Z","permalink":"http://nixum.cc/p/redis/","title":"Redis"},{"content":"[TOC]\nKubernetes 基本  \n容器的本质是进程，Kubernetes相当于操作系统，管理这些进程组。\n CNI：Container Network Interface，容器网络接口规范，如 Flannel、Calico、AWS VPC CNI kubelet：负责创建、管理各个节点上运行时的容器和Pod，这个交互依赖CRI的远程调用接口，通过Socket和容器运行时通信。 kubelet 还通过 gRPC 协议同一个叫作 Device Plugin 的插件进行交互。这个插件，是 Kubernetes 项目用来管理 GPU 等宿主机物理设备的主要组件 kubelet 的另一个重要功能，则是调用网络插件和存储插件为容器配置网络和持久化存储，交互的接口是CNI和CSI CRI：Container Runtime Interface，容器运行时的各项核心操作的接口规范，是一组gRPC接口。包含两类服务，镜像服务和运行时服务。镜像服务提供下载、检查和删除镜像的RPC接口；运行时服务包含用于管理容器生命周期，与容器交互的调用的RPC接口（exec / attach / port-forward等）。dockershim、containerd、cri-o都是遵循CRI的容器运行时，称为高层级运行时。 CSI：Container Storage Interface，容器存储的接口规范，如PV、PVC OCI：Open Container Initiative，容器运行时和镜像操作规范，镜像规范规定高层级运行时会下载一个OCI镜像，并把它解压称OCI运行时文件系统包；运行时规范描述如何从OCI运行时文件系统包运行容器程序，并且定义其配置、运行环境和生命周期。定义新容器的namespaces、cgroups和根文件系统；它的一个参考实现是runC，称为底层级运行时。 CRD：Custom Resource Definition，自定义的资源对象，即yaml文件中的Kind，如Operator就是实现CRD的控制器，之后直接使用Operator创建的CRD声明对象即可使用 Master节点作用：编排、管理、调度用户提交的作业  Scheduler：编排和调度Pod，基本原理是通过监听api-server获取待调度的pod，然后基于一系列筛选和评优，为pod分配最佳的node节点。 APIServer：提供集群对外访问的API接口实现对集群资源的CRUD以及watch，是集群中各个组件数据交互和通信的枢纽，当收到一个创建pod的请求时会进行认证、限速、授权、准入机制等检查后，写入etcd。 Controller Manager：管理控制器的，比如Deployment、Job、CronbJob、RC、StatefulSet、Daemon等，核心思想是监听、比较资源实际状态与期望状态是否一致，否则进行协调。   Device Plugin：管理节点上的硬件设备，比如GPU Kube-Proxy：作为daemonset部署在每个节点上，主要用于为Pod创建代理服务，从API-Server获取所有service信息，创建Endpoints，转发service到Pod间的请求，默认使用iptables模式，但当service数量变多时有性能问题，1.8版本后使用IPVS模式提升性能 coreDNS：低版本的kubernetes使用kube-dns，1.12后默认使用coreDNS，用于实现域名查找功能  调度器Scheduler 主要职责就是为新创建的Pod寻找合适的节点，默认调度器会先调用一组叫Predicate的调度算法检查每个Node，再调用一组叫Priority的调度算法为上一步结果里的每个Node打分，将新创建的Pod调度到得分最高的Node上。\n原理  \n  第一个控制循环叫Informer Path，它会启动一系列Informer，监听etcd中的Pod、Node、Service等与调度相关的API对象的变化，将新创建的Pod添加进调度队列，默认的调度队列是优先级队列；\n此外，还会对调度器缓存进行更新，因为需要尽最大可能将集群信息Cache化，以提高两个调度算法组的执行效率，调度器只有在操作Cache时，才会加锁。\n  第二个控制循环叫Scheduling Path，是负责Pod调度的主循环，它会不断从调度队列里出队一个Pod，调用Predicate算法进行过滤，得到可用的Node，Predicate算法需要的Node信息，都是从Cache里直接拿到；\n然后调用Priorities算法为这些选出来的Node进行打分，得分最高的Node就是此次调度的结果。\n  得到可调度的Node后，调度器就会将Pod对象的nodeName字段的值，修改为Node的名字，实现绑定，此时修改的是Cache里的值，只会才会创建一个goroutine异步向API Server发起更新Pod的请求，完成真正的绑定工作；这个过程称为乐观绑定。\n  Pod在Node上运行起来之前，还会有一个叫Admit的操作，调用一组GeneralPredicates的调度算法验证Pod是否真的能够在该节点上运行，比如资源是否可用，端口是否占用之类的问题。\n  调度策略 调度的本质是过滤，通过筛选所有节点组，选出符合条件的节点。\nPredicates阶段：\n GeneralPredicates算法组，最基础的调度策略，由Admit操作执行  PodFitsResources：检查节点是否有Pod的requests字段所需的资源 PodFitsHost：检查节点的宿主机名称是否和Pod的spec.nodeName一致 PodFitsHostPorts：检查Pod申请的宿主机端口spec.nodePort是否已经被占用 PodMatchNodeSelector：检查Pod的nodeSelector或nodeAffinity指定的节点是否与待考察节点匹配   Volume的检查  NodeDiskConflict：检查多个Pod声明的持久化Volume是否有冲突，比如一个AWS EBS不允许被多个Pod使用 MaxPDVolumeCountPredicate：检查节点上某一类型的持久化Volume是否超过设定值，超过则不允许同类型Volume的Pod调度到上面去 VolumeZonePredicate：检查持久化Volume的可用区(Zone)标签，是否与待考察节点的标签匹配 VolumeBindingPredicate：检查Pod对应的PV的nodeAffinity是否与某个节点的标签匹配   与Node相关的规则  PodToleratesNodeTaints：检查Pod的Toleration字段是否与Node的Taint字段匹配 NodeMeemoryPressurePredicate：检查当前节点的内存是否充足   与Pod相关的规则，与GeneralPredicates类似  PodAffinityPredicate：检查待调度的Pod与Node上已有的Pod的亲和(affinity)和反亲和(anti-affinity)的关系    筛选出可用Node之后，为这些Node进行打分\nPriorities阶段(打分规则)：\n  LeastRequestedPriority：选出空闲资源（CPU和Memory）最多的宿主机。\nscore = (cpu((capacity - sum(requested))10 / capacity) + memory((capacity-sum(requested))10 / capacity)) / 2\n  BalancedResourceAllocation：调度完成后，节点各种资源分配最均衡的节点，避免出现有些节点资源被大量分配，有些节点则很空闲。\nscore = 10 - variance(cpuFraction, memoryFraction, volumeFraction) * 10，Fraction=Pod请求资源 / 节点上可用资源，variance=计算每两种Faction资源差最小的节点\n  优先级与抢占机制 给Pod设置优先级，使得高优先级的Pod可用先调度，即使调度失败也比低优先级的Pod有优先调度的机会。\n需要先定义一个PriorityClass的API对象才能给Pod设置优先级。\n1 2 3 4 5 6 7 8 9 10 11 12 13  apiVersion:scheduling.k8s.io/v1beta1 kind:PriorityClass metadata:name:high-priority value:1000000globalDefault:falsedescription:\u0026#34;This priority class should be used for high priority service pods only.\u0026#34;---Pod内spec:containers:...priorityClassName:high-priority  PriorityClass里的value值越高，优先级越大，优先级是一个32bit的整数，最大不超过 10亿，超过10亿的值是Kubernetes保留给系统Pod使用，保证系统Pod不会被用户Pod抢占；globalDefault为true表示该PriorityClass的值会成为系统默认值，为false表示只有使用了该PriorityClass的Pod才有优先级，没有声明则默认是0。\n抢占过程 当一个高优先级的Pod调度失败时，调度器会试图从当前集群里寻找一个节点，当该节点上的一个或多个低优先级的Pod被删除后，待调度的高优先级的Pod可用被调度到该节点上，但是抢占过程不是立即发生，而只是在待调度的高优先级的Pod上先设置spec.nominatedNodeName=Node名字，等到下一个调度周期再决定是否针对要运行在该节点上，之所以这么做是因为被删除的Pod有默认的30秒优雅退出时间，在这个过程中，可能有新的更加被适合给高优先级调度的节点加入。\n抢占原理 抢占算法的实现基于两个队列，一个是activeQ队列，存放下一个调度周期里需要调度的Pod；另一个是unschedulableQ队列，存放调度失败的Pod，当一个unschedulableQ里的Pod被更新之后，调度器就会把该Pod移动到activeQ里；\n 当调度失败时，抢占算法开始工作，首先会检查调度失败的原因，判断是否可以为调度失败的Pod寻找一个新节点； 之后调度器会把自己缓存的所有节点信息复制一份，通过这份副本模拟抢占过程，找出可以使用的节点和需要被删除的Pod列表，确定抢占是否可以发生； 执行真正的抢占工作，检查要被删除的Pod列表，把这些Pod的nominatedNodeName字段清除，为高优先级的Pod的nominatedNodeName字段设置为节点名称，之后启动一个协程，移除需要删除的Pod。  第2步和第3步都执行了抢占算法Predicates，只有这两遍抢占算法都通过，才算抢占成功，之所以要两次，是因为需要满足InterPodAntiAffinity规则和下一次调度周期不一定会调度在该节点的情况。\nKubelet和CRI Kubelet本质也是一个控制循环SyncLoop，在这个控制循环里又包含了很多小的控制循环，每个控制循环有自己的职责，比如Volume Manager、Image Manager、Node Status Manager、CPU Manager等，而驱动整个主的控制循环的事件有四种：\n Pod的更新事件 Pod生命周期变化 Kubelet本身设置的执行周期 定期的清理事件  每个节点上的Kubelet里的SyncLoop主控制循环通过Watch机制，监听nodeName是自己节点的Pod，在内存里缓存这些Pod的信息和状态，当接收到新事件时，执行相应的操作。在管理Pod内的容器时，不会直接调用docker的API，而是通过CRI的grpc接口来间接执行，这样无论底层容器可以简单的从docker换成其他容器程序。\n每台工作节点上的docker容器会使用dockershim提供CRI接口，来与kubelet交互，如果不是使用docker容器，则需要额外部署CRI shim来处理，通过这些shim转发kubelet的请求来操作容器。\nCRI是对容器操作相关的接口，而不是对于Pod，分为两组类型的API，\n RuntimeService：处理容器相关操作，比如创建和启动容器，删除容器，执行命令 ImageService：处理容器镜像的相关操作，比如拉取镜像、删除镜像   \netcd etcd作为Kubernetes的元数据存储，kube-apiserver是唯一直接跟etcd交互的组件，kube-apiserver对外提供的监听机制底层实现就是etcd的watch。\napi-server在收到请求后，会进行一系列的执行链路。\n 认证：校验发起请求的用户身份是否合法，支持多种方式如x509客户端证书认证、静态token认证、webhook认证 限速：默认读 400/s，写 200/s，1.19版本以前不支持根据请求类型进行分类、按优先级限速，1.19版本以后支持将请求按重要程度分类限速，支持多租户，可有效保障Leader选举之类的高优先级请求得到及时响应，防止一个异常client导致整个集群被限速。 审计：记录用户对资源的详细操作行为 授权：检查用户是否有权限对其访问的资源进行相关操作，支持RBAC、ABAC、webhook，1.12版本后默认授权机制是RBAC 准入控制：提供在访问资源前拦截请求的静态和动态扩展能力，如镜像拉取策略。 与etcd交互  资源存储格式 资源以 prefix + / + 资源类型 + / + namespace + / + 具体资源名称 组成，作为key。基于etcd提供的范围查询能力，支持按具体资源名称查询、namespace查询。默认的prefix是 /registry。\n对于基于label查询，是由api-server通过范围查询遍历etcd获取原始数据，然后再通过label过滤。\n资源创建流程 创建资源时，会经过 BeforeCreate 策略做etcd的初始化工作，Storgae.Create接口调用etcd进行存储，最后在经过AfterCreate和Decorator。\n由于put并不是并发安全的接口，并发时可能导致key覆盖，所以api-server会调用Txn接口将数据写入etcd。\n当资源信息写入etcd后，api-server就会返回给客户端了。资源的真正创建是基于etcd的Watch机制。\ncontroller-manager内配备多种资源控制器，当触发etcd的Watch机制时会通知api-server，api-server在调用对应的控制器进行资源的创建。比如，当我们创建一个deployment资源写入etcd，通过Watch机制，通知给api-server，api-server调用controller-manager里的deployment-controller创建ReplicaSet资源对象，写入etcd，再次触发Watch机制，经过api-server调用ReplicaSet-controller，创建一个待调度的Pod资源，写入etcd，触发Watch机制，经过api-server，scheduler监听到待调度的Pod，就会为其分配节点，通过api-server的Bind接口，将调度后的节点IP绑定到Pod资源上。kubelet通过同样的Watch机制感知到新建的Pod，发起Pod创建流程。\nKubernetes中使用Resource Version实现增量监听逻辑，避免客户端因为网络等异常出现中断后，数据无法同步的问题；同时，客户端可通过它来判断资源是否发生变化。\n在Get请求中ResourceVersion有三种取值：\n  未指定，默认为空串：api-server收到后会向etcd发起共识读/线性读，获取集群最新数据，所以在集群规模较大时，未指定查的性能会比较差。\n  =字符串0：api-server收到后会返回任意资源版本号的数据，优先返回最新版本；一般情况下先从api-server缓存中获取数据返回给客户端，有可能读到过期数据，适用于一致性要求不高的场景。\n  =非0字符串：api-server收到后，会保证Cache中的最新ResouorceVersion大于等于请求中的ResourceVersion，然后从Cache中查询返回。\nCache的原理是基于etcd的Watch机制来更新，=非0字符串且Cache中的ResourceVersion没有大于请求中的ResourceVersion时，会进行最多3秒的等待。\n   若使用的Get接口，那么kube-apiserver会取资源key的ModRevision字段填充Kubernetes资源的ResourceVersion字段（ v1. meta/ ObjectMeta.ResourceVersion）。若你使用的是List接口，kube-apiserver会在查询时，使用etcd当前版本号填充ListMeta.ResourceVersion字段（ v1. meta/ ListMeta.ResourceVersion）。\n 在Watch请求中ResourceVersion的三种取值：\n 未指定，默认为空串：一是为了帮助客户端建立初始状态，将当前已存在的资源通过Add事件返回给客户端；二是会从当前版本号开始监听，后续新增写请求导致数据变化时会及时推给客户端。 =字符串0：帮助客户端建立初始状态，但它会从任意版本号开始监听，接下来的行为和 未指定 时一致。 =非0字符串：从精确的版本号开始监听，只会返回大于等于精确版本号的变更事件。  Pod Pod是最小的API对象。\n由于不同容器间需要共同协作，如war包和tomcat，就需要把它们包装成一个pod，概念类似于进程与进程组，pod并不是真实存在的，只是逻辑划分。\n同一个pod里的所有容器，共享同一个Network Namespace，也可以共享同一个Volume。\n这种把多个容器组合打包在一起管理的模式也称为容器设计模式。\n原理 由于不同容器间可能存在依赖关系（如启动顺序的依赖），因此k8s会起一个中间容器Infra容器，来关联其他容器，infra容器一定是最先起的，其他容器通过Join Network Namespace的方式与Infa容器进行关联。\n一个 Pod 只有一个 IP 地址，由Pod内容器共享，Pod 的生命周期只跟 Infra 容器一致，Infra管理共享资源。\n对于initContainer命令的作用是按配置顺序最先执行，执行完之后才会执行container命令，例如，对war包所在容器使用initContainer命令，将war包复制到挂载的卷下后，再执行tomcat的container命令启动tomcat以此来启动web应用，这种先启动一个辅助容器来完成一些独立于主进程（主容器）之外的工作，称为sidecar，边车\nPod可以理解为一个机器，容器是里面的进程，凡是调度、网络、储存、安全相关、跟namespace相关的属性，都是Pod级别的\nPod在K8s中的生命周期  Pending：Pod的yaml文件已经提交给k8s了，API对象已经被创建保存在etcd中，但是这个Pod里有容器因为某些原因导致不能被顺利创建。 Running：Pod已经调度成功，跟一个具体的节点绑定，内部容器创建成功，并且至少有一个正在运行。 Succeeded：Pod里所有容器都正常运行完毕，并且已经退出，在运行一次性任务时比较常见。 Failed：Pod里至少有一个容器以不正常的状态退出，需要查看Events和日志查看原因。 Unknown：异常状态，Pod的状态不能持续通过kubelet汇报给kube-apiserver，可能是主从节点间通信出现问题。  除此之外，Pod的status字段还能细分一组Conditions，主要是描述造成当前status的具体原因，比如PodScheduled、Ready、Initialized以及Unschedulable。\n在Pod的containers定义中，有个lifecycle字段，用于定义容器的状态发生变化时产生的hook。\nSide Car 在声明容器时，使用initContainers声明，用法同containers，initContainers作为辅助容器，必定比containers先启动，如果声明了多个initContainers，则会按顺序先启动，之后再启动containers。\n因为Pod内所有容器共享同一个Network Namespace的特性i，nitContainers辅助容器常用于与Pod网络相关的配置和管理，比如常见的实现是Istio。\nPod中的Projected Volume(投射数据卷) k8s将预先定义好的数据投射进容器，支持的种类：secret、ConfigMap、Downward API，这三种PV一般存放不经常更新的数据，ServiceAccountToken则是在访问Kubernetes API Server时会使用到。\nSecret 将Pod想要访问的加密数据，存放到etcd中，通过在Pod中的容器里挂载Volume的方式访问这些数据。如果etcd里的这些数据发生了改变，挂载到容器里的数据会在一定的延时后进行更新。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  apiVersion:v1kind:Podmetadata:name:test-projected-volume spec:containers:- name:test-secret-volumeimage:busyboxargs:- sleep- \u0026#34;86400\u0026#34;volumeMounts:- name:mysql-credmountPath:\u0026#34;/projected-volume\u0026#34;readOnly:truevolumes:- name:mysql-credprojected:sources:- secret:name:user- secret:name:pass  这里是挂载了一个类型为projected的volume，存放数据的sources是一个secret对象(有属性name)，name的值表示数据存放的名字，比如通过这个名字取得对于的值（值可以是对应的存放数据的文件路径），\n如：kubectl create secret generic user --from-file=./username.txt 也可直接在yaml文件中定义 secret对象，如下，通过data字段定义，kind是secret，值需要经过base64编码\n1 2 3 4 5 6 7 8  apiVersion:v1kind:Secretmetadata:name:mysecrettype:Opaquedata:user:YWRtaW4=pass:MWYyZDFlMmU2N2Rm  通过这种方式进行定义并运行后，会先将信息保存到ectd中，然后在以文件的形式挂载在容器的Volume目录里，文件名是${name}或者${data.key}。\nconfigMap configMap的作用、用法同secret，只是其内容不需要经过加密\ndownward API 让Pod里的容器能够直接获取到这个Pod API对象本身的信息，只能获取Pod启动之前就能确定的信息，Pod运行之后的信息只能通过sidecar容器获取。\n比如下面这份配置，volume的类型是projected，数据来源是downwardAPI，声明要暴露的信息是当前yaml文件定义的metadata.labels信息，K8S会自动挂载为容器里的/podInfo/labels文件。\nDownward API支持的字段是固定的，使用fieldRef字段可以查看宿主机名称、IP、Pod的标签等信息；resourceFieldRef字段可以查看容器CPU、内存等信息。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  apiVersion:v1kind:Podmetadata:name:test-downwardapi-volumelabels:zone:us-est-coastcluster:test-cluster1rack:rack-22spec:containers:- name:client-container...volumeMounts:- name:podinfomountPath:\u0026#34;/podInfo\u0026#34;volumes:- name:podinfoprojected:sources:- downwardAPI:items:- path:\u0026#34;labels\u0026#34;fieldRef:fieldPath:metadata.labels  serviceAccountToken serviceAccountToken是一种特殊的Secret，一般用于访问Kubernetes API Server时提供token作为验证的凭证。Kubernets提供了一个默认的default Service Account，任何运行在Kubernets中的Pod都可以使用，无需显示声明挂载了它。\n默认挂载路径：/var/run/secrets/kubernetes.io/serviceaccount，里面包含了ca.crt，namespace，token三个文件，用于授权当前Pod访问API Server。\n如果要让Pod拥有不同访问API Server的权限，就需要不同的service account，也就需要不同的token了。\n挂载宿主机上的目录 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  apiVersion:v1kind:Podmetadata:name:two-containersspec:restartPolicy:Nevervolumes:- name:shared-datahostPath:path:/datacontainers:- name:nginx-containerimage:nginxvolumeMounts:- name:shared-datamountPath:/usr/share/nginx/html- name:debian-containerimage:debianvolumeMounts:- name:shared-datamountPath:/pod-datacommand:[\u0026#34;/bin/sh\u0026#34;]args:[\u0026#34;-c\u0026#34;,\u0026#34;echo Hello from the debian container \u0026gt; /pod-data/ index.html\u0026#34;]  声明了两个容器，都挂载了shared-data这个Volume，且该Volume是hostPath，对应宿主机上的/data目录，所以么，nginx-container 可 以 从 它 的/usr/share/ nginx/html 目 录 中， 读取到debian-container生 成 的 index.html文件。\nPod的资源分配 Pod的资源分配由定义的Container决定，比如\n1 2 3 4 5 6 7 8 9 10 11 12  ...spec:containers:- name:appresources:requests:memory:\u0026#34;64Mi\u0026#34;# 单位是bytes，注意1Mi=1024*1024，1M=1000*1000cpu:\u0026#34;250m\u0026#34;# 单位是个数，250m表示250millicpu，使用0.25个CPU的算力，也可以直接写成0.25，默认是1，且是cpu share的limits:memory:\u0026#34;128Mi\u0026#34;cpu:\u0026#34;500m\u0026#34;...  CPU属于可压缩资源，当CPU不足时，Pod只会\u0026quot;饥饿\u0026quot;，不会退出；\n内存数与不可压缩资源，当内存不足时，Pod会因为OOM而被kill掉；\nMatser的kube-scheduler会根据requests的值进行计算，根据limits设置cgroup的限制，不同的requests和limits设置方式，会将Pod划分为不同的QoS类型，用于对Pod进行资源回收和调度。\n  Guaranteed：只设置了limits或者limits和requests的值一致；\n在保证是Guaranteed类型的情况下，requests和limits的CPU设置相等，此时是cpuset设置，容器会绑到某个CPU核上，不会与其他容器共享CPU算力，减少CPU上下文切换的次数，提升性能。\n  Burstable：不满足Guaranteed级别，但至少有一个Container设置了requests；\n  BestEffort：requests和limits都没有设置；\n  kubelet默认的资源回收阈值：\nmemory.available \u0026lt; 100Mi；nodefs.available \u0026lt; 10%；nodefs.inodesFree \u0026lt; 5%；imagefs.available \u0026lt; 15%；\n达到阈值后，会对node设置状态，避免新的Pod被调度到这个node上。\n 当发生资源（Eviction）回收时的策略：\n首当其冲的，自然是BestEffort类别的Pod。\n其次，是属于Burstable类别、并且发生“饥饿”的资源使用量已经超出了requests的Pod。\n最后，才是Guaranteed类别。并且，Kubernetes会保证只有当Guaranteed类别的Pod的资源使用量超过了其limits的限制，或者宿主机本身正处于Memory Pressure状态时，Guaranteed的Pod才可能被选中进行Eviction操 作。\n Pod中的健康检查 对于Web应用，最简单的就是由Web应用提供健康检查的接口，我们在定义的API对象的时候设置定时请求来检查运行在容器中的web应用是否健康\n1 2 3 4 5 6 7 8 9 10  ...livenessProbe:httpGet:path:/healthzport:8080httpHeaders:- name:X-Custom-Headervalue:AwesomeinitialDelaySeconds:3periodSeconds:3  Pod的恢复机制 API对象中spec.restartPolicy字段用来描述Pod的恢复策略，默认是always，即容器不在运行状态则重启，OnFailure是只有容器异常时才自动重启，Never是从来不重启容器\nPod的恢复过程，永远发生在当前节点，即跟着API对象定义的spec.node的对应的节点，如果要发生在其他节点，则需要deployment的帮助。\n当Pod的restartPolicy是always时，Pod就会保持Running状态，无论里面挂掉多少个，因为Pod总会重启这些容器；当restartPolicy是never时，Pod里的所有容器都挂了，才会变成Failed，只有一个容器挂了也是Running。\n不能简单的依赖Pod的status字段，而是要通过livenessProbe或者readnessProbe来的健康检查来判断是否需要恢复。\nPodPreset 预置类型的Pod，是Pod配置文件上追加字段的预置模板，PodPreset里定义的内容，只会在Pod对象被创建之前追加在这个对象本身，不会影响任何Pod控制器的定义。\n一个Pod可以对应多个PodPreset，多个PodPreset间会进行合并，如果有冲突字段，则后面执行的PodPreset不会修改前面的字段。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  apiVersion:settings.k8s.io/v1alpha1kind:PodPresetmetadata:name:allow-databasespec:selector:matchLabels:role:frontend env:- name:DB_PORT value:\u0026#34;6379\u0026#34;volumeMounts:- mountPath:/cache name:cache-volume volumes:- name:cache-volume emptyDir:{}  在先创建完这个API对象后，在创建对应Pod，这个PodPreset会根据selector，选择lables中有role: frontend的Pod，为其添加env、volumeMounts、volumes等声明。\nPod的通信 Kubernetes上的CNI网桥相当于docker0网桥，容器(Pod)与宿主机通信，仍然使用VethPair设备，CNI网桥会接管所有CNI插件负责的容器(Pod)，主要是与Pod中的Infra容器的Network Namespace交互。\nCNI网络方案主要由两部分工作\n 1：实现网络方案，比如创建和配置所需要的虚拟设备、配置路由表、ARP和FDB表； 2：实现对应的CNI插件，比如通过该插件可以配置Infra容器里的网络栈，并把它连接在CNI网桥上。  本质上Kubernetes的网络跟docker是类似的，在集群中，所有的容器都可以直接使用IP地址与其他容器通信，而无需IP映射；宿主机也可直接使用IP与所有容器通信，而无需IP映射；每个容器都拥有自己的IP地址，且在其他容器，宿主机看到的是一样的。\nPod内的容器间通信 Pod内部容器是共享一个网络命名空间的。\n在Pod内部有一个默认的叫Pause的容器，作为独立共享的网络命名空间，其他容器启动时使用 -net=container就可用让当前容器加入的Pause容器，以此拥有同一个网络命名空间，所以Pod中的容器可以通过localhost来互相通信。\n对容器来说，hostname就是Pod的名称。因为Pod中的所有容器共享同一个IP地址和端口空间，所以需要为每个需要接收连接的容器分配不同的端口，也就是说，Pod内的容器应用需要自己协调端口的使用。\n另外，也可以使用PV和PVC来实现通信。\n同一节点下Pod间的通信 通过节点上的网桥和Pod上的 Veth Pair 实现，整体跟同一节点上容器间的通信 很像，只不过 Veth Pair 是挂在Pod的共享网络空间上的。Veth Pair将节点上的网桥和Pod上的共享网络空间进行连接，再通过网桥，连接不同的Pod的共享网络空间，实现不同Pod间网络通信。\n \n不同节点下Pod间的通信 不同的节点下Pod间的通信，通过一套接口规范（CNI, Container Network Interface）来实现，常见的有CNI插件实现有Flannel、Calico以及AWS VPC CNI。\n其中，flannel有VXLAN、host-gw、UDP三种实现。\nflannel UDP模式下的跨主机通信\n下图container-1发送请求给container-2流程:\n container-1发送数据包，源：100.96.1.2，目标：100.96.2.3，经过docker0，发现目标IP不存在，此时会把该数据包交由宿主机处理。 通过宿主机上的路由表，发现flannel0设备可以处理该数据包，宿主机将该数据包发送给flannel0设备。 flannel0设备（TUM）由flanneld进程管理，数据包的处理从内核态(Linux操作系统)转向用户态(flanneld进程)，flanneld进程知道目标IP在哪个节点，就把该数据包发往node2。 node2对该数据包的处理，则跟node1相反，最后container2收到数据包。   \nflanneld通过为各个宿主机建立子网，知道了各个宿主机能处理的IP范围，子网与宿主机的对应关系，都会保存在etcd中，flanneld将原数据包再次封装成一个UDP包，同时带上目标节点的真实IP，发往对应节点。\n在由fannel管理的容器网络里，一个节点上的所有容器，都属于该宿主机被分配的一个子网。flannel会在宿主机上注册一个flannel0设备，保存各个节点的容器子网信息，flanneld进程会处理由flannel0传入的IP包，匹配到对应的子网，从etcd中找到该子网对应的宿主机的IP，封装成一个UDP包，交由flannel0，接着就跟节点间的网络通信一样，发送给目标节点了。因为多了一步flanneld的处理，涉及到了多次用户态与内核态间的数据拷贝，导致性能问题，优化的原则是减少切换次数，所以有了VXLAN模式、host-gw模式。\n UDP模式下，在发送IP包的过程，经过三次用户态与内核态的数据拷贝\n 用户态的容器进程发出IP包经过docker0网桥进入内核态 IP包根据路由表进入flannel0设备，从而回到用户态的flanneld进程 flanneld进行UDP封包后重新进入内核态，将UDP包通过宿主机的eth0发送出去   flannel VXLAN模式下的跨主机通信\n通过VXLAN模式（Virtual Extensible LAN）解决UDP模式下上下文切换频繁带来的性能问题。\n原理是通过在二层网络上再设置一个VTEP设备，该设备是Linux内核中一个模块，可以在内核态完成数据的封装和解封。flannel.1设备（VTEP）既有IP地址又有MAC地址，在数据包发往flannel.1设备时，通过二层数据帧，将原数据包加上目标节点的MVC地址，再加上VTEP标识，封装成一个二层数据帧，然后再封装成宿主机网络里的普通UDP数据包，发送给目标节点，目标节点在内核网络栈中发现了VTEP标识，就知道可以在内核态处理了。\n \n数据包的发送都要经过OSI那几层模型的，经过的每一层都需要进行包装和解封，才能得到原始数据，在这期间二层网络(数据链路层)是在内核态处理，三层网络(网络层)是在用户态处理。\nflannel host-gw模式（三层网络方案）\n二层指的是在知道下一跳的IP对应的MAC地址后，直接在数据链路层通信，如果不知道，就需要在网络层设置路由表，通过路由通信，此时就是三层网络。\n将每个Flannel子网的下一跳设置成该子网对应的宿主机的IP地址，用这台宿主机充当网关，Flannel子网和主机信息都保存在ETCD中，由flanneld进程WATCH这些数据的变化，实时更新路由表，这种方案的性能最好。\n \ncalico的跨主机通信\n类似flannel host-gw模式，calico会在宿主机上创建一个路由表，维护集群内各个物理机、容器的路由规则，通过这张路由表实现跨主机通信。通过边界网关协议BGP，在集群的各个节点中实现路由信息共享。\n因此calico不需要在宿主机上创建任何网桥设备，通过Veth Pair设备 + 路由表的方式，即可完成节点IP寻找和转发。\n但这种方案会遇到路由表的规模问题，且最优情况是集群节点在同一个子网。\n \nPod中的网络隔离 通过NetworkPolicy对象来实现，控制Pod的入站和出站请求，实现了NetworkPolicy的网络插件包括Calico、Weave，不包括Flannel。通过NetworkPolicy Controller监控NetworkPolicy，修改宿主机上的iptable。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  apiVersion:networking.k8s.io/v1 kind:NetworkPolicy metadata:name:test-network-policy namespace:default spec:podSelector:matchLabels:role: db # 标签为role:db的Pod拥有该NetworkPolicy，如果为空，则该namespace下所有Pod都无法通信policyTypes:- Ingress - Egress ingress:- from:# from数组内元素是or关系，元素内可以包含其他Selector，他们是and关系- ipBlock:cidr:172.17.0.0/16# 只允许这个网段的节点访问，且不允许172.17.1.0/24网段的节点访问except:- 172.17.1.0/24- namespaceSelector:matchLabels: # 只允许标签为 project:myproject 的Pod访问project:myproject - podSelector:matchLabels:role:frontend ports:# 只允许访问这些pod的6379端口- protocol:TCP port:6379egress:- to:- ipBlock:cidr:10.0.0.0/24# 只允许这些Pod对外访问这个网段的节点，且端口是5978ports:- protocol:TCP port:5978  PV、PVC、StoageClass   PVC（Persistent Volume Claim）：定义持久化卷的声明，作用类似于接口，开发人员直接使用而不用知道其具体实现，比如定义了数据库的用户名和密码之类的属性。\nPVC的命名方式：\u0026lt;PVC名字\u0026gt;-\u0026lt;StatefulSet名字\u0026gt;-\u0026lt;编号\u0026gt;，StatefulSet创建出来的所有Pod都会使用此PVC，Kubernetes通过Dynamic Provisioning的方式为该PVC匹配对应的PV。\n  PV（Persistent Volume）：持久化卷的具体实现，即定义了持久化数据的相关属性，如数据库类型、用户名密码。\n  StorageClass：创建PV的模板，只有同属于一个StorageClass的PV和PVC，才可以绑定在一起，K8s内默认有一个名字为空串的DefaultStorageClass。\nStorageClass用于自动创建PV，StorageClass的定义比PV更加通用，一个StorageClass可以对应多个PV，这样就无需手动创建多个PV了。\n  PVC与PV的绑定条件：\n PV和PVC的spec字段要匹配，比如存储的大小 PV和PVC的storageClassName字段必须一样   用户提交请求创建pod，Kubernetes发现这个pod声明使用了PVC，那就靠PersistentVolumeController帮它找一个PV配对。\n如果没有现成的PV，就去找对应的StorageClass，帮它新创建一个PV，然后和PVC完成绑定。\n新创建的PV，还只是一个API 对象，需要经过“两阶段处理”变成宿主机上的“持久化 Volume”才真正有用： 第一阶段Attach：由运行在master上的AttachDetachController负责，为这个PV完成 Attach 操作，为宿主机挂载远程磁盘； 第二阶段Mount：运行在每个节点上kubelet组件的内部，把第一步attach的远程磁盘 mount 到宿主机目录。这个控制循环叫VolumeManagerReconciler，运行在独立的Goroutine，不会阻塞kubelet主循环。\n完成这两步，PV对应的“持久化 Volume”就准备好了，Pod可以正常启动，将“持久化 Volume”挂载在容器内指定的路径。当需要卸载时，则先Unmount再进行Dettach。\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35  apiVersion:apps/v1kind:StatefulSetmetadata:name:web spec:serviceName:\u0026#34;nginx\u0026#34;replicas:2selector:matchLabels:app:nginx # 声明Podtemplate:metadata:labels:app:nginx spec:containers:- name:nginx image:nginx:1.9.1 ports:- containerPort:80name:web volumeMounts:- name:www mountPath:/usr/share/nginx/html# 声明挂载的PVCvolumeClaimTemplates:- metadata:name:www spec:accessModes:- ReadWriteOnce resources:requests:storage:10Gi  控制器模型 常见的控制器有Deployment、Job、CronbJob、ReplicaSet、StatefulSet、DaemonSet等\n原理 控制循环：在一个无限循环内不断的轮询集群中的对象，将其状态与期望的状态做对比后，对该对象采取相应的操作。实际状态来自集群本身，如Kubelet汇报的容器状态、节点状态，监控系统的监控数据；期望状态来自用户提交的yaml文件；对象X指的是Pod或是其他受控制器控制的对象。\n1 2 3 4 5 6 7 8 9  for { 实际状态 := 获取集群中对象X的实际状态（ Actual State） 期望状态 := 获取集群中对象X的期望状态（ Desired State） if 实际状态 == 期望状态{ 什么都不做 } else { 执行编排动作，将实际状态调整为期望状态 } }   与事件驱动的区别：事件驱动是被动型，接收到事件就执行相应的操作，事件是一次性，因此操作失败比较难处理，控制循环是不对轮询操作值只状态与期望一致的\nDeployment 最基本的控制器对象，管理Pod的工具，比如管理多个相同Pod的实例，滚动更新\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  apiVersion:apps/v1kind:Deploymentmetadata:name:nginx-deploymentspec:# 通过spec.selector.matchLabels根据Pod的标签选择Podselector:matchLabels:app:nginxreplicas:2# 上面的部分定义了控制内容，判断实际与期望，并进行相应的操作，下面是被控制的对象template:metadata:labels:app:nginxspec:containers:- name:nginximage:nginx:1.7.9ports:- containerPort:80  Deployment想要实现水平扩展/收缩，实际操控的是ReplicaSet对象，而ReplicaSet管理着定义数量的Pod，所以它是一种三层结构，Deployment -\u0026gt; ReplicaSet -\u0026gt; 多个平行的Pod，Deployment是一个两层控制器，Deployment控制的是ReplocaSet的版本，ReplicaSet控制的是Pod的数量。\nReplicaSet表示版本，比如上面那份配置，replicas:2是一个版本，replicas:3是一个版本，这里是因为数量不同产生两个版本，每一个版本对应着一个ReplicaSet，由Deployment管理。\n当我们修改Deployment的replicas字段时，会触发水平扩展/收缩，修改template.Image或者版本号时，就会触发滚动更新。\n1 2 3 4 5 6 7 8 9  # 设置更新策略...spec:...strategy:type:RollingUpdate# 滚动更新策略rollingUpdate:maxSurge:1# 指定Desired数量，maxUnavailable:1# 一次更新中，可以删除的旧的Pod的数量  滚动更新相关命令\n 使用kubectl describe deploy xxx -n yyy或者kubectl rollout status deploy xxx -n yyy即可查看滚动更新的流程； 当新的版本有问题时，使用kubectl rollout undo deploy xxx [--to-revision=n]回滚到上个版本； 使用kubectl rollout history查看每次变更的版本，但最好在执行apply -f deployment.yaml后加上-record参数； 如果不想每次修改都触发滚动更新，可以先使用kubectl rollout pause deploy xx -n yy暂停Ddeployment的行为，修改为yaml后使用kubectl rollout resume deploy xx -n yy恢复，让其只触发一次修改。  Deployment只适合控制无状态的Pod，如果是Pod与Pod之间有依赖关系，或者有状态时，deployment就不能随便杀掉任意的Pod再起新的Pod，比如多个数据库实例，因为数据库数据是存在磁盘，如果杀掉后重建，会出现实例与数据关系丢失，因此就需要StatefulSet。\nStatefulSet StatefulSet可以解决两种情况下的状态：\n  拓扑状态，如果PodA和PodB有启动的先后顺序，当它们被再次创建出来时也会按照这个顺序进行启动，且新创建的Pod和原来的Pod拥有同样的网络标识（比如DNS记录），保证在使用原来的方式通信也可行。\nStatefulSet通过Headless Service，使用这个DNS记录维持Pod的拓扑状态。在声明StatefulSet时，在spec.serviceName里指定Headless Service的名称，因为serviceName的值是固定的，StatefulSet在为Pod起名字的时候又会按顺序编号，为每个Pod生成一条DNS记录，通过DNS记录里的Pod编号来进行顺序启动。\nStatefulSet只会保证DNS记录不变，Pod对应的IP还是会随着重启发生改变的。\n  存储状态，PodA第一次读取到的数据，隔了一段时间后读取到的仍然是同一份，无论其他有没有被重建过。\nStatefulSet通过PVC + PV + 编号的方式，实现 数据存储与Pod的绑定。每个Pod都会根据编号与对应的PVC绑定，当Pod被删除时，并不会删掉对应的PV，因此在起新的Pod的时候，会根据PVC找到原来的PV。\n  StatefulSet直接管理Pod，每个Pod不再认为只是复制集，而是会有hostname、名字、编号等的不同，并生成对应的带有相同编号的DNS记录，对应的带有相同编号的PVC，保证每个Pod都拥有独立的Volume。\nStatefulSet的滚动更新，会按照与Pod编号相反的顺序，逐一更新，如果发生错误，滚动更新会停止；StatefulSet支持按条件更新，通过对spec.updateStrategy.rollingUpdate的partition字段进行配置，也可实现金丝雀部署或灰度发布。\nStatefulSet可用于部署有状态的应用，比如有主从节点MySQL集群，在这个case中，虽然Pod会有相同的template，但是主从Pod里的sidecar执行的动作不一样，而主从Pod可以根据编号来实现，不同类型的Pod存储通过PVC + PV实现。\nDaemonSet DaemonSet 的会在Kubernetes 集群里的每个节点都运行一个 Daemon Pod，每个节点只允许一个，当有新的节点加入集群后，该Pod会在新节点上被创建出来，节点被删除，该Pod也被删除。\n一般的Pod都需要节点准备好了(即node的状态是Ready)才可以调度上去，但是有些Pod需要在节点还没准备好的时候就需要部署上去，比如网络相关的Pod，因此需要使用DaemonSet。\nDaemonSet Controller通过 控制循环，在etcd上获取所有Node列表，判断节点上是否已经运行了标签为xxx的Pod，来保证每个节点上只有一个。可以通过在Pod上声明nodeSelector、nodeAffinity、tolerations字段告诉控制器如何选择node。\n 在node上打上标签，即可通过nodeSelector选择对应的node； nodeAffinity的功能比nodeSelector强大，支持更加灵活的表达式来选择节点； tolerations来容忍Pod在被打上污点标签的节点也可以部署，因为一般有污点的节点是不允许将Pod部署在上面的。  DaemonSet是直接管理Pod的，DaemonSet所管理的Pod的调度过程，都由它自己完成，而不是通过Kube-Scheduler完成， 是因为DaemonSet在创建Pod时，会为其增加spce.nodeName字段，此时以及明确了该Pod要运行在哪个节点，就不需要kube-scheduler来调度了，但也带了问题，无论节点可不可用，DaemonSet都会将该Pod往上面调度。\nDaemonSet的应用一般是网络插件的Agent组件、存储插件的Agent组件、节点监控组件、节点日志收集等。\nJob Job是一种特殊的Pod，即那些计算完成之后就退出的Pod，指状态变为complated\nJob 会使用这种携带了 UID 的 Label，为了避免不同 Job 对象所管理的 Pod 发生重合，Job是直接控制Pod的\nspec: backoffLimit: 5 //默认是6 activeDeadlineSeconds: 100 //单位：秒 parallelism: 2 completions: 4 backoffLimit表示失败后的重试次数，下一次重试的动作分别发生在10s、20s、40s\nactiveDeadlineSeconds表示最长运行的时间，如果超过该限定时间，则会立即结束\nparallelism表示一个 Job 在任意时间最多可以启动多少个 Pod 同时运行\ncompletions表示 Job 至少要完成的 Pod 数目，即 Job 的最小完成数\nJob Controller 在控制循环中进行的调谐（Reconcile）操作，是根据实际在 Running 状态 Pod 的数目、已经成功退出的 Pod 的数目，以及 parallelism、completions 参数的值共同计算出在这个周期里，应该创建或者删除的 Pod 数目，然后调用 Kubernetes API 来执行这个操作，当Job执行完处于complate状态时，并不会退出\nCronJob 如果仍然使用Deployment管理，因为它会对退出的Pod进行滚动更新，所以并不合适，因此需要使用CronJob\n作用类似于Job类似于Pod，CronJob类似于Deployment\nCronJob使用 spec.schedule来控制，使用jobTemplate来定义job模板，spec.concurrencyPolicy来控制并行策略\nspec.concurrencyPolicy=Allow（一个Job没执行完，新的Job就能产生）、Forbid（新Job不会被创建）、Replace（新的Job会替换旧的，没有执行完的Job）\nOperator 本质是一个Deployment，会创建一个CRD，常用于简化StatefulSet的部署，用来管理有状态的Pod，维持拓扑状态和存储状态。需要编写与Kubernetes Matser交互的代码，才能实现自定义CRD的行为。\nService 工作在第四层，传输层，一般转发TCP、UDP流量。通过spec.selector，根据Pod的标签选择对应的Pod。\n每次Pod的重启都会导致IP发生变化，导致IP是不固定的，Service可以为一组相同的Pod套上一个固定的IP地址和端口，让我们能够以TCP/IP负载均衡的方式进行访问。\n虽然Service每次重启IP也会发生变化，但是相比Pod会更加稳定。\n一般是pod指定一个访问端口和label，Service的selector指明绑定的Pod，配置端口映射，Service并不直接连接Pod，而是在selector选中的Pod上产生一个Endpoints资源对象，通过Service的VIP就能访问它代理的Pod了。\n创建Service时，会在Service selector的Pod中的容器注入同一namespace下所有service的ip和端口作为环境变量，该环境变量会随着Pod或Service的ip和端口的改变而改变，可以实现基于环境变量的服务发现，但是只有在同一namespace下的Pod内才可以使用此环境变量进行访问。\nService由kube-proxy组件 + kube-dns组件 + iptables共同实现。kube-proxy会为创建的service创建一条路由规则（由service到pod），并添加到宿主机的iptables中，所以请求经过Service会进行kube-proxy的转发。\n大量的Pod会产生大量的iptables导致性能问题，kube-proxy使用IPVS模式来解决。\n关于Headless Service在集群内部被访问，有两种方式\n  Service的VIP，即clusterIP，访问该IP时，Service会把请求转发到其代理的某个Pod上。\n  Service的DNS方式，比如Service有my-svc.my-namespace.svc.cluster.local这条DNS记录，访问这条DNS记录，根据这条DNS记录，查询出对应的clusterIP，根据clusterIP + iptables转发给对应的pod，实现负载均衡。\n如果这条DNS记录没有对应的Service VIP，即Service的clusterIP是None，则称为Headless Service，此时的DNS记录格式为\u0026lt;pod-name\u0026gt;.\u0026lt;svc-name\u0026gt;.\u0026lt;namespace \u0026gt;.svc.cluster.local，直接映射到被代理的某个Pod的IP，由客户端来决定自己要访问哪个pod，并直接访问。通过headless service访问不会进行负载均衡。\n  Service和Pod都会被Kubernetes分配对应的DNS A记录。\nClusterIp模式下的A记录\n Service：.. svc.cluster.local，对应Cluster IP Pod：.. pod.cluster.local，对应Pod的IP  Headless模型下的A记录\n Service：.. svc.cluster.local，对应的是所有被代理的Pod的IP地址的集合 Pod：.. svc.cluster.local，对应Pod的IP  如何被集群外部访问 Service的本质是通过kube-proxy在宿主机上设置iptables规则、DNS映射，工作在第四层传输层。\n  Service设置type=NodePort，暴露virtual IP，访问这个virtual IP的时候，会将请求转发到对应的Pod，默认下nodePort会使用主机的端口范围：30000 - 32767。\n在这里，请求可能会经过SNAT操作，因为当client通过node2的地址访问一个Service，node2可能会负载均衡将请求转发给node1，node1处理完，经过SNAT，将响应返回给node2，再由node2响应回client，保证了client请求node2后得到的响应也是node2的，此时Pod只知道请求的来源，并不知道真正的发起方；\n如果Service设置了spec.extrnalTrafficPolicy=local，Pod接收到请求，可以知道真正的外部发起方是谁了。\n  Service设置type=LoadBalancer，设置一个外部均衡服务，这个一般由公有云提供，比如aws，阿里云的LB服务。\n  Service设置type=ExternalName，并设置externalName的值，这样就可以通过externalName访问，Service会暴露DNS记录，通过访问这个DNS，解析得到DNS对应的VIP，通过VIP再转发到对应的Pod；此时也不会产生EndPoints、clusterIP。\n  Service设置externalIPs的值，这样也能通过该ip进行访问。\n  可以直接通过port forward的方式，将Pod端口转发到执行命令的那台机器上，通过端口映射提供访问，而不需要Service。\n  检查网络是否有问题，一般先检查Master节点的Service DNS是否正常、kube-dns(coreDns)的运行状态和日志；\n当无法通过ClusterIP访问Service访问时，检查Service是否有Endpoints，检查Kube-proxy是否正确运行；\n最后检查宿主机上的iptables。\nIngress 工作在第七层，应用层，即一般代理Http流量。\n作用与Service类似，主要是用于对多个service的包装，作为service的service，设置一个统一的负载均衡（这样可以避免每个Service都设置一个LB，也可以避免因为节点扩缩容导致service的端口变化需要修改访问配置），设置Ingress rule，进行反向代理，实现的是Http负责均衡。\nIngress是反向代理的规则，Ingress Controller是负责解析Ingress的规则后进行转发。可以理解为Nginx，本质是将请求通过不同的规则进行路由转发。常见的Ingress Class实现有Nginx-Ingress-Controller、AWS-LB-Ingress-Controller，使用Ingress时会在集群中创建对应的controller pod。\nIngress Controller可基于Ingress资源定义的规则将客户端请求流量直接转发到Service对应的后端Pod资源上（比如aws-lb-ingress-controller 的 IP mode），其会绕过Service资源，直接转发到Pod上，省去了kube-proxy实现的端口代理开销。\n和Istio Ingressgateway的区别 Istio Ingressgateway是Kubernate Ingress Controller的一种实现，能够支持更多方式的流量管理。\n    Nginx Ingress Controller Istio Ingressgateway     根据HTTP Header选择路由规则 不支持 支持   Header规则支持正则表达式 不支持 支持   服务之间设置权重拆分流量 不支持 支持   Header和权重规则组合使用 不支持 支持   路由规则检查 不支持 支持   路由规则粒度 Service Service下的不同Pod   支持的协议 HTTP1.1/HTTP2/gRpcTCP/Websockets HTTP1.1/HTTP2/gRpcTCP/Websockets/MongoDB    RBAC  Role和RoleBinding：只针对某一namespace的操作授权，namespace只是逻辑上的隔离。  以下配置就是给用户名为example-user绑定了一个example-role的角色，该角色只能操作namespace为mynamespace下的pod的所有读操作。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  apiVersion:rbac.authorization.k8s.io/v1kind:Role metadata:namespace:mynamespace name:example-role rules:- apiGroups:[\u0026#34;\u0026#34;]resources:[\u0026#34;pods\u0026#34;]verbs:[\u0026#34;get\u0026#34;,\u0026#34;watch\u0026#34;,\u0026#34;list\u0026#34;]---apiVersion:rbac.authorization.k8s.io/v1kind:RoleBindingmetadata:name:example-rolebinding namespace:mynamespace subjects:- kind:User # 这里的User，是Kubernetes内置的用户类型，只是一个授权的逻辑概念name:example-user apiGroup:rbac.authorization.k8s.ioroleRef:kind:Role name:example-role apiGroup:rbac.authorization.k8s.io   ClusterRole和ClusterRoleBinding：针对的是整个集群的操作的授权  以下配置就是给用户名为example-user绑定了一个example-role的角色，该角色能对整个集群下的Pod进行读操作。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  apiVersion:rbac.authorization.k8s.io/v1kind:ClusterRole metadata:name:example-clusterrole rules:- apiGroups:[\u0026#34;\u0026#34;]resources:[\u0026#34;pods\u0026#34;]verbs:[\u0026#34;get\u0026#34;,\u0026#34;watch\u0026#34;,\u0026#34;list\u0026#34;]---apiVersion:rbac.authorization.k8s.io/v1kind:ClusterRoleBindingmetadata:name:example-clusterrolebinding subjects:- kind:User name:example-user apiGroup:rbac.authorization.k8s.io roleRef:kind:ClusterRole name:example-clusterrole apiGroup:rbac.authorization.k8s.io  verbs操作可以有get、list、watch、create、update、patch、delete。\nresources指的是configmaps、pods、services、deployments、nodes。\nresourceNames指定是对应资源的名称。\nsubjects的类型不止有User，也可以是ServiceAccount，创建该service后，k8s会自动为其分配一个Secret对象，该字段保存一个用来与API Server交互的token，token的内容可以是证书或者密码，以Secret对象的方式保存在etcd中。\n在Pod中也可以使用ServiceAccount，通过serviceAccountName字段配置使用，如果Pod没有显式使用ServiceAccount，K8s会使用默认的拥有绝大多数权限的default ServiceAccount。创建完ServiceAccount后，会在K8s中产生一个用户名和用户组，\n用户名：system:serviceaccount:\u0026lt;Namespace名字\u0026gt;:\u0026lt;ServiceAccount名字\u0026gt;\n用户组：system:serviceaccount:\u0026lt;Namespace名字\u0026gt;\n1 2 3 4 5  apiVersion:v1 kind:ServiceAccount metadata:namespace:mynamespace name:example-user  在K8s内，已经内置了很多系统保留的ClusterRole，给Master节点内的组件使用，以system:开头；此外，还有一些权限粒度比较大的ClusterRole，如cluster-admin(拥有最高权限)、admin、edit、view\n声明式API 通过编排对象，在为它们定义服务的这种方法，就称为声明式API，\nPod就是一种API对象，每一个API对象都有一个Metadata字段，表示元数据，通过里面的labels字段（键值对）来找到这个API对象；每一个API对象都有一个Spec字段，来配置这个对象独有的配置，如为Pod添加挂载的Volume。\n命令式配置文件操作：编写一个yaml配置，使用kubectl create -f config.yaml创建controller和Pod，然后修改yaml配置，使用kubectl replace -f config.yaml，更新controller和Pod，kube-apiserver一次只能处理一个命令；或者直接使用命令 kubectl set image \u0026hellip; 或 kubectl edit \u0026hellip; 这些都属于命令式的\n声明式配置文件操作：编写yaml配置和更新yaml配置均使用kubectl apply -f config.yaml，kube-apiserver一次处理多个命令，并且具备merge能力。\n工作原理 k8s根据我们提交的yaml文件创建出一个API对象，一个API对象在etcd里的完整资源路径是由Group（API 组）、Version（API 版本）和 Resource（API 资源类型）三个部分组成的\n \napiVersion: batch/v2alpha1 kind: CronJob ... 以上面的配置为例，CronJob是API对象的资源类型、batch是组、v2alpha1是版本\n核心API对象如Pod、Node是没有Group的，k8s是直接在/api这个层级进行下一步的匹配\n过程步骤\n yaml文件被提交给APIServer，APIServer接收到后完成前置工作，如授权、超时处理、审计 进入MUX和Routes流程，APIServer根据yaml提供的信息，使用上述的匹配过程，找到CronJob的类型定义 APIServer根据这个类型定义，根据yaml里CronbJob的相关字段，创建一个CronJob对象，同时也会创建一个SuperVersion对象，它是API资源类型所有版本的字段全集，用于处理不同版本的yaml转成的CronJob对象 APIServer 会先后进行 Admission() 和 Validation() 操作，进行初始化和校验字段合法性，验证过后保存在Registry的数据结构中 APIServer把验证过的API对象转换成用户最初提交的版本，进行序列化操作，保存在ETCD中  CRD（ Custom Resource Definition），一种API插件机制，允许用户在k8s中添加一个跟Pod、Node类型的，新的API资源，即kind为CustomResourceDefinition，类似于类的概念，这样就可以通过这个类，来创建属于这个类的实例(编写yaml文件)，这个实例就称为CR\n比如有CRD为\napiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: networks.samplecrd.k8s.io spec: group: samplecrd.k8s.io version: v1 names: kind: Network plural: networks scope: Namespaced CR为\napiVersion: samplecrd.k8s.io/v1 kind: Network metadata: name: example-network spec: cidr: \u0026quot;192.168.0.0/16\u0026quot; gateway: \u0026quot;192.168.0.1\u0026quot; 其中的资源类型、组、版本号要一一对应\n上面这些操作只是告诉k8s怎么认识yaml文件，接着就需要编写代码，让k8s能够通过yaml配置生成API对象，以及如何使用这些配置的字段属性了，接着，还需要编写操作该API对象的控制器\n控制器的原理：\n \n控制器通过APIServer获取它所关心的对象，依靠Informer通知器来完成，Informer与API对象一一对应\nInformer是一个自带缓存和索引机制，通过增量里的事件触发 Handler 的客户端库。这个本地缓存在 Kubernetes 中一般被称为 Store，索引一般被称为 Index。\nInformer会使用Index库把增量里的API对象保存到本地缓存，并创建索引，Handler可以是对API对象进行增删改\nInformer 使用了 Reflector 包，它是一个可以通过 ListAndWatch 机制获取并监视 API 对象变化的客户端封装。\nReflector 和 Informer 之间，用到了一个“增量先进先出队列”进行协同。而 Informer 与你要编写的控制循环之间，则使用了一个工作队列来进行协同\n实际应用中，informers、listers、clientset都是通过CRD代码生成，开发者只需要关注控制循环的具体实现就行\n配置相关 Pod级别下的一些配置，即当kind: Pod NodeSelector：将Pod和Node进行绑定的字段\n如： apiVersion: v1 kind: Pod ... spec: nodeSelector: disktype: ssd 表示该Pod只能运行在携带了disktype:ssd标签的节点上，否则它将调度失败\nHostAliases：定义了Pod的hosts文件（比如/etc/hosts）里的内容\nspec: hostAliases: - ip: \u0026quot;10.1.2.3\u0026quot; hostnames: - \u0026quot;foo.remote\u0026quot; - \u0026quot;bar.remote\u0026quot; 表示在/etc/hosts文件的内容是将 ip 10.1.2.3映射为 foo.remote和bar.remote\nshareProcessNamespace: true，Pod里面的容器共享PID namespace，即在同一个Pod里的容器可以相互看到对方\nhostNetwork: true、hostIPC: true、hostPID: true表示共享宿主机的network、IPC、namespace\nImagePullPolicy=alaways(默认)、never、ifNotPresent，每次创建Pod都会${value}拉取镜像\nLifecycle: 容器状态发生变化时触发的一系列钩子，属于container级别\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  apiVersion:v1kind:Podmetadata:name:lifecycle-demospec:containers:- name:lifecycle-demo-containerimage:nginxlifecycle:postStart:exec:command:[\u0026#34;/bin/sh\u0026#34;,\u0026#34;-c\u0026#34;,\u0026#34;echo Hello from the postStart handler \u0026gt; /usr/share/message\u0026#34;]preStop:exec:command:[\u0026#34;/usr/sbin/nginx\u0026#34;,\u0026#34;-s\u0026#34;,\u0026#34;quit\u0026#34;]  在lifecycle里的postStart，指的是容器启动后，要执行的操作，但是它不是串行的，即在docker的ENTRYPOINT命令执行后就开始执行了，此时ENTRYPOINT命令执行的动作可能还没完成；preStart，指的是在容器被杀死之前，要执行的操作，只有这个动作执行完成，才允许容器被杀死\n常用命令 查看所有Pod kubectl get pod -A 查看所有configMap kubectl get configmap -A 查看某个configmap的内容 kubectl describe configmap confing名字 -n 命名空间 将更新的configmap内容更新到etcd中 kubectl apply -f 文件名 删除名字为xxx，namespace为yyy的pod kubectl delete pods xxx -n yyy 查看所有Pod以及ip之类的信息 kubectl get pods --all-namespaces -o wide 进入pod里，并打开sh命令行 kubectl exec -it pod名称 -n [名称空间] sh 进入pod里的指定容器AA kubectl exec -it pod名称 -c 容器名称 sh 查看pod 的event kubectl get event -n [名称空间] 查看容器日志 kubectl logs [pod名称] -n [名称空间] -c[pod内容器名称] -f 查看pod的yaml内容 kubectl get pod [pod名称] -n osaas -o yaml 以yaml格式查看configmap的内容 kubectl get cm [configmap的名称] -n [命名空间] -o yaml 对于处于terminating的pod或namespace的删除方法 1. 将对应的pod或namespace转成json形式，将属性finalizers改为[] kubectl get [pod或namespace] [对应的名称] -o json |jq '.spec = {\u0026quot;finalizers\u0026quot;:[]}' \u0026gt; json文件名.json 2.打开另一个窗口，起一个代理 kubectl proxy --port=8081 3.访问kubelet接口进行删除 curl -k -H \u0026quot;Content-Type: application/json\u0026quot; -X PUT --data-binary @json文件.json 127.0.0.1:8081/api/v1/[pod或namespaces]/[对应的名称]/finalize 或者： 删除状态为Terminating的命名空间 kubectl get ns [命名空间的值] -o json \u0026gt; xxxx.json，修改finialize为空数组[] kubectl replace --raw \u0026quot;/api/v1/namespaces/[命名空间的值]/finalize\u0026quot; -f xxx.json 删除指定命名空间下所有资源例如pod、deployment、svc，包括里面所有副本 delete all -n [命名空间] --all 删除命名空间下的event kubectl delete events -n [命名空间] --all 启动busybox调试网络 kubectl run -it --rm --restart=Never busybox --image=busybox sh 查看node标签 kubectl get nodes --show-labels 给node设置标签 kubectl label nodes [node名称] disktype=ssd 删除节点 1. 先排干上面的pod kubectl drain [node名称] --delete-local-data --force --ignore-daemonsets 2. 删除 kubectl delete node [node名称] 如果误驱逐节点，进行恢复 kubectl uncordon [node名称] 将镜像打成压缩包 docker save -o 压缩包名字 镜像名字:标签 还原成镜像 docker load \u0026lt; 压缩包名字 直接使用命令，不保持容器启动 docker run --rm --name kubectl bitnami/kubectl:latest version 启动时修改entrypoint docker run --rm -it --entrypoint env 镜像:tag /bin/bash Istio upstream：发出请求的流量\ndownstream：接收请求的流量\nxDS：控制平面与数据平面通信的统一API标准，包括 LDS（监听器发现服务）、CDS（集群发现服务）、EDS（节点发现服务）、SDS（密钥发现服务）和 RDS（路由发现服务）\n基本 Service Mesh本质上是分布式的微服务网络控制的代理，以side car的方式实现：通过envoy+iptable对流量进行劫持，通过对流量的控制，实现诸如注册中心、负载均衡器、路由策略、熔断降级、限流、服务追踪等功能，而不需要耦合在具体的业务服务中，而Istio是其中一种实现。\nIstio核心功能：\n 流量控制：路由（如灰度发布、蓝绿部署、AB测试）、流量转移、弹性（如超时重试、熔断）测试（如故障注入、流量镜像，模拟生产环境的流量进行测试）， 安全：认证、授权 可观察：指标、日志、追踪 策略：限流、黑白名单  Istio分为控制平面control plane和数据平面data plane，控制面主要负责资源管理、配置下发、证书管理、控制数据面的行为等，数据面则负责流量出入口。\n控制平面\n在低版本中分为Pilot、Mixer、Citadel；\nPilot负责配置下发，将配置文件转化为Istio可识别的配置项，分发给各个sidecar代理(piloy-agent)；\nCitadel负责安全、授权认证，比如证书的分发和轮换，让sidecar代理两端实现双向TLS认证、访问授权；\nMixer负责从数据平面收集数据指标以及流量策略，是一种插件组件，插件提供了很好的扩展性，独立部署，但每次修改需要重新部署，之后1.1版本将插件模块独立一个adaptor和Gallery，但是Mixer由于需要频繁的与sidecar进行通信，又是部署在应用进程外的，因此性能不高。\nGallery负责对配置信息格式和正确性校验，将配置信息提供pilot使用。\n高版本1.5后中分为将Pilot、Citadel、Gallery整合为istiod，同时istiod里也包含了CA、API-Server，配合ingressgateway、egressgateway\n数据平面：Pod中的每个Envoy容器，即istio-proxy；Envoy会以side car的方式运行在Pod中，利用Pod中的所有容器共享同一个Network Namespace的特性，通过配置Pod里的iptables规则，管理进出Pod的流量。\n \n自动注入实现 依赖Kubernetes中的Dynamic Admission Control的功能，也叫Initializer。\nIstio会将Envoy容器本身的定义，以configMap的方式进行保存，当用户提交自己的Pod时，Kubernetes就会通过类似git merge的方式将两份配置进行合并。这个合并的操作会由envoy-initializer的Pod来实现，该Pod使用 循环控制，不断获取用户新创建的Pod，进行配置合并。\n核心CRD   VirtualService：路由规则，主要是把请求的流量路由到指定的目标地址，解耦请求地址与工作负载。\n  DistinationRule：定义了VirtualService里配置的具体的目标地址形成子集，设置负载均衡模式，默认是随机策略。\n  上面两个主要是管理服务网格内部的流量。\n  Gateway（ingress gateway）：是网格的边界，管理进出网格的流量，比如为进出的流量增加负载均衡的能力，增加超时重试的能力，有ingress gateway和egress gateway分别管理。\n与k8s Ingress的区别：\n k8s Ingress只支持7层协议，比如http/https，不支持tcp、udp这些，没有VirtualService，直接对的Service Gateway支持 4 - 6 层协议，只设置入口点，配合VirtualService解耦路由规则的绑定，实现路由规则复用。    ServiceEntry：面向服务，将外部的服务注册到服务网格中，为其转发请求，添加超时重试等策略，扩展网格，比如连接不同的集群，使用同一个istio管理。\n  Sidecar使用Envoy，代理服务的端口和协议。\n应用场景   VirtualService和DestinationRule：按服务版本路由、按比例切分流量、根据匹配规则进行路由(比如请求头必须包含xx)、路由策略(如负载均衡，连接池)\n蓝绿部署：同时准备两套环境，控制流量流向不同环境或版本\n灰度发布(金丝雀发布)：小范围测试和发布，即按比例切分流量\nA/B测试：类似灰度发布，只是侧重点不同，灰度发布最终流量会流向最新版本，而A/B测试只是用于测试A、B两个环境带来的影响。\n这两个可以用于ingressgateway或egressgateway\n  Gateway：暴露集群内的服务给外界访问、将集群内部的服务以https的方式暴露、作为统一应用入口、API聚合\n  ServiceEntry：添加外部的服务到网格，从而管理外部服务的请求，扩大网格，默认情况下，Istio允许网格内的服务访问外部服务，当全部禁止后，需要使用ServiceEntry注册外部服务，以供网格内部的服务使用，一般配合engressgateway，控制网格内部向外部服务发出的请求。\n  超时和重试：通过virtualService的route配置timemout设置服务接收请求处理的超时时间，retries.attempts和retries.perTryTimeout设置重试次数和重试时间间隔，retries.retryOn设置重试条件\n  熔断：通过DestinationRule的trafficPolicy里connectionPool和outlierDection的配置实现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  apiVersion:networking.istio.io/v1alpha3kind:DestinationRulemetadata:name:httpninspec:host:httpbintrafficPolicy:connectionPool:tcp:maxConnections:1# tcp最大连接数http:http1MaxPendingRequests:1# 每个连接能处理的请求数maxRequestsPerConnection:1# 最大被阻挡的请求数outlierDetection:consecutiveErrors:1# 允许出错的次数interval:1s# 失败次数计数时间baseEjectionTime:3m# 最小驱逐时间，经过此时间后将pod重新加入，默认30s，乘于触发次数后作为熔断持续时间maxEjectionPercent:100# 熔断触发时驱逐pod的比例    故障注入：通过VirtualService的fault配置实现\n  流量镜像：通过VirtualService的mirror和mirrorPercentage配置，比如将发送给v1版本的真实流量镜像一份给v2\n  限流：1.5之前有Mixer提供，但是1.5之后移除了Mixer，只能使用Envoy + filter实现，不属于istio生态的了\n  授权认证，Istio的认证更多的是服务间的访问认证，可根据namespace、具体的服务、服务的接口、请求方法、请求头、请求来源等进行设置\n对外提供HTTPS mTLS访问方式，设置域名证书和Gateway即可；设置网格内部的mTLS双向认证；设置JWT认证，使用RequestAuthentication资源进行认证配置，使用AuthorizationPolicy 资源进行授权配置；\n  调试 使用istioctl的dashboard工具、Envoy的admin接口、Pilot的debug接口等，查看网格的信息，比如资源使用率、日志级别、Envoy性能相关信息等\n  istioctl x describe pod [pod名称]，查看pod是否在网格内，验证其VirtualService、DestinationRule、路由等\n  istioctl analyze [-n 命名空间名称] 或 [具体的yaml文件] 或 --use-kube=false [yaml文件]；只分析文件进行网格配置的诊断\n  istioctl d [istiod的pod名称] -n istio-system使用controlZ可视化自检工具，调整日志输出级别、内存使用情况、环境变量、内存信息\n  istioctl d envoy [pod名称].[命名空间] 使用Envoy的admin接口，进行Envoy的日志级别调整、性能数据分析、配置、指标信息的查看\n  kubectl port-forward service/istio-pilot -n istio-system 端口:端口使用pilot的debug接口，查看xDS和配置信息、性能问题分析、配置同步情况\n  istioctl dashboard [controlZ/envoy/Grafana/jaeger/kiali/Prometheus/zipkin]使用istio提供的工具\n  istioctl ps(proxy-status的缩写) [pod名称]进行配置同步检查。\n  istioctl pc(proxy-config的缩写) [cluster/route...] [pod名称].[命名空间]查看配置详情。\n  参考 极客时间-深入剖析k8s-张磊\n极客时间-ServiceMesh实战-马若飞\n","date":"2020-07-22T00:00:00Z","permalink":"http://nixum.cc/p/kubernetes%E5%92%8Cistio/","title":"Kubernetes和Istio"},{"content":"[TOC]\n基础架构 MySQL逻辑架构图  \n 连接器：负责跟客户端建立连接、获取权限、维持和管理连接。登录进去后修改权限，默认是将在下一次登录后生效 查询缓存：MySQL接收到查询请求后会先查询缓存，key是查询语句，value是查询结果，之后经过执行器的权限判断再返回，如果查不到则往后走。不建议使用，因为若有更新操作，会删除对应表的缓存，可能导致缓存命中低，可以设置query_cache_type=demand，默认不使用缓存，需要在查询时显示指定。MySQL8.0删除此功能 分析器：对SQL语句进行分析，词法分析判断各个字符串代表的含义（包括列是否存在），语法分析判断SQL的语法是否正确，这一层操作之后，MySQL就知道你要做什么了 优化器：决定是否要使用索引，使用哪个索引，决定表的连接顺序 执行器：先判断是否有对该表的操作权限，之后判断要使用哪个引擎提供的接口 引擎：对数据进行具体操作的执行者，事务和索引都是在这层做的，但具体需要引擎支持，例如MyISAM不支持事务，InnoDB支持  日志系统   redo log重做日志：InnoDB独有，物理日志，记录这个页做了什么改动，使用二阶段提交保证两份日志逻辑一致。记录写入到redo log后状态是prepare，binlog写入磁盘，事务提交，redo log改为commit状态，在写的时候是先写进redo log buffer，commit后才写进redo log(磁盘)\n当有记录要更新的时候，InnoDB会先把记录(包含数据变更和change buffer的变更)写到redo log里，并更新内存，再在恰当的时候更到磁盘里，redo log prepare、commit 的XID对应bin log的XID实现关联。\nInnoDB的redo log是固定大小的，比如有一组4个文件组成的“环形队列”，首位指针表示当前记录的位置和当前擦除位置，擦除前会把记录更新到磁盘，这种能力也称为crash-safe\n建议设置innodb_flush_log_at_trx_commit=1，表示每次事务的redo log会持久化到磁盘\n  bin log归档日志：属于server层的日志，逻辑日志，记录所有逻辑操作，追加写入，不会覆盖以前的日志，bin log有两种模式，statement 格式的话是记sql语句， row格式会记录行的内容，一般使用row，记录行变化前和变化后的数据，缺点是日志变大。从库是使用bin log进行备份的\n建议设置sync_binlog=1，表示每次事务的bin log都会持久化到磁盘\n  可以只使用redo log来实现崩溃恢复，但无法只使用bin log，原因是 InnoDB使用WAL机制（执行事务时，将操作记录写入内存和日志，事务就完成了，此时数据可能还没写入磁盘，MySQL会在合适的时机将内存里的数据刷入磁盘），如果此时数据库崩溃，要依赖日志来恢复数据页，但是bin log并没有记录数据页的更新细节，而redo log因为环形写入的问题，无法对所有记录进行归档，仅仅只能实现崩溃恢复\n备份时间的长短会影响日志文件的大小，文件的完整性，从而影响到恢复速度和恢复效果\n undo log回滚日志：InnoDB独有，逻辑日志，主要用于事务失败时的回滚，以及MVCC中版本数据查看。当事务被提交后，并不会马上被删除，而是放到待清理链中，=到没有事务用到该版本信息时才可以清理。  参考：MySQL中的日志机制\n常用SQL Count(*)、Count(1)、Count([列])区别 在count(*)不带条件在MyISAM里查询比较快，因为MyISAM会存储总条数，不带条件查询的时候直接用就行，而InnoDB带了事务，支持MVCC，因此每次count(*)时都会扫表\n以下归纳基于InnoDB，count会返回满足条件的结果集的总行数，它会使用存储引擎进行全表扫描获取结果，比如count(1)会直接返回1，count(主键)会获取主键，返回给server层，由server层进行计数，因此按效率排序是：count(字段) \u0026lt; count( 主键id) \u0026lt; count(1) ≈ count(*)\n count（列）会计算列或这列的组合不为空的计数 count(*) 跟 count(1) 的结果一样，都包括对NULL的统计，而count([列名]) 是不包括NULL的统计 对于计数，也可以通过创建列为表名、total的表进行计数，利用事务能力，一般是先insert在update，理由是并发进行total值的更新时，是会上行锁的，如果先update total值可能会导致事务处理时间过长  having的使用   having一般需要搭配 group by 使用，在group by之后，order by之前\n  having一般配合聚合函数使用，而where后面不能加聚合函数\n  where是对表的字段进行条件过滤，having是对select出来的字段进行条件过滤\n可以想成 查询一些字段，先通过where进行一次过滤，group by进行一次分组，having对分组后的结果再过滤一次，having后的字段必须出现在select中\n  常见一点的sql，比如有如下表，这里为了方便理解以中文的形式表示字段\n1 2 3 4 5 6 7 8 9 10 11 12 13  +-----+--------Log------+------------+ |id|网站名称|点击数|date|+-----+---------+-------+------------+ |1|A|10|2016-05-10||2|C|60|2016-05-13||3|A|230|2016-05-14||4|B|45|2016-05-14||5|E|545|2016-05-14||6|D|13|2016-05-15||7|C|105|2016-05-15||8|E|660|2016-05-16||9|C|301|2016-05-17|+-----+---------+-------+------------+   查询 除了D网站外 各个网站的点击数 大于100 的 网站名称 和 点击数 并 降序 表示\nselect 网站名称, SUM(点击数)\nfrom Log where 网站名称!=\u0026lsquo;D\u0026rsquo;\ngroup by 网站名称 having SUM(点击数)\u0026gt;100 order by SUM(点击数)\norder by  全字段排序：查询条件是索引，但是order by 条件不是，会先遍历索引，再回表取值，每次取到数据就丢sort_buffer，完了之后在sort_buffer里根据order by条件排序 （利用sort_buffer + 临时表），会根据数据量采用内存排序或者外部归并排序， rowId排序：如果select的字段太多，超过设置的最大长度max_length_for_sort_data，就会只取主键和order by的条件丢进去sort_buffer里进行排序，最后再回表根据主键取出其他select的字段 如果order by的条件正好是索引顺序，就不需要使用sort_buffer进行排序了，直接使用索引顺序即可 order by rand()，随机排序，使用内存临时表，使用rowId + 随机数进行排序 不带查询条件进行order by，就算order by条件是索引，是不一定会走索引进行排序，原因是如果MySQL优化器判断走索引后要去回表数量太大，就不会走 带limit的order by，mysql会采用堆排 默认的临时内存表是16M，由tmp_table_size设置  group by  group by一般是使用在select + 聚合函数的情况，如果select 没有聚合函数，语义（即按照group by后的字段进行分组，相同的分组只返回一行）与distinct是类似的，性能也一样 SQL语句的执行顺序是 from \u0026gt; where \u0026gt; group by ＞ having \u0026gt; order by \u0026gt; limit，join 和on和and的组合属于from范围 group by实际上会进行排序操作的，先根据group by后的字段进行排序，再聚合，最后select出需要的字段返回，这个过程会用到内存临时表(可能会退化为磁盘临时表) 无法这样使用select * from table group by column  distinct  distinct 接多个列，会对多个列的不同组合都列出来 无法这样使用select a, distinct b from table  join   由于有时优化器会选择错误的驱动表，使用 straight_join 则可以让MySQL默认使用左边的表作为驱动表\n  当有A、B两表，是一个1对1的关系，逻辑外键在B表带有索引，使用join进行关联查询，小表驱动大表(这样扫描的行数较少)，每扫一行，通过外键索引，在另一个表找对应的行数据，总共执行1条语句，扫描len(A) + len(B) 行，但是如果不使用join查，而是先查出A表所有数据，再根据A的id查回B的数据，虽然扫描的行数一样，但是却执行了len(B) + 1条SQL语句，显然是使用了join的方式性能强，前提是B上带了索引（NLJ算法）。\n在MySQL 5.6之后，使用MRR进行优化，需要设置set optimizer_switch =' mrr = on, mrr_cost_based = off, batched_key_access = on';\n  启用MRR优化：在没使用MRR优化前，通过索引回表是一行一行去查的，每次通过普通索引查到后的主键id是无序的，多次回表查性能不太好，而MRR优化指的是，如果主键索引有序，记录递增插入，则会先根据索引找到所有的id，再对id进行排序，再回表查。\n  如果B上关联键没有使用索引，则算法是这样的，先对A表进行全表扫描存进内存，再对B表进行全表扫描存进内存，然后在内存(join buffer)里进行匹配，如果内存太小，则分块加载，匹配后将结果集返回，清空内存，再分配加载这样循环处理（BNL算法）\n对BNL的优化，一种是在业务端查回两张表的数据，在通过hash匹配组合，另一种是查join查询前，先创建临时表，创建索引，查询被驱动表的数据，插入临时表中，与临时表进行join操作，将BNL转为NLJ\n小表驱动，小表指的是行数相对少，或者select时表的数据量相对少的表\n  驱动表走全表扫描，被驱动表最好走索引扫描，使用NLJ算法，如果被驱动表是全表扫描，则使用(S)BNL算法\n  left join，使用left join时，左边的表不一定是驱动表，如果要使用left join语义，不能把被驱动表的字段放在where条件里做等值和不等值判断，必须放在on里，原因是MySQL会先用on作为条件进行过滤，完了才使用where进行过滤，放在on里能让过滤出来的条数少，要注意两者表达的语义还是有些不同的\n  drop、delete与truncate  delete和truncate只删除表数据不删除表结构 速度上 drop \u0026gt; truncate \u0026gt; delete drop和truncate是ddl语句，操作是立即生效，原数据不放到rollback segment中，不能回滚，而delete是dml语句，该操作会放在rollback segment中，事务提交后才生效 不需要表时使用drop，删除某些行时使用delete，保留表但清空表的数据时使用truncate  Limit  limt M offset N，从第N条记录开始，返回M条记录，比如limit 5, 10，表示返回6-15行 当limit后面只跟一个参数时，表示返回最大的记录行数目，比如limit 5，表示只返回前5行 初始偏移量是0  日期类查询   curdate()函数：得到今天的日期，格式： 年-月-日\n  now()函数：得到今天的日期和时间，格式：年-月-日 时:分:秒\n  两个datetime类型的字段相减，得到的单位跟日期的格式有关，如果格式有到秒，那减出来就是多少秒，如果格式只到日，那减出来就是多少日\n  UNIX_TIMESTAMP(datetime类型的字段) 将datetime类型的字段转换为时间戳，要注意时间戳是以1970 年 1 月 1 日开始算的\n  DATE_SUB(date, INTERVAL expr type) 函数：从日期减去指定的时间间隔\ndate_format(date字段, ‘%Y%m%d %H:%i:%s') 函数：日期格式化函数，\n可以利用这些来查询最近多少天的数据如\n查询 近一小时的数据 where date字段 \u0026gt;= DATE_SUB(now(), INTERVAL 1 Hour) and date字段 \u0026lt; now()\n查询 昨天的数据 where date字段 \u0026gt;= DATE_SUB(CURDATE(), INTERVAL 1 Day) and date字段 \u0026lt; CURDATE()\n查询 近7天的数据 where date字段 \u0026gt;= DATE_SUB(CURDATE(), INTERVAL 7 Day)\n查询 本月的数据 where date_format(date字段, ‘%Y%m') = date_format(curdate() , ‘%Y%m')\n查询 上个月的数据 where period_diff(date_format(now() , ‘%Y%m') , date_format(date字段, ‘%Y%m')) =1\n查询 今年的数据 where YEAR(date字段)=YEAR(NOW())\n查询 去年的数据 where YEAR(date字段)=year(date_sub(now(),interval 1 year))\n查询本季和上一季的跟 查年的差不多 ，把 YEAR函数 换成 QUARTER函数 即可\n  数据类型 varchar和char char是固定长度，varchar是可变长度，varchar(50)和varchar(200)存储字符串 “hello” 所占空间一样，但后者在排序时会消耗更多内存，因为order by采用fixed_length计算字段长度\nint和int(20) 有符号的整型范围是-2147483648~2147483647；无符号的整型范围是0~4294967295；\nint(20)表示能显示的宽度是20，比如id的值是10，那MySQL就会在前面加0，自动补全到20位，仍然占4个字节存储，存储范围也不变。\nDatetime  保存从 1001 年到 9999 年的日期和时间，精度为秒，使用 8 字节的存储空间 与时区无关  TimeStamp   和 UNIX 时间戳相同，保存从 1970 年 1 月 1 日午夜（格林威治时间）以来的秒数，使用 4 个字节，只能表示从 1970 年到 2038 年\n  它和时区有关，每个时间戳在不同时区所代表的具体时间不同\n  默认情况下，如果插入时没有指定 TIMESTAMP 列的值，会将这个值设置为当前时间\n  MySQL 提供了 FROM_UNIXTIME() 函数把 UNIX 时间戳转换为日期，\n提供了 UNIX_TIMESTAMP() 函数把日期转换为 UNIX 时间戳\n  NULL  NULL跟任何值执行等值判断和不等值判断的结果都是NULL  索引 1.常见索引及概念   聚集索引：InnoDB中的主键索引，每张表的主键构造一棵B+树，同时叶子节点中存放的即为整张表的行记录数据。一个表只能包含一个聚集索引，聚集索引通常提供更快的数据访问速度。\n  非聚集索引：表中行的物理顺序与键值的逻辑顺序不匹配，查到记录对应的主键值 ，再使用主键的值通过聚集索引查找到需要的数据，这个过程也称为回表。\n要细分的话可以分为普通索引，唯一索引，组合索引，全文索引这些。\n  稠密索引：每个索引对应一个值\n  稀疏索引：每个索引对应一个存储块\n  覆盖索引：要查询的字段只需要去查询索引表就可以\n  组合索引：最左匹配，建立一个组合索引等于建立多个索引，能达到覆盖索引的目的，效率高；例如有组合索引(a，b，c)，则同时得到了索引(a)，(a，b)，（a，b，c）， MySQL5.6 后有个索引下推，当查询条件带a，b的时候，会先查找索引树匹配a，再判断b，然后才回表找数据，从而减少回表次数，如果没有索引下推，MySQL是先查找索引树匹配a，拿到id回表查数据，判断是否匹配b，这样回表次数就太多了。\n组合索引之索引是最左匹配跟B+树有关，也是类似Order by的过滤，根据索引依次排列数据的，如Order by a,b,c 则先排a，a相同再排b，b相同再排c\n  索引可能因为删除，或者页分裂等原因，导致数据页有空洞，重建索引的过程会创建一个新的索引，把数据按顺序插入，这样页面的利用率最高，也就是索引更紧凑、更省空间，但主键索引的重建会导致整个表重建，一般可以使用alter table T engine=InnoDB来达到重建主键索引的效果\n2.特点 需要建立的列：经常需要搜索的列、主键列、外键列、排序的列、经常在where后面出现的列\n 避免进行数据库全表的扫描，大多数情况，只需要扫描较少的索引页和数据页，而不是查询所有数据页。而且对于非聚集索引，有时不需要访问数据页即可得到数据。 聚集索引可以避免数据插入操作，集中于表的最后一个数据页面。 在某些情况下，索引可以避免排序操作 加速表与表之间的连接 在使用分组和排序子句进行数据检索时，可以显著减少查询中分组和排序的时间 增，删，改会带来不小性能开销  3.原理 b+树：表的数据为叶子节点，非叶子节点为索引，有两条路径，一条是树，一条是各叶子相连\t（mysql）\n注意\n B+树 \n注：\n N叉树的N在MySQL5.6后可以通过page大小来间接控制，叶子节点是数据页(page)，页与页之间组成双向链表 一个数据页(page)可以包含多个行(记录)，行按(记录)照主键顺序，行与行之间组成单向链表；每一个数据页中有一个页目录，方便按照主键查询行 页目录中通过槽把行(记录)分成不同小组，每个小组内包含多条行(记录)，按照主键搜索页中行(记录)时，使用二分法查找，从槽开始依次往下找 B+树的插入可能会引起数据页的分裂，删除可能会引起数据页的合并，二者都是比较重的IO消耗，所以比较好的方式是顺序插入数据，这也是我们一般使用自增主键的原因之一  b树：二叉搜索树，出度为2\nb-树：在b树的基础上，不限制出度的个数，所有节点为表的数据，只有一条路，从根节点开始\n B树 \n为什么说B+树比B树更适合MySQL数据库索引  B+树的磁盘读写代价更低：B+树的内部节点并没有指向关键字具体信息的指针，因此其内部节点相对B树更小，如果把所有同一内部节点的关键字存放在同一盘块中，那么盘块所能容纳的关键字数量也越多，一次性读入内存的需要查找的关键字也就越多，相对IO读写次数就降低了。\nB+树的查询效率更加稳定：由于非叶子点并不是最终指向文件内容的结点，而只是叶子结点中关键字的索引。所以任何关键字的查找必须走一条从根结点到叶子结点的路。所有关键字查询的路径长度相同，导致每一个数据的查询效率相当。\n由于B+树的数据都存储在叶子结点中，分支结点均为索引，方便扫库，只需要扫一遍叶子结点即可，但是B树因为其分支结点同样存储着数据，我们要找到具体的数据，需要进行一次中序遍历按序来扫，所以B+树更加适合在区间查询的情况，所以通常B+树用于数据库索引。\nB树不适合遍历数据，B树比较合适查询单一记录，常用与NoSQL的索引结构，NoSQL一般是Key-value形式的存储，文档性一般是Json存储\n 与二叉查找树、AVL树的比较  AVL树的出度为2，而且AVL树要严格保持平衡，但旋转保持平衡比较耗时，适合用于插入删除次数比较少，但查找多的情况 二叉查找树在查找最大值或最小值的时候，二叉查找树就完全退化成了线性结构了 其他缺点同下面  与红黑树的比较   红黑树出度为2，B+树出度不止2，因此红黑树的高度会比B+树高，查找的次数也多了。（红黑树不是严格的平衡二叉树，旋转次数相对少，高度比平衡二叉树的低些）\n  B+树在降低磁盘I/O操作数方面占优势\n 为了减少磁盘 I/O，磁盘往往不是严格按需读取，而是每次都会预读。预读过程中，磁盘进行顺序读取，顺序读取不需要进行磁盘寻道，并且只需要很短的旋转时间，速度会非常快。\n操作系统一般将内存和磁盘分割成固态大小的块，每一块称为一页，内存与磁盘以页为单位交换数据。数据库系统将索引的一个节点的大小设置为页的大小，使得一次 I/O 就能完全载入一个节点。并且可以利用预读特性，相邻的节点也能够被预先载入。\n   参考一步步分析为什么B+树适合作为索引的结构\n4.失效情况 在使用到索引列的情况下\n 对索引列作运算如 + - * / ! 索引属性出错，比如该索引列是字符串，但是写时候没有加``号，字符串和数字比较，MySQL会把字符串转为数字，无法转成数字的字符串都会被转换成0；或者是字段的字符编码格式不同，，比如有两张表，它们有外键关系，但是由于一张表是UTF-8另一张表是UTF-8mb4，也会导致索引失效；它们的原因都是MySQL在进行比较的时候，会使用函数对字段进行转换 模糊查询 like ‘%keyword%` 查询不能有前置的%，如果是 like ‘keyword%’ 这样还是可以用到索引的 索引列里有字段为null，null值不会加入到索引中 使用or连接条件，如果or连接的条件中有一个不是索引，会失效，可以改成使用union all来连接两条sql语句 组合索引没有体现最左匹配 is null / is not null 对索引作判断 索引上使用 != 或者 \u0026lt;\u0026gt; 还有not in 索引的值只有几种情况，如性别只有男和女，这种情况虽然也会用索引，只是意义不大 表的量级较小，存储引擎判定使用全表扫描更快 有一case：1.select * from T where k in(1,2,3,4,5)；2.select * from T where k between 1 and 5，k为索引，但是推荐使用方法2，因为方法1会导致树查5次，而2是1次 对索引字段使用了函数进行计算，可能会导致MySQL不使用该索引，或者进行了全索引扫描，无法用到索引进行快速定位 如果查询的值的长度是否大于索引定义的长度，如果大于，虽然也会走索引，因为MySQL是先把查询值的长度截断成跟索引定义的长度一致去遍历索引，但是它还要再回表得到数据进行比较，所以查询会很慢  5.优化   注意区分度，计算索引最优长度，使用这个计算 count(distinct left(列名, 索引长度)) / count(*) from table，区分度越高越好，另外使用前缀索引虽然会减少索引存储空间，但是可能会增加扫描次数或者覆盖索引不生效\n  当表的字符集编码或者属性不同时，需要想办法把函数加再索引对应的值上，而不是索引字段上，或者去掉函数，使用其他方法替代\n  当要充当索引的字段在某些长度的区分度太小时，可以增加一个字段，采用索引字段的倒序存储或者hash的方法来充当索引，缺点是无法使用索引进行范围查询，而hash更是只能支持等值查询，查询时需要进行额外的计算，也是一种性能消耗\n  由于MySQL在选择索引的时候会根据 索引区分度 和索引对应的预估扫描行数（包括回表），但是有可能预估的结果是不准的，如果通过explain命令发现rows的值与想象中的偏差较大，可以执行analyze table [tableName]来重新统计索引信息，或者使用force index([索引名称])来强制使用索引，或者重写SQL，引导MySQL使用正确的索引\n  注意索引的最左前缀原则，如果在设置联合索引时，可以通过调整顺序来达到少维护一个索引，拿这个顺序就可以优先考虑，因为一个要考虑的就是索引的大小\n  尽量的扩展索引，不要新建索引。比如表中已经有a的索引，现在要加(a,b)的索引，那么只需要修改原来的索引即可。\n  关于普通索引和唯一索引\n在查询上，唯一索引和普通索引的区别是，唯一索引在查找到结果后就不会继续往下查了，但其实性能跟普通索引差别不会很大，但是更新的时候唯一索引由于用不上change buffer机制，更新的性能比较差\n在更新上，InnoDB会先判断更新的数据是否在内存，如果在就直接更新内存，如果不在，就把更新操作写到change buffer，等到数据加载到内存，在从change buffer里将更新操作更新到内存。change buff只适用普通索引上的更新操作，因为唯一索引因为需要先读取所有数据，判断索引是否重复后再插入，如果此时数据没有被读进内存，需要磁盘随机IO读取，最终导致更新变慢。\n另外，为了保证更新操作的稳定性，实际上在写内存的过程中还会把相关操作记录按顺序写进redo log（磁盘），才算真正完成更新操作。查询的时候其实可以直接查内存里的数据（内存已更新），或者先把磁盘里的数据读到内存，再配合change buffer就能得到更新后的数据了\n  使用临时表，如果是InnoDB引擎，那创建的临时表是写磁盘的，如果是Memory引擎，则是写内存的。创建语句是 create temporary table ···engine=xxx，用法跟普通表类似，但仅在当前线程可见，show tables不显示临时表，当当前线程处理完成后，临时表会被清空，但会保留表结构，允许在不同线程间重名，如果bin log格式是statement或mix，操作临时表的记录也会记录在bin log，备库也会跟着操作。\n内部临时表常用于不带索引的join、分库分表时的联合查询、union查询(对两个查询结果求并集并去重，union all则不去重)、group by查询。当查询的数据量比较大时，默认会先使用内存临时表，发现太大后才转成磁盘表，因此如果数据量太大，可以使用SQL_BIG_RESULT（ select SQL_BIG_RESULT 字段 from 表 group by xxx）告诉MySQL强制使用磁盘临时表\n一般情况下还是使用内存表快些，通过调大tmp_table_size来加大内存临时表的大小，默认是16M，这种临时表是由MySQL查询算法决定使用的\n  group by 或order by多个字段时，需要为这多个字段建组合索引，不然也是全表扫\n  关于大数据量时的分页，优化的思路也是尽可能的使用索引，比如\n select * from table where id \u0026gt; (页数*页面大小) order by id limit M，缺点是不太准确，会漏数据， order by 使得结果稳定 select * from table where id \u0026gt; (select ID from table order by id limit M, 1) order by id limit n，先利用子查询把id查出来，依靠id上的所有，外层效果跟上面的类似 select * from table where id in (select id from table limit M, N)，同样也是利用id上的索引，覆盖索引，只查主键时，效率很高。    6.分析   Explain + SQL语句，给出该SQL语句的分析结果，看看查询的类型，有没有用到索引，是不是全表扫描\n比较重要的字段：\n select_type：查询类型，如 简单查询、联合查询、子查询等 type：访问类型，ALL(全表扫描)、index(索引查询)、range(索引范围查询)、ref(表间的连接匹配条件)、const(常量)，index_merge(索引合并，5.0后才有的功能，使得MySQL可以在一次查询种使用多个索引，但是使用场景比较局限，多发生在查询条件涉及多个and和or的场景)，一个好的SQL起码得达到range级； possible_keys：能使用哪个索引找到行，查询涉及到字段上若存在索引，则该索引将被列出，但不一定被查询使用 key：索引列的名称，如果没有使用索引，显示位NULL ref：表示上述表的连接匹配条件，即哪些列或常量被用于查找索引列上的值 rows：扫描的行数 extra：额外信息说明，using index指使用到了覆盖索引；using index condition指使用了索引下推；using join buffer（block nested loop）指使用join连表，算法是block nested loop；using union指使用索引并取并集；using sort_union 指先对取出的数据按rowid排序，然后再取并集；using intersect 索引取交集；    show processlist，此命令用于查看目前执行的sql语句执行的状态，比如当CPU使用率飙升时，可通过该命令查看哪些SQL语句在执行\n  show status，查看数据库运行的实时状态，比如查询运行期间SQL的执行次数、连接数、缓存内的线程数量、连接数等，具体看 mysql SHOW STATUS 变量\n  show variables，查看系统参数，一些静态参数，比如开启慢查询，设置索引缓冲区大小，具体参考：Mysql show variables命令详解\n  performance_schema和sys系统库\n  MySQL启动前需要设置performance_schema=on，但是性能会比off少10%\n  查询sys.schema_table_lock_waits、sys.innodb_lock_waits表可以知道那条语句在占用锁\n  查看 information_schema.innodb_trx表可以看到事务具体的状态\n    使用show engine innodb status查看数据库请求情况\n  使用show status like 'innodb_row_lock%';查询行锁竞争情况\n  使用show status like 'table%';查询表锁竞争情况\n  select trx_id,trx_state,trx_started,trx_wait_started,trx_operation_state,trx_tables_in_use,trx_rows_locked,trx_rows_modified,trx_query from information_schema.innodb_trx;查询当前事务情况\n  select * from information_schema.innodb_lock_waits;查看锁等待对应关系\n  select * from information_schema.innodb_locks;查看当前出现的锁\n  当有语句执行过久或有语句一直被阻塞时，可以kill掉它\n使用kill query/connection + 线程id终止语句或连接，但kill不是直接终止线程，只是告诉该线程这条语句不用继续执行了，MySQL会在执行逻辑上打上断点标记，线程执行到该位置，才会判断状态是否是处于被kill状态，然后进行收尾工作，比如释放掉之前持有的锁\n  慢查询分析\n  打开：set global slow_query_log='ON'，临时开启，无需重启，永久开启则在my.cnf文件里设置\n设置日志存放位置：set global slow_query_log_file='/usr/local/mysql/data/slow.log';\n设置超过x秒就会记录到慢查询日志中：set global long_query_time=x;\n查看慢查询相关设置：show variables like 'slow_query%';\n慢查询日志分析工具：mysqlsla、mysqldumpslow\n慢查询日志中，rows_examined字段，表示某个语句执行过程中扫描了多少行\nMySQL索引原理及慢查询优化\n存储引擎 1.MyISAM   设计简单，数据以紧密格式存储。对于只读数据，或者表比较小、可以容忍修复操作，则依然可以使用它。\n  提供了大量的特性，包括压缩表、空间数据索引等。\n  不支持事务。\n  不支持行级锁，只能对整张表加锁，读取时会对需要读到的所有表加共享锁，写入时则对表加排它锁。但在表有读取操作的同时，也可以往表中插入新的记录，这被称为并发插入（CONCURRENT INSERT）。\n  可以手工或者自动执行检查和修复操作，但是和事务恢复以及崩溃恢复不同，可能导致一些数据丢失，而且修复操作是非常慢的。\n  如果指定了 DELAY_KEY_WRITE 选项，在每次修改执行完成时，不会立即将修改的索引数据写入磁盘，而是会写到内存中的键缓冲区，只有在清理键缓冲区或者关闭表的时候才会将对应的索引块写入磁盘。这种方式可以极大的提升写入性能，但是在数据库或者主机崩溃时会造成索引损坏，需要执行修复操作。\n2.InnoDB   是 MySQL 默认的事务型存储引擎，只有在需要它不支持的特性时，才考虑使用其它存储引擎。\n  实现了四个标准的隔离级别，默认级别是可重复读（REPEATABLE READ）。在可重复读隔离级别下，通过多版本并发控制（MVCC）+ 间隙锁（Next-Key Locking）防止幻读。\n  主索引是聚簇索引，在索引中保存了数据，从而避免直接读取磁盘，因此对查询性能有很大的提升。\n  内部做了很多优化，包括从磁盘读取数据时采用的可预测性读、能够加快读操作并且自动创建的自适应哈希索引、能够加速插入操作的插入缓冲区等。\n  支持真正的在线热备份。其它存储引擎不支持在线热备份，要获取一致性视图需要停止对所有表的写入，而在读写混合场景中，停止写入可能也意味着停止读取。\n  关于有时更新或者查询时突然变慢的原因：首先，InnoDB更新采用WAL机制，即更新时通过redo log记录更新操作，修改内存里的数据，然后再在恰当时间flush到磁盘（即先写日志，再写磁盘），原因就是出在flush磁盘上，当redo log满了，或者内存满了，脏页太多，都会将内存里的数据flush到磁盘，以腾出空间，如果要刷的数据特别多，那消耗的时间就长。\n解决方法：1. 设置到 innodb_io_capacity参数，该参数会告诉InnoDB机器的磁盘能力，可以使用fio工具测出 2. 控制脏页的比例，设置innodb_max_dirty_pages_pct的值，默认是75%，达到了就会刷 3. 刷新脏页时是否会递归检测隔壁数据页是否也是脏页，如果是会连着一起刷，通过innodb_flush_neighbors=1表示采用这种机制，=0表示只会刷自己，这个机制对机械硬盘关系比较大，SSD则不会\n关于InnoDB的删除：\ninnodb_file_per_table=OFF：表示表数据放在系统共享表空间，=on表示各个表空间放在独立文件下，后缀名是 .ibd ，一般设置=on，便于管理。\n当我们使用delete删除数据时，InnoDB实际上是把数据页上的该数据标记为删除，表示该位置可以进行复用，此时磁盘上的文件并不会变小，当数据随机插入时会因为页分裂，分裂后的页可能存不满数据，就会标记某些位可复用，导致页的利用率不高，当有大量的增删时，会导致数据页存在大量空洞，为了压缩空间，此时的解决办法是重建表，一般使用alter table [tableName] enging=InnoDB达到重建的目的，MySQL会自动创建临时表，进行数据转存，交换表名，删除旧表，此时会阻塞，阻止增删改，5.56版本后使用onlineDDL机制，解决了这个问题，解决方法是使用redo log记录新插入的数据 + MDL读锁(写锁会退化) + IO + CPU\n3.Memory  menory引擎的内存表不同于InnoDB的内部临时表，内存表是库内全局可见，写内存的，而innodb的内部临时表是写磁盘的，同一线程内可见，线程结束就会清空 默认是hash索引，数据单独存放，索引上保存数据的位置，这种形式称为推组织表，而InnoDB那种b+数结构的则是索引组织表 InnoDB数据存放是有顺序的，因此有可能会产生空洞，而memory则是有空位就可以存放，因此当数据位置发生变化时，innoDB只需要修改主键索引，而内存表需要修改所有索引（哈希表扩容导致） 内存表不支持变长数据类型，只能固定字符串长度，即varchar(N)会退化为char(N) 内存表的索引地位是相等的，而Innodb表分成了主键索引和普通索引 由于hash索引并不适合索引范围查询，范围查询实际上是全表扫描，如果要支持索引范围查询，需要建立B树索引 内存表仅支持表级锁，粒度较大，并发度低 重启会清空内存表的特性会影响主备复制  4.MyISAM与InnoDB引擎区别  MyISAM是非事务安全；InnoDB是事务安全型 MyISAM的锁是表锁；InnoDB支持行锁 MyISAM支持全文索引；InnoDB不支持（5.6版本后才支持） MyISAM适合 SELECT 密集型的表；InnoDB适合 INSERT 和 UPDATE 密集型的表 MyISAM表是保存成文件形式，跨平台转移方便 InnoDB表比MyISAM表安全 MyISAM对于不会进行修改的表，支持压缩表，极大减少磁盘空间占用 MyISAM 崩溃后发生损坏的概率比 InnoDB 高很多，而且恢复的速度也更慢；InnoDB支持安全恢复 MyISAM不支持外键；InnoDB 支持外键，InnoDB 支持在线热备份  事务 1.ACID原则 原子性（atomicity）、一致性（consistency）、隔离性（isolation）和持久性（durability）\n  原子性：事务中的所有操作要么全部提交成功，要么全部失败回滚\n  一致性：数据库总是从一个一致性的状态转换到另一个一致性的状态\n  隔离性：一个事务所做的修改在最终提交以前，对其他事务是不可见的\n  持久性：一旦事务提交，则其所做的修改将永久保存到数据库\n  2.并发情况下带来的问题   脏读：如有事务A和B，A读取了B未提交的数据\n  丢失更新：如有事务A和B，AB均写入数据，A写入的数据被B覆盖\n  不可重复读：如有事务A和B，A负责读取，B负责写入，A连续读的过程中B写入了一次，A前后两次读出来的数据不一样\n  幻读：如有事务A和B，A修改表内数据的过程中，B向表内插入了一条数据，A修改完后发现数据并没有被全部修改完，或者是在RR隔离级别下，事务A内前后两条相同的SQL带“当前读”查询查回来的数据数不一致\n  不可重复读和幻读的区别：不可重复读侧重于update，而幻读侧重于insert和delete。不可重复读是在一个事务内前后两次读取的数据不一致，此时数据数量没有变化，重复读取得到的数据不一致，所以叫不可重复读；而幻读是在一个事务内前后两次读取的数据不一致，读的数据量变多或者变少了，这些多了的数据或少了的数据就像幻觉，所以叫幻读\n3.事务隔离级别 隔离级别就是为了解决上述并发时候带来的问题\n  DEFAULT：默认隔离级别，即使用底层数据库默认的隔离级别；\n  READ_UNCOMMITTED：未提交读，一个事务未提交时，它的变更可以被其他事务看到\n可能出现 脏读、不可重复读、丢失更新、幻读；\n  READ_COMMITTED：提交读，一个事务提交之后，它做的变更才会被其他事务看到，保证了一个事务不会 读 到另一个并行事务已修改但未提交的数据\n避免了“脏读”，可能出现不可重复读、丢失更新；\nOracle默认隔离级别\n  REPEATABLE_READ：可重复读，一个事务在执行中看到的数据，总是跟这个事务在启动时看到的数据一致，保证了一个事务不会 修改 已经由另一个事务读取但未提交（回滚）的数据。\n避免了脏读、不可重复读取、丢失更新，可能存在幻读；\nmysql默认是此隔离级别；MySQL使用MVCC和间隙锁来防止 幻读\n  SERIALIZABLE：序列化,最严格的级别，事务串行执行,即一个事务要等待另一个事务完成才可进行\n效率最差，但也解决了并发带来的那4种问题\n  例子：\n   事务A 事务B     启动事务，查询得到值1 启动事务    查询得到值1    将1改为2   查询得到的值v1     提交事务B   查询得到的值v2    提交事务A    查询得到值v3     在不同隔离级别下的答案\n未提交读：v1=2，v2=2，v3=2\n提交读：v1=1，v2=2，v3=2\n可重复读：v1=1，v2=1，v3=2\n串行：v1=1，v2=1，v3=2，且直到事务A提交后，事务B才可以继续执行\n一般避免使用长事务，即在一个事务里做过多操作，长事务会导致回滚日志变大，也会占用锁资源\n4.事务相关命令  显式启动事务，使用begin或strart transaction启动事务，commit提交事务，rollback回滚 set autocommit=0，关掉自动提交，任何语句执行都需要显式的提交(主动commit或rollback)才算执行完成 set autocommit=1，执行任意一条语句都会默认开启单次事务执行完成后隐式提交，事务也可以显式开启，直到显示使用commit、rollback或断开连接。一般是使用set autocommit=1，开启事务，再commit提交事务，执行commit work and chain则提交事务并开启下一次事务  锁 根据范围，可以分为全局锁、表级锁、行锁，当多种锁同时出现时，必须得所有锁不互斥，才能并行，否则就得等。\n  全局锁：对整个数据库实例加锁，命令： Flush tables with read lock，让整个数据库变成只读，禁止任何ddl、dml语句\n一般用于全库逻辑备份，但有可能造成主从库数据延迟或者业务停摆，不用的话又会导致数据不一致问题，一般这种方式是给不支持 可重复读 事务的引擎使用的，像InnoDB可以在可重复读隔离级别下开启事务读数据，利用MVCC来保证在此期间数据一致\n  表级锁：\n一种需要显示启动，比如lock tables t1 read, t2 write; 表示线程在执行unlock tables之前，只能读t1，读写t2，其他操作做不了。\n另一种是 MDL(metadata lock)，不需要显示使用，在访问一个表时自动加上，作用是保证读写正确性，当对一个表内数据做CRUD时，加MDL读锁，当对一个表做结构变更时，加MDL写锁。\nMDL锁和表锁时可以同时出现的，比如MyISAM表上更新一行，会加上MDL读锁和表锁\n读锁间不互斥，读写、写写间互斥，MDL会在事务提交后释放。当需要对热点表做结构变更时，最好在变更语句上加等待时间，避免出现死锁导致整个表无法读写，\n  行锁：在InnoDB事务中，采用二阶段锁协议，行锁是在事务结束后才释放，在事务过程中，即使一开始用了后面没用到也不会被释放，因此，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放，减少锁的影响时间；行锁是统称，主要可以细分为共享锁和排他锁这些。\n  记录锁：行锁，对索引进行锁定，如果是主键索引就会锁一条，但如果是二级索引，就会锁所有匹配的记录。\n  共享锁 - S锁（SQL + lock in share mode）：行锁，事务T对数据A加上共享锁，其他事务只能对A添加共享锁，不能加排他锁，获取共享锁的事务只能读不能写。\n  排他锁 - X锁（SQL + for update）：行锁，事务T对数据A加上排他锁，则其他事务不能再对A加任何类型的锁。获得排它锁的事务即能读数据又能修改数据。\n  共享意向锁 - IS锁：表级锁，表示事务持有表中行的共享锁或者打算获取行的共享锁，仅表示意图，不阻塞其他操作，当事务在获取表中的共享行锁时，需要先获取表中的共享意向锁\n  共享排他锁 - IX锁：表级锁，表示事务持有表中行的排他锁或者打算获取行的排他锁，仅表示意图，不阻塞其他操作，当事务在获取表中的排他行锁时，需要先获取表中的共享排他锁\n共享锁的作用主要在alert语句修改表结构的时候使用，场景：\n事务A想修改表T的行R，A获得行R的排他锁，锁住了行R，事务B使用Alter Table语句修改表T的结构，此时需要获取表T的共享锁，由于它不知道表T是否存在行锁，只能去遍历，当表有行锁时，只能等行锁释放才能修改表结构，因为遍历很耗性能，所以需要意向锁来解决这个问题，事务A在获得行R的排他锁时，需要先上表T的共享排他锁，事务B在Alter 表T时就可以直接判断该表是否被上行锁了。\n  Auto-Inc 锁：自增锁，比较特殊，当设置键为自增时使用，比如自增主键，在生成自增id时，会先获取相关表的 Auto-Inc 锁，阻塞其他事务的插入操作，保证自增的唯一性。不遵循二阶段锁协议，因为它并不是在事务提交时释放，而是在inset语句执行后释放，即使是在回滚时，自增值也不会减一。\n  多版本并发控制（MVCC）：用来解决读-写冲突的无锁并发控制，为事务分配一个单向增长的时间戳，为每个修改保存一个版本，版本与事务时间戳关联，读操作只读该事务开始前的数据库的快照。MVCC 在语境中倾向于 “对多行数据打快照造平行宇宙”，然而 CAS 一般只是保护单行数据而已\nMVCC只在提交读 和 可重复读隔离级别下有效，保证在读操作不用阻塞写操作，写操作不用阻塞读操作的同时，避免了脏读和不可重复读。 对于可重复读，查询只承认在事务启动前就已经提交完成的数据； 对于提交读，查询只承认在语句启动前就已经提交完成的数据;\nMVCC的版本快照，指的是什么呢，而且它是基于整个库的，总不能保存多个版本的库的所有数据吧？\n实际上MVCC得到的快照是逻辑上的数据，是推测出来的，通过当前值，利用事务Id和undo log日志，根据日志\u0026quot;回滚\u0026quot;得到各个版本的数据，事务id可以简单理解为对该行数据进行更改时产生的id，当然一个事务内可以对多条数据进行操作，这多条数据的事务id都是相同的\n例子：\n假如一个值1被按顺序改为2、3、4，每一次更改都会记录在回滚日志(undo log)里，如：将2改为1 -\u0026gt; 将3改为2 -\u0026gt; 将4改为3，当前的值是4，在查询这条记录的时候，不同时刻启动的事务会有不同的视图，比如有视图A（将2改成1）里看到的值是1，即同一条记录在数据库中存在多个版本\n  间隙锁：当我们用范围条件而不是相等条件检索数据，并请求共享或排他锁时，InnoDB会给符合条件的已有数据记录的索引项加锁；对于键值在条件范围内但并不存在的记录，叫做间隙锁。\n主要是为了解决幻读问题，只在可重复读隔离级别下有效，一般与行锁一同出现。\n间隙锁也会导致死锁，比如两个事务同时在一段范围内的数据加入间隙锁(事务间的间隙锁不会冲突，可以加成功)，又insert或update在这个范围内的数据，就会导致死锁\n另外，\u0026ldquo;间隙\u0026rdquo; 是 由这个间隙右边的记录定义的，也就是说，如果 有索引 c的值为0，5，10，15，先有事务A查询select c from t where c \u0026gt; 5 lock in share mode，加锁范围是(5, 10], (10, 15], (15, max]，然后有事务B更新update t set c = 1 where c = 5，执行成功，此时c的值有 0，1，10，15，再执行update t set c=5 where c=1;会执行不成功，被blocked住，原因是间隙锁的间隙变成了 [1, 10]，所以更新失败\n  Next-Key 锁 = 间隙锁(gap lock) + 行锁(record lock)，在可重复读隔离级别下，InnoDB在查找和扫描索引时，都会使用Next-Key锁来防止幻读的发生。\n  插入意向锁（II Gap）：特殊间隙锁，只有在插入时使用，表示插入的意向，属于行级锁，但不与行级锁冲突，而是与间隙锁和 Next-Key 锁冲突，当插入前需要获取插入意向锁，此时会与 Next-Key 锁冲突而阻塞，从而防止幻读。\n  可重复读隔离级别时的加锁规则：\n  查询过程中扫描到的行才会加锁，锁的基本单位是next-key lock（左开右闭）\n  如果等值查询的对象不存在，会在该等值的 前后 遇到的第一个存在的数据的这段范围加上间隙锁（左开右开）\n  索引上的等值查询，如果是唯一索引，加的是行锁；如果非唯一索引，需要访问该等值的左边到右边第一个不满足条件的值，这个范围加上间隙锁（左开右开，中间包含该等值）\n可以理解为，等值查询的加锁范围是从该等值左边第一个不满足条件的值开始到该等值的右边第一个值不满足条件的值的左开右开区间，同时包含该等值，如果是唯一索引，则退化成行锁\n  不带等值的范围查询上，无论是否是唯一索引，范围查询都需要访问到第一个不满足条件的值为止，在这个范围加间隙锁（左开右闭），带等值的范围查询时，规则还是同上\n  锁是加在索引上的，根据where条件依次加上，比如有表A，索引为id、a、b，当update时的条件是where a=xx时，会对索引a、id、b的顺序加锁；间隙锁只会加在where条件中的索引上，对于该索引定位到的行加的是行锁\n  select加的锁，如果查询的列刚好是满足覆盖索引，且覆盖索引不包括其他索引，则只会锁where条件上的索引\n  以上规则需要组合起来使用，InnoDB会对扫描过的行都加上行锁和间隙锁，所以如果查询条件不是索引，就会全表扫描，并对扫到的行主键上锁，表现出来就是锁表了。\n在InnoDB中，一般设置innodb_thread_concurrency的值在64~128之间，=0表示不限制并发线程数量，这里的并发线程数量指的是并发查询数量，并发连接数量可以有上千个，但是并发查询数量不能太多，否则太耗CPU资源，另外，在查询进入锁等待时，并发查询数量会减一，不计入并发查询数量里，select sleep(秒)则会计入。\n关于死锁\n当线程出现循环资源依赖，导致多个线程互相等待的状态称为死锁，解决方案：\n 设置超时时间 innodb_lock_wait_timeout，在InnoDB中默认是50s 死锁检测，当发现死锁后，主动回滚死锁链条中的某一个事务，设置innodb_deadlock_detect=on，默认值是on  一般使用第二种，但死锁检测会消耗大量CPU资源，主要发生在对同一行进行更新的检测上，其算法是O(n)，虽然在同一行更新不会造成死锁，但是当并发很高时进行检测时就会消耗大量CPU资源，解决方案有两种，一种是在中间件或者MySQL server层上，增加对同一行更新的判断，进行排队，或者将那一行改为逻辑上的多行，来分散压力\n因此对同一组资源，尽量以相同的顺序访问\n关于事务可见性的疑问\n在可重复读的隔离级别下，事务A开启，操作数据D，此时会创建一个当前数据D的视图(MVCC)，但此时又刚好有事务B，已经操作到这条数据，并给这条数据加了行锁，对这条数据进行操作，事务B操作完成后，释放数据D的行锁，那之前的事务A在最终修改操作数据D时，数据D的值是什么呢？\n假如一开始k的值是1，autocommit=1，即单独一条SQL执行本身就是一个事务，会自动提交\n另外，事务中的begin / start transaction命令，是以执行它们之后的第一个sql语句为启动开始事务，而start with consisten snapshot是以这条命令为起点开启事务\n   事务A 事务B 事务C     start transaction with consisten snapshot;      start transaction with consisten snapshot;      update t set k = k + 1 where id = D    update t set k = k + 1 where id = D;     select K from t where id = D;    select k from t where id = D;     commit;      commit;     上面这道题的答案是 事务A查到k的值分别是1，事务B查到k的值是3\n原因：事务A之所以查到的值是1，是因为事务A开启时，事务B和C还没开启，此时的快照k=1，因此得到的值是1。事务B查到k的值是3，虽然事务C是在事务B之后开启的，感觉看不到事务C修改后的值，但是由于更新操作是先读后写的，此时的读是当前读（当前读总是读该已提交的数据的最新版本），而当前读的值是2，因此更新后k的值是3，如果不这么做，就会导致事务C更新丢失，而在同一个事务内读值，是可以读到由当前事务修改的值的，所以事务B读到的值是3。普通select语句，在可重复读情况下，为了实现一致性读，是通过读undo log实现的，如果undo log太长（可能因为更新次数太多），会导致查的很慢\n题外话，如果事务A读的时候加锁，就会变成当前读，例如将事务A的select语句后面加上lock in share mode（共享锁）或者for update（排他锁），那么查到的值就是3了。如果事务B在执行更新前先select了，查到的值也是1。如果事务C是显式启动事务，在事务B select后commit前才执行commit操作，就会触发二阶段锁协议，两条更新语句同时更新一行数据，先执行的语句会对这条数据加行锁，所以事务B需要等到事务C提交后，才能执行更新操作\n这个问题的关键在于要理解 MVCC原理，更新操作前的当前读，事务的隔离级别，一致性读、行锁\n参考：\nMysql加锁过程详解（1）-基本知识\nMysql中的锁机制\nInnodb中的事务隔离级别和锁的关系\n大表优化 MySQL大表优化方案\n分库分表 主要是为了解决数据量太大导致查询慢（一般可以分表，主从分离），还有高并发问题（一般是分库）。\n分表时一般分为水平拆分(对行)和垂直拆分(对列)\n分片算法   范围分片，但容易产生热点问题\n  哈希分片，针对某一列做哈希取模，平均分配到各个分表中，如果要扩容，由于模会变，导致数据要重新哈希，停机迁移数据，这样是不行的，因此一开始要设计好，例如使用一致性哈希算法，减少迁移的数据量；或者分表的时候取2的n次方，这样扩容的时候也以2的n次进行扩容，这样原来的key重新取模是在原来的位置或者原来的2倍；或者换个数据库了，像上面大表优化里面提到的那样。\n  查表法，先通过分片映射表查到要查询的分片，再到对应的分片进行查询，但是要二次查询，即使上缓存，查询性能也是一般。\n  Sharding Key选择问题 一般是使用最常用的查询条件做分片key。当分库分表遇到Sharding Key冲突时，只能选择冗余数据了。或者通过将多个列组合成一个新的列来标识。\n数据部分 MySQL一般的集群架构是 有两台MySQL实行双Master部署，进行主备复制，注意要把备用的master的主备复制关闭，避免循环复制，另外会部署一些机子作为从库，以其中一台master为主库进行部署\n数据存储和恢复 只要redo log 和 bin log能够持久化到磁盘，就能确保MySQL异常重启后，数据就可以恢复\n原理   bin log的写入机制：事务执行过程中，先把日志写进bin log cache，事务提交时，再把bin log cache写进bin log文件（先写到文件系统的page cache，再进行持久化）中，然后把bin log cache清空。\nbin log cache每个线程自己维护，bin log的写入是一个顺序操作\nbin log cache的大小通过binlog_cache_size控制，如果超过就暂存到磁盘\nsync_binlog=x表示每次事务提交都会write到磁盘page cahce，但是会累计提交x个事务后才把bin log的数据持久化到磁盘，一般设置范围是100~1000，对应的风险是，如果机器宕机，会丢失最近N个事务的bin log日志\n  redo log写入机制：原理与bin log类似，但是它是二阶段提交，有状态，事务执行过程中，redo log先是prepare状态，写入redo log buffer，再写bin log，提交事务，变为redo log commit状态（即WAL机制）\nredo log buffer全局共用，与bin log cache不同\ninnodb_flush_log_at_trx_commit=0，表示每次事务提交只是把redo log写入redo log buffer，=1 表示每次事务提交会持久化redo log到硬盘中，=2 表示每次事务提交会把redo log写到page cache，innodb后台有进程每秒钟将redo log buffer中的日志写进page cache，再持久化到磁盘，所以有可能会把事务未提交的redo log持久化到硬盘\n  一般会把sync_binlog和innodb_flush_log_at_trx_commit都设置为1，即一个事务完整提交前，会刷两次盘。另一种设置是让sync_binlog=1000和innodb_flush_log_at_trx_commit=2，一般是在主备复制存在很大延迟时，为了让从库的备份速度跟上主库\n为了提高刷盘效率，MySQL一般会让多个事务在一段时间内完成，或尽量让page cache里的redo log和bin log组合在一起提交，减少刷盘次数\n主从复制 一般从库设置为read only，可以避免主从切换过程的双写，实现的是最终一致性。\n原理 利用MySQL中的bin-log二进制文件，该文件记录了所有sql信息，主数据库会主动把bin-log文件发送给从数据库，在从数据库的relay-log重放日志文件中利用这些信息进行恢复。\nbin log分为三种格式\n statement：记录每次执行的SQL，但由于索引选择问题或者SQL语句使用聚合函数，有可能会导致主从不一致的问题 row：记录的是事件，表示每条SQL语句执行后的数据信息，比如delete操作后，会记录delete事件和delete删除的行的所有字段(可设置为记录所有字段或者只记录主键)；update操作会记录行数据前后的记录；insert操作会记录insert的所有字段信息，缺点是占空间，但对数据的恢复有利 mix（混合上面两种）：由MySQL自己判断，如果会出现主从不一致，就使用row，否则使用statement  bin log上会记录每台机子的server id，用于避免循环复制\n具体步骤 每个MySQL数据库上都有这三个线程，默认是主服务器写完Bin Log后就算事务成功，Bin Log复制是异步执行。\n  binlog 线程 ：负责将主库上的数据更改写入Bin Log。之后事务线程提交事务，响应成功给客户端。\n  I/O 线程 ：负责从主库上读取Bin Log，并写入从库的重放日志（replay log）中。\n  SQL 线程 ：负责读取replay log日志并重放其中的 SQL 语句。MySQL 5.6前只支持单线程，5.6后改为多线程，SQL线程分为container对事务进行分发，调度不同的worker线程进行执行，分发策略：1、更新同一行的两个事务，必须被分发到同一个worker；2、同一个事务不能被拆开，必须在同一worker中执行\n从库在同步Bin Log时，必须保证Bin Log的顺序，才能确保数据一致性。\n   MySQL主从复制 \n默认情况下，由于是异步复制，无法保证数据第一时间复制到从库上，但如果采用同步复制，即等从库复制完主库的Bin Log后才响应给客户端，性能就太差了。\n在MySQL 5.7后的版本，增加半同步复制(Semisynchronous Replication)，事务线程不用等到所有都复制成功才响应，只要一部分复制响应回来后即可响应给客户端，比如一主二从，等一从成功即可成功。配置rpl_semi_sync_master_wait_no_slave表示至少等待多少个从库复制成功才算成功。rpl_semi_sync_master_wait_point表示主库执行事务的线程是提交事务前等待复制(默认)，还是提交事务之后等待复制\n主备延迟 当备库重放日志的速度小于主库产生bin log的速度，会出现主备延迟，可能的原因：\n 主备机器配置不一致，备库机器性能较差 备库压力大，比如在备库上进行SQL分析、大量查询、大表的DDL，主库上的长事务操作等，消耗大量CPU资源导致  默认情况下，异步复制也会带来的读延时问题，可以采取 一主多从，主写从读，分散压力；利用好缓存中间件；持久化层的处理。\n在MySQL 5.6后的版本，可以设置slave_parallel_workers来决定从库在进行重放时工作的线程数，一般设置在8~16，以通过并行重放的目的加快主从复制速度。\n当MySQL集群搭建采用一主多从时，最好采用GTID模式来实现一主多从的切换。\n主备切换策略  可靠性优先：前提，备库是只读的，首先，备库持续判断与主库同步间的延迟时间，如果小于可接受的值，主库改为只读，主库等待备库同步延迟时间降为0，备库改为可读写，业务切换到备库，业务不可写的时间取决于主库等待备库同步数据的延迟时间 可用性优先：步骤与上面的类似，只是主库改为只读后，不等待备库同步完数据，就切到备库，此时数据会不一致，后面再自己根据bin log手动调整更正  MySQL的高可用（通过主库发生故障时切到从库），是依赖主备复制的，主备延迟时间越小，可用性越高\n库内表的复制 将一张表里的数据导出到文件，再写回原表，以下两种方式都可以跨引擎\n  使用mysqldump，将表里的数据转成insert语句\nmysqldump -h$host -P$port -u$user --add-locks = 0 --no-create-info --single-transaction --set-gtid-purged = OFF db1 t --where =\u0026quot;a\u0026gt;900\u0026quot; --result-file =/client_tmp/t.sql – single-transaction：，在导出数据的时候不需要对表db1.t加表锁，而是使用STARTTRANSACTIONWITHCONSISTENTSNAPSHOT的方法； – add-locks：设置为0，表示在输出的文件结果里，不增加\u0026quot;LOCKTABLEStWRITE;\u0026quot;； – no-create-info：不需要导出表结构； – set-gtid-purged=off：不输出跟GTID相关的信息； – result-file：指定了输出文件的路径，其中client表示生成的文件是在客户端机器上的。 - skip-extended-insert：将每行数据输出成一条SQL语句 使用mysql -h$host -P$port -u$user $db -e \u0026quot;source /client_tmp/ t.sql\u0026quot;   导成CSV文件，在select语句后面加上into outfile `filepath` ，使用load data infile `filepath/filename` into table $db.$table，该语句也会被传到备库，如果备库没有该文件就会报错，因此在执行完该语句后，还要再执行load data local infile `filepath/filename` into table $db.$table，另外，该语句不会导出表结构，表结构需要另外导出\n  读写分离 主数据库负责写，从数据库负责读，从而缓解锁的争用、节约系统开销，提高并发量\n读写分离常用代理方式来实现，应用层不需要感知后端的MySQL集群部署结构，直接访问代理层，代理层接收应用层传来的读写请求，然后决定转发到哪个MySQL\n由于主从库之间可能发生主备延迟，导致在查从库的结果会慢于主库，解决方法：\n 将查询请求分类，对查询结果严格的请求直接发到主库上 查询前先进行sleep操作（性能不好） 等主库位点方案：在从库使用命令 select master_post_wait(file, pos[ , timeout])，参数file和pos指主库上的文件名和位置，timeout表示等待时间，正常的返回结果是一个正整数M，表示从命令开始执行，到应用完file和pos表示的bin log位置，执行了多少事务，即如果M \u0026gt;=0 ，表示从库已与主库同步，可以接受查询，如果等待超过了timeout时间，就去查主库 GTID方案：与等主库位点方案类似，都是要知道命令执行过程中事务执行的数量，使用命令 select wait_for_executed_gtid_set(gtid_set, 1)，参数gtid_set表示从库执行的事务是否包含该gtid_set，包含返回0，说明主从已同步过，可以执行查询，超时返回1，超时则查主库。该方案的难点在于gtid_set的获取，需要修改MySQL代码，让其在事务提交后返回gtid值  分区表\n  建立分区表语句，demo是以范围做分区的，也可以使用hash分区、list分区\n1 2 3 4 5 6 7 8  CREATETABLE`t`(`ftime`datetimeNOTNULL,`c`int(11)DEFAULTNULL,KEY(`ftime`))ENGINE=InnoDBDEFAULTCHARSET=latin1PARTITIONBYRANGE(YEAR(ftime))(PARTITIONp_2017VALUESLESSTHAN(2017)ENGINE=InnoDB,PARTITIONp_2018VALUESLESSTHAN(2018)ENGINE=InnoDB,PARTITIONp_2019VALUESLESSTHAN(2019)ENGINE=InnoDB,PARTITIONp_othersVALUESLESSTHANMAXVALUEENGINE=InnoDB);    分区表会按分区存储在对应的文件里，第一次访问这张表时，MySQL会对所有分区表的文件进行打开操作，但是打开文件的个数是有限制的，如果分区表太多，会导致SQL语句无法执行\n  分区表对于server层来说是一张表，DDL操作时会对所有分区表上锁，导致后面落到具体分区表的语句阻塞\n  分区表对于引擎层来讲是多张表，因此在进行一些DML语句时，只会在对应分区表加间隙锁，不会影响其他分区表\n  alter table $tableName drop partition xxx用于删除分区表，与drop 整张普通表类似，但是速度快，因为只删除了部分数据\n  在跨分区查询数据时，会比普通表慢\n  MySQL主从复制\nMySQL健康状态检测 当MySQL的查询并发数满了之后，会导致不可用，后面的操作都会被阻塞\n检测方法  查询检测：在系统库里建一张表，比如叫health_check，里面只放一个数据，然后定期执行 select * from mysql.health_check; 判断执行的SQL语句是否不可用，当语句超时则表示不可用 更新判断：由于当磁盘满了之后，MySQL仍然可读，但是bin log却写不进去，导致更新语句和事务 commit会被阻塞，在上面表的基础上，增加一个timestamp字段，每台MySQL的serverId作为主键插入表中，MySQL每次执行把当前时间更新到对应的行上，update mysql.health_check where server_id=xx set t_modified = now(); 主备都需要开启检测。当语句执行超时时表示不可用  由于每个执行请求都有可能获得IO资源，所以有时检测请求执行成功了，但是此时系统资源即将被耗尽了，已经可以进行主备切换了，但是仍然要在下次检测才能知道，因此需要判断多每次IO请求的时间，通常是检测(select)performance_schema表的信息\n其他 自增主键  自增id不一定是连续的，可能会产生空洞；比如 插入操作出现唯一键冲突，自增值也会+1；事务回滚时，自增值不会回滚； MyISAM自增id的下一个值是存在表结构里的，InnoDB是放内存的，在MySQL 8.0以前，自增值并不会持久化到磁盘，每次重启后，自增值会被清空，在第一次读表的时候会把最大id给读出来+1，达到恢复原来的自增值；8.0后是记录在redo log里，重启后通过持久化的值来恢复 自增的两个重要参数auto_increment_offset和auto_increment_increment都是系统参数，默认值为1 设置 innodb_autoinc_lock_mode，=1时，普通insert语句会申请完之后释放，批量insert时会等到所有批量insert的SQL都结束的时候才释放锁，原因是如果bin log不是row，备库在复制时产生的行的id可能于主库的不一致问题；=2时，自增id锁每次获取完就会释放。如果选择=1，在批量insert时性能就会很差，MySQL的优化是让insert的语句不使用连续的自增id，不过这样就会让自增id不连续了。所以一般的操作是选择=2，bin log=row 理论上自增id是无限的，但是因为字段的类型已经限定了最大的位数，比如如果id使用unsigned int是4个字节，上限就是2^32 -1，当达到上限后，自增值不变，就会导致报重复主键的错，所以如果表的上限需要比较大，需要设置成unsigned bigint 如果没有明确设置主键，innoDB会默认给一个row_id，虽然实际上是一个unsigned bigint，但是只用到了6个字节，所以长度是0 ~ 2^48-1，子增值达到上限后的下一个值是0，此时在insert会覆盖原有的行 当产生唯一键冲突时，除了会报错，还会加锁(主键+行锁，普通索引+间隙锁)，然后在回滚时才释放，当出现唯一键冲突时，如果有多个事务同时插入，容易造成死锁(双方都在等待对方的间隙锁释放导致) insert into ... on duplicate key update ... ，表示插入一行数据，如果出现唯一键冲突，就执行后面的update语句，该更新语句只会修改跟第一个索引冲突的行  Xid   Xid是 server层维护，表示一个事务id，存在bin log中，可以作为bin log和redo log中同一事务的关联id，让innoDB事务和server层做关联。\n  由全局变量global_query_id赋给Query_id，Query_id+1后赋值给Xid，作为事务开始的第一条语句的id。\n  由于global_query_id是内存变量，MySQL重启后会被清零，但是重启后会生成新的bin log，所以同一bin log不会出现两个相同的Xid\n  global_query_id达到上限2^64 - 1后，会从0开始计数，所以同一数据库里可能同时存在相同的Xid\n  trx_id  trx_id不同于Xid，trx_id是由InnoDB维护，用在事务可见性方面的，比如MVCC，视图一致性 普通只读语句不分配trx_id（而是临时算的，会比较大，主要是为了区分读写语句），读写语句才会分配 trx_id，由max_trx_id + 2(至少，比如update，实际上要先当前读在update的，所以是+2) max_trx_id会持久化，重启不会变0，上限是2^48-1，然后从0开始计数  thread_id  系统保存全局变量thread_id_counter，每新键一个连接，thread_id_counter + 1后赋给thread_id，作为线程id 上限是2^32-1，达到上限后重置为0  JDBC SUN的 JDBC 是一套接口，而实现是各个数据库厂商的驱动包，因此使用了桥接模式\nDriverManager注册驱动包，com.mysql.jdbc.Driver类中的static块会创建驱动实例，因此只需要把驱动字节码加载到JVM里即可，Class.forName(“com.mysql.jdbc.Driver”);\nConnection conn = DriverManager.getConnection(url, username, password)获取连接\nStatement stmt = con.createStatement(); 之后使用stmt的方法执行SQL语句即可，返回ResultSet\nResultSet下标从1开始\n使用Connection类的setAutoCommit(false) 方法来实现事务，以这个开始，Connection类的commit()方法提交，Connection类的rollback()方法回滚\n最后关闭ResultSet、Statement和Connection\n数据库连接池 链表实现，在使用连接对象之前，先创建好一定数量的连接对象，以链表的形式连接，从端首取，用完回到段尾。\n当池子中没有连接对象可取时，就让其先等待，如果等待超时还没有回获取到连接对象，就新建一个连接对象让其使用，用完后销毁该创建的对象\n连接池负责管理、监控和维护这些连接对象\n连接池单例\n连接池需要保证线程安全\n参考 MySQL ACID及四种隔离级别的解释\nCyC2018/CS-Notes/MySQL\nInnodb中的事务隔离级别和锁的关系\n极客时间 - MySQL实战45讲\n后记\n极客时间 - MySQL实战45讲真的是质量很高的讲MySQL的课程，非常推荐\n","date":"2020-07-09T00:00:00Z","permalink":"http://nixum.cc/p/mysql/","title":"MySQL"},{"content":"[TOC]\n从浏览器输入URL之后都发生了什么 浏览器输入URL，按回车，\n 浏览器根据输入内容，匹配对应的URL和关键词，校验URL的合法性，补全URL，使其符合通用URI的语法。 请求发出前，如果当前的URL被访问过，会先进入缓存中查询是否有要请求的文件，有则直接返回；如果没有，则跳过缓存，进入网络操作。缓存会存在于路由缓存、DNS缓存、浏览器缓存、ServiceWorker、MemoryCache、DiskCache、PushCache、系统缓存等。 从URL中解析出域名，依次经过浏览器缓存、系统缓存、hosts 文件、路由器缓存、 递归搜索DNS服务器，找到对应的IP地址。 应用层程序准备好数据后，委托给操作系统，复制应用层数据到内核的内存空间中，交给网络协议栈（将其打包为tcp包(传输层)，帧(数据链路层)，并数据其从内核拷贝到网卡，后续由网卡负责数据的发送）建立 TCP/IP 连接（三次握手具体过程）。 HTTP 请求经过路由器的转发，通过服务器（CDN、反向代理之类的）的防火墙，该 HTTP 请求到达了服务器 服务器处理该 HTTP 请求，返回一个 HTML 文件 浏览器解析该 HTML 文件，解析HTML文件后，构建dom树 -》构建render树 -》布局render树 -》绘制 render树，自上而下加载，边加载边解析渲染，显示在浏览器端，对于图片音频等则是异步加载  本质上是OSI七层模型 + 相应协议、组件实现\nHTTP方法 菜鸟HTTP教程/HTTP请求方法\nGet和Post的区别   语义上的区别，Get一般表示查询、获取，Post是更新\n  Get具有幂等性，Post没有\n  参数传递方面，Get一般参数接在Url上，对外暴露，有长度限制（1024个字节即256个字符），只接收ASCII字符，需要进行url编码\nPost参数放在request body里，支持多种编码\n  GET请求会被浏览器主动cache，而POST不会，除非手动设置\n  GET产生的URL地址可以加入书签，而POST不可以\n  GET请求参数会被完整保留在浏览器历史记录里，而POST中的参数不会被保留\n  GET在浏览器回退时是无害的，而POST会再次提交请求\n  其实本质都是一种协议的规范，规定参数的存放位置，参数长度大小等，当然也可以反着来，只要服务器能够理解即可\n幂等性：同样的请求被执行一次与连续执行多次的效果是一样的，服务器的状态也是一样的，每次返回的结果一样，不产生副作用；\n根据语义，简单的把get看成查询，只要服务器的数据没变，每次查询得到的结果是一样的，而把post看成添加，每次post请求都会创建新资源，服务器状态改变\n具有幂等性的方法：GET、HEAD、OPTIONS、DELETE、PUT\n没有幂等性的方法：POST\n安全性：安全的 HTTP 方法不会改变服务器状态，也就是说它只是可读的。\n常见状态码 参考HTTP状态码\n 301：永久移动请求的网页已永久移动到新位置，即永久重定向；返回信息会包括新的URI，浏览器会自动定向到新URI。今后任何新的请求都应使用新的URI代替，新的URI会放在响应header的Location字段中；搜索引擎会抓取新内容同时也将旧地址修改为新地址 302：临时移动请求的网页暂时跳转到其他页面，即暂时重定向；旧地址的资源还在，只是重定向到临时的新地址中，对SEO有利，搜索引擎会抓取新内容保存旧地址 502：Bad Gateway，作为网关或者代理工作的服务器尝试执行请求时，从上游服务器接收到无效的响应 504：Gateway Time-out，充当网关或代理的服务器，未及时从上游服务器收到请求  HTTP头 通用头 cache-control 设置http缓存规则，控制缓存开关，缓存策略，缓存的有效时间等，请求头和响应头有不同的枚举值\nkeep-alive 保持长连接，连接复用，避免频繁建立连接带来的性能损耗。HTTP1.0默认是关闭的，HTTP1.1默认是开启的。对应的key是Connection。可以设置参数Keep-Alive: max=5, timeout=120表示一次keep-alive可以发送的请求次数是5，连接保持时间是120s。\n与TCP的keep-alive不同：\n  TCP 的 KeepAlive 是由操作系统内核来控制，通过 keep-alive 报文来防止 TCP 连接被对端、防火墙或其他中间设备意外中断，和上层应用没有任何关系，只负责维护单个 TCP 连接的状态，其上层应用可以复用该 TCP 长连接，也可以关闭该 TCP 长连接。 HTTP 的 KeepAlive 机制则是和自己的业务密切相关的，浏览器通过头部告知服务器要复用这个 TCP 连接，请不要随意关闭。只有到了 keepalive 头部规定的 timeout 才会关闭该 TCP 连接，不过这具体依赖应用服务器，应用服务器也可以根据自己的设置在响应后主动关闭这个 TCP 连接，只要在响应的时候携带 Connection: Close 告知对方   HTTPs HTTPS = HTTP + SSL，SSL是介于HTTP之下TCP之上的协议层，提供 加密明文，验证身份，保证报文完整 的保障。TLS是升级版的SSL，作用类似，比SSL多了些其他功能\nHTTP 先和 SSL（Secure Sockets Layer）通信，再由 SSL 和 TCP 通信。一般情况下，TLS需要先经过TCP三次握手，建立可靠连接之后，才能做TLS握手的事。\n  加密\n使用 对称加密 加密 报文(私钥加密私钥解密)，使用 非对称加密 (公钥加密私钥解密)加密 对称加密的密钥 保证该密钥的传输安全\n  验证身份\n通过第三方（CA）发布证书，对通信方进行认证\n服务器的运营人员向 CA 提出公开密钥的申请，CA 在判明提出申请者的身份之后，会对已申请的公开密钥做数字签名，然后分配这个已签名的公开密钥，并将该公开密钥放入公开密钥证书后绑定在一起。\n进行 HTTPs 通信时，服务器会把证书发送给客户端。客户端取得其中的公开密钥之后，先使用数字签名进行验证，如果验证通过，就可以开始通信了，通信时使用上述的加密机制保护报文\n证书信任的方式：操作系统和浏览器内置；CA颁发；手动导入证书\n  保护报文完整\nSSL 提供报文摘要功能（即签名）结合加密和认证来进行完整性保护\n  流程：\n在HTTP的基础上\n  客户端访问服务端的网页，首先经过浏览器内置的受信任的CA机构列表，查看该服务器是否向CA机构提供了证书\n  如果服务器证书中的信息与当前正在访问的网站（域名等）一致，那么浏览器就认为服务端是可信的，并从服务器证书中取得服务器公钥\n  建立会话密钥，客户端通过服务器公钥加密会话密钥发送给服务端，服务端用自己私钥解密得到会话密钥，用于接收和发送数据，之后传输的http数据都是经过加密的。\n  总结：非对称加密的手段传递密钥，然后用密钥进行对称加密传递数据，CA会保存服务端提供的非对称加密的公钥并签名\nHTTP 2.0 HTTP2.0建立在Https协议的基础上，支持二进制流而不是文本，支持多路复用而不是有序阻塞，支持数据压缩减少包大小，支持server push等特性，低延时，高性能。\n二进制分层帧\n 帧：包含 类型Type、长度Length、标记Flags、流标识Stream和载荷frame payload 消息：一个完整的请求或响应，由一个或多个帧组成   流是连接中的一个虚拟信道，可以承载双向消息传输。每个流有唯一整数标识符。为了防止两端流ID冲突，客户端发起的流具有奇数ID，服务器端发起的流具有偶数ID。\n流标识是描述二进制frame的格式，使得每个frame能够基于http2发送，与流标识联系的是一个流，每个流是一个逻辑联系，一个独立的双向的frame存在于客户端和服务器端之间的http2连接中。一个http2连接上可包含多个并发打开的流，这个并发流的数量能够由客户端设置。\n在二进制分帧层上，http2.0会将所有传输信息分割为更小的消息和帧，并对它们采用二进制格式的编码将其封装，新增的二进制分帧层同时也能够保证http的各种动词，方法，首部都不受影响，兼容上一代http标准。其中，http1.X中的首部信息header封装到Headers帧中，而request body将被封装到Data帧中。\n 多路复用\n连接是持久的，客户端和服务器之间只需要一个连接，每个数据流可以拆分成很多不依赖的帧，这些帧可以乱序发送，也可以分优先级，多个流的数据包能够混合在一起通过同样的连接传输，服务端在根据不同帧首部的流标识进行区分和组装。\n头部压缩\n http1.x的头带有大量信息，而且每次都要重复发送。http/2使用encoder来减少需要传输的header大小，通讯双方各自缓存一份头部字段表，既避免了重复header的传输，又减小了需要传输的大小。\n 各自缓存之后，之后发送的请求如果不包含首部，就会自动使用之前请求发送的首部，如果首部发生变化，则只需将变化的部分加入到header帧中，改变的部分会加入到头部字段表中，首部表在HTTP2.0的连接存续期内始终存在，由客户端喝服务端共同渐进式更新。\n请求优先级\n将HTTP消息分为很多独立帧之后，就可以通过优化这些帧的交错喝传输顺序进一步优化性能，服务端也可以根据流的优先级，优先将最高优先级的帧发送给客户端。\n服务端推送\n服务端可以对一个客户端请求发送多个响应，而无需客户端明确地请求，省去客户端重复请求的步骤\n基本    OSI七层模型 对应网络协议 作用     应用层 HTTP、TFTP、FTP、NFS、SMTP、Telnet 应用程序间通信的网络协议   表示层 Rlogin、SNMP、Gopher 数据格式化、加密、解密   会话层 SMTP、DNS 建立、维护、管理会话连接   传输层 TCP、UDP 建立、维护、管理端到端连接   网络层 IP、ICMP、ARP、RARP、AKP、UUCP IP寻址和路由选择   数据链路层 FDDI、Ethernet、Arpanet、PDN、SLIP、PPP 控制网络层与物理层间的通信   物理层 IEEE 802.1A、IEEE 802.2到802.11 比特流传输    数据链路层：\n 数据包叫Frame，“帧”； 由两部分组成：标头和数据，标头标明数据发送者、接收者、数据类型； 用MAC地址定位数据包路径； 相关设备是交换机；  网络层：\n 数据包叫packet，“包”； IPv4：32个二进制，4字节*8位；IPv6：1同一子网28个二进制，8字节*16位； 子网掩码与IP的and运算判断是否为同一子网下； 路由：把数据从原地址转发到目标地址，同一局域网内，通过广播的方式找到，不同局域网内，原主机先将包根据网关添加路由器/主机地址，通过交换机的广播方式发给目标主机，原主机将数据包传输给目标主机，再由目标主机根据MAC广播交给对应目标 ARP协议：IP与MAC地址的映射，仅限IPv4，IPv6使用 Neighbor Discovery Protocol替代； 相关设备是路由器，网关  传输层：\n  数据包叫segment，“段”；\n  Socket、UDP、TCP见下\n  应用层使用TCP传输数据时，会先将数据打到TCP的Segment中，然后TCP的Segment会打到IP的Packet中，然后再打到以太网Ethernet的Frame中，传输到目标主机后，再一层层解析，往上传递。\nTCP（Transmission Control Protocol 传输控制协议） 特点  面向连接的，提供可靠交付，丢包重传，有状态服务 有流量控制，拥塞控制 提供全双工通信 面向字节流（把应用层传下来的报文看成字节流，把字节流组织成大小不等的数据块） 头部20字节 每一条 TCP 连接只能是点对点的（一对一）。  应用场景  Http、FTP、SSH、SMTP  连接与关闭连接 SYN：发起一个新连接\nACK：回复，确认序号有效\nFIN：释放一个连接\nack：回复，确认序号，=发送方seq+1\nRST：复位标志，需要重新连接\nFIN：结束连接\n三次握手  三次握手 \n  第一次握手：Client将标志位SYN置为1，随机产生一个值seq=J，并将该数据包发送给Server，Client进入SYN_SENT状态，等待Server确认。 每一端先发出去的都会由SYN，收到之后会发出ACK。\n如果client发送失败，会周期性进行超时重传，直到收到server的确认。\n  第二次握手：Server收到数据包后由标志位SYN=1知道Client请求建立连接，Server将标志位SYN和ACK都置为1，ack=J+1，随机产生一个值seq=K，并将该数据包发送给Client以确认连接请求，Server进入SYN_RCVD状态。\n如果server发送响应失败，会周期性进行超时重传，直到收到client的确认。\n  第三次握手：Client收到确认后，检查ack是否为J+1，ACK是否为1，如果正确则将标志位ACK置为1，ack=K+1，并将该数据包发送给Server，Server检查ack是否为K+1，ACK是否为1，如果正确则连接建立成功，Client和Server进入ESTABLISHED状态，可以开始传输数据了。\nclient第三次挥手，此时client会认为自己已经established，server还未收到，此时仍然为active状态：如果server一直没收到连接请求，server会重复第二次挥手，直到自己收到第三次挥手的请求，此时才是established；如果此时client发送了data数据且加上了ACK，server也会切换为established；如果server有数据发送却发送不出，也会重复第二次挥手。\n  三次握手建立连接后，还有一件重要的事情是确定TCP包的序号，每个连接都要有不同的序号，序号的起始号随时间变化，每4ms加一，如果有重复，需要4个多小时后才会出现，因为IP包头里有TTL(生存时间)，超过4小时早过时了。\n之所以要确认包的序号是因为有可能出现client与server建立连接并进行通信后断开重连，如果每次连接没有新的起始序号，会导致server分辨不出收到的包是这次连接的还是上次连接的。这个号会作为以后的数据通信序号，以保证应用层接收到的数据不会因网络传输问题而乱序。\n四次挥手  四次挥手 \n 第一次挥手：Client发送一个FIN、一个seq，用来关闭Client到Server的数据传送，Client进入FIN_WAIT_1状态。 第二次挥手：Server收到FIN后，发送一个ACK给Client，确认序号ack为收到seq+1（与SYN相同，一个FIN占用一个序号），Server进入CLOSE_WAIT状态。 第三次挥手：Server发送一个FIN、一个ACK、ack为上面的seq+1、一个seq，用来关闭Server到Client的数据传送，Server进入LAST_ACK状态。 第四次挥手：Client收到FIN后，Client进入TIME_WAIT状态（等待时间设为2MSL，即报文最大生存时间），接着发送一个ACK给Server，确认序号为收到序号+1，Server进入CLOSED状态，完成四次挥手。  由于TCP连接时全双工的，因此，每个方向都必须要单独进行关闭。这一原则是当一方完成数据发送任务后，发送一个FIN来终止这一方向的连接，收到一个FIN只是意味着这一方向上没有数据流动了，即不会再收到数据了，但是在这个TCP连接上仍然能够发送数据，直到这一方向也发送了FIN。首先进行关闭的一方将执行主动关闭，而另一方则执行被动关闭。\n为什么不能用两次握手进行连接？\n一个重要的原因，是双方需要确认好起始的序列号，双方的初始序列号必须保持一致，来保证连接后的可靠传输，即双方都需要确认对方收到了自己的序列号。\n如果两次握手确定连接，client发送连接请求给server，server接收后发送响应给client，此时连接建立，client可以正常的发送和接收，但是server并不知道自己发送的请求client有没有收到，此时无法确认自己的序列号，那它会认为没有确立连接，一直等待client的确认信号，忽略其他数据，server发出的数据被忽略，则会一直发送。\n另外，还会有一个问题，如果是两次握手建立连接，如果client在重复发送连接请求给server，server收到连接请求，响应回去，建立连接后，因为序列号问题，那些重复的连接请求包到达了server，此时server端也无法分辨是数据包还是连接包。\n另外，三次握手是因为第二次握手的时候，server收到client的请求和自己的请求一次性响应回了client，也可以把这一步拆出来，变成四次握手也是可以的。四次挥手的第二和三次不能合并是因为此前连接已建立，贸然关闭会导致部分报文没有接受完成。\n为什么建立连接是三次握手，而关闭连接却是四次挥手呢？\n  三次握手：发送一次信息就是一次握手，第一次握手确定Client可以发送消息给Server，第二次握手确定Server可以收到Client的信息，并且发送信息给Client，第三次握手Client确定可以收到Server的信息，三次握手是为了防止失效的连接请求到达服务器，让服务器错误打开连接。\n  四次挥手：第一次挥手Client发送消息确定Client想关闭连接，第二次挥手Server发送消息确定Client可以关闭连接，第三次挥手Server发送消息确定Server想关闭连接，第四次挥手Client发送消息确定Server可以关闭连接，少了哪一次都可能导致没有完全关闭，造成一方可发送或者接受。\n  之所以要四次，是因为server收到Fin后，不能同时将ACK确认信号和FIN，有可能此时client有一些报文还没收完，如果client没有收到，server才有机会重发，所以第二、三次挥手不能合并。\n关闭连接时，当收到对方的FIN报文时，仅仅表示对方不再发送数据了但是还能接收数据，己方也未必全部数据都发送给对方了，所以己方可以立即close，也可以发送一些数据给对方后，再发送FIN报文给对方来表示同意现在关闭连接，因此，己方ACK和FIN一般都会分开发送。\n挥手时，为什么client在发送最后一个ACK后进入Time-wait状态，为什么Time-wait状态必须等待2MSL的时间？\n 主要是为了保证client发送的最后一个ACK报文能到达server。因为这个ACK报文可能丢失，此时server接收不到，那他就会重新发送FIN+ACK报文，client就能在这2MSL内收到这个重传的报文，再次进入2MSL等待时间，发送ACK给server，直到两者都进入closed状态。如果没有2MSL等待时间，而是client发送完报文直接关闭，就会出现server无法接收而导致无法进入closed状态。 如果server超过了2MSL时间依然没收到client的ACK，会再次重发，但此时client会发送RST标志，表示异常关闭连接 client在发送完最后一个ACK报文段后，在经过2MSL，可以使本连接持续的时间内所产生的所有报文都从网络消失，用这个时间让这个连接不会和后面的连接混在一起，使得下一个新的连接不会出现旧的连接请求报文。  连接过程中超时怎么办？\nservice端接收到client发的SYN后回了SYN-ACK后，client掉线，server没有收到client响应回来的ACK，此时连接处于一个中间状态，此时，server端如果一定时间内没有收到client的ACK，则会重试发送SYN-ACK，Linux下重试次数是5次，以2的指数增长，如果5次都超时，总共要等等63s，TCP才会断开连接。\n建立连接后出故障了怎么办？\n  TCP设有一个保活计时器，每收到一次请求都会复位这个计时器，如果规定时间(2小时)内没收到，则发送探测报文测试对方是否出现故障，连续10次/75分钟，仍没反应，说明对方故障\n  IP头有一个TTL时间，配合序号可判断该数据包是否过期\n  包头  TCP包头 \n端口号：用于找到对应的应用，TCP包是没有IP地址的，因为IP数据是网络层的事，所以只会有端口号；\n序号：让包能顺序发送和接收，解决乱序问题。三次握手除了确立双方建立连接，还有一件事是确立双方包的序列号，每一次连接都要有不同的序列号用于区别，序列号的起始通常是随时间，如果每次连接的序列号相同，可能会导致前一次连接的包发送到了下一次连接里；序号的增加和传输的字节数相关；\n确认序列号：发出去的数据包时进行确认的标记，解决丢包问题；\n状态位：客户端和服务端连接的状态，即包的类型，操控TCP的状态机；\n窗口大小：解决流量控制问题，标识自己当前的处理能力；\n如何实现靠谱协议  TCP协议规定在建立连接后，会确定包的序号的起始ID，按照ID一个个发送，对于发送的包会进行应答，应答是累计应答，应答某个ID的包就表示在这个ID之前的包都收到了 在发送端和接收端会分别使用缓存来保存这些包的记录，一般分为四个：1. 已发送并确认的、2. 已发送未确认的、3. 没有发送但准备发送的、4. 没有发送且暂时不会发送的，滑动窗口就是处理第二、三部分的数据包  滑动窗口 窗口大小即自己的数据接收缓冲池的大小\n由发送方和接收方在三次握手阶段，互相通知自己的最大可接收的字节数。\n当发送方窗口左部字节已发送且收到通知，窗口右滑直到左部第一个字节不是已发送并且已确认的状态，接受方窗口移动同理\n接收窗口只会对窗口内最后一个按序到达的字节进行确认，确认之后表示之前的所有字节都接收到了\n在处理过程中，当接收缓冲池的大小发生变化时，要给对方发送更新窗口大小的通知。\n粘包问题：\n发送方发送的若干包数据到接收方接收时粘成一包，从接收缓冲区看，后一包数据的头紧接着前一包数据的尾。\n只有TCP会（通过窗口大小来接收数据，窗口大小又是动态的），而UDP因为有消息边界（头部有规定报文的大小），所以不会\n  产生的原因\n 发送端粘包：发送端等到缓冲区满了才发送出去，造成粘包  有时为了提高发送数据的效率，服务端会把多个数据块合并成一个大的数据块后封包发送，单由于面向流的通信是无消息保护边界的，接收端就很难分辨出完成的数据包\n 接受端粘包：接受端不及时接收缓冲区的包，造成多个包接收    解决方法\n 发送固定长度的消息、消息尺寸和消息一块发送、特殊标记标记消息区间 通信双方规定好协议 + 编解码器（比如规定定长的请求头、请求体），按协议的格式进行解析 程序控制发送和接收频率    TCP粘包问题分析和解决（全）\n丢包问题：\n发送方按顺序发送一系列的包，接收方接收这些包是中间一部分包没收到\n  产生原因\n 中间这些包经过其他链路导致延时接收方还没收到、 接收方收到且发送了ACK给发送方，但是发送方没有收到ACK、 数据链路有点问题，数据包真的丢了、 因为顺序发送和接受问题，接收方后面的包收到了，但是前面的包还没收到，导致后面的包收到了也不能发送ACK给发送端    解决方法\n  超时重传，有自适应重传算法来决定超时时间，另外是每次重试的时间间隔会加倍，超过两次认为网络环境查，取消重传。\n  还有一种快速重传机制，因为包是有顺序的，所以客户端可以检测出缺失的包，然后发送对应的冗余ACK给服务端，通过在服务端设置收到规定通过发送/接收冗余的ACK包的次数，如果达到了规定次数就指定需要重传的包，不用等超时时间。so 快速重传主要是解决超时问题。\n比如：接收端中间漏收了Seq2，后面又接收到了Seq3、4、5，那就会在接收时重复发送ACK2，发送端收到重复的ACK后就会重新发送Seq2了\n \n在Linux中，还有一种SACK机制，它会在TCP头里加一个SACK的东西，ACK还是基于快速重传的ACK，只是SACK会把接收端收到的所有包的序列，都反馈给发送端，发送端根据遗漏的ACK序号，进行重传。\n另外，还有个D-SACK，使用SACK来告诉发送方有哪些数据被重复接收了，其使用SACK的第一个段来做标志，如果SAKC的第一个段范围被ACK覆盖，就是D-SACK，如果SACK的第一个段的范围被SACK的第二个段覆盖，就是D-SACK。引入D-SACK可以让发送方知道是发出去的包丢了，还是回来的ACK包丢了；还是发送方的超时太短，导致重传；还是先发出去的包后到的情况，还是数据包被复制了。\n \n    超时重传中，timeout（RTO）的设置尤为关键，设置过长，重发就慢，效率低性能差；设置过短，就会导致没有丢就重发，重发过快，就会增加网络拥塞，导致更多的超时，从而又导致更多的重发，因此无法设置成一个写死的值，而是通过RTT算法动态调整。\n流量控制 TCP需要解决可靠传输和包的乱序问题，就需要知道网络实际的数据处理带宽或数据处理速度，才不会引起网络拥塞，导致丢包，so需要做流量控制。在TCP的包头中通过Window字段控制，这个字段是接收端每次ACK时会告诉发送端自己还有多少缓冲区可以接收数据，于是发送端就可以根据这个接收端的处理能力来发送数据，而不会导致接收端处理不过来。滑动窗口需要依赖发送端和接收端。\n实现滑动窗口，发送端和接收端在进行数据交互时，会商定滑动窗口的大小，在对于包的确认时会携带一个窗口的大小，通过该窗口大小来实现流量控制（或者时不时发送一个窗口探测的数据段来确认双方的窗口大小），当发送方和接收方的滑动窗口大小为0时，发送方会定时发送窗口探测数据包，来更新窗口大小，即Zero Window。\n  Zero Window：当滑动窗口的大小变成0，意味着发送端不发数据了，当接收方的滑动窗口可以更新时，会使用Zero Window Probe来通知发送端可以发送数据，流程是当发送端在窗口变成0后，会发ZWP包给接收方，让接收方重新确定滑动窗口的大小，这个值会被设置成3次，每次大约30-60s，如果3次过后还是0，可能就会端口连接了。\n  Silly Window Syndrome：如果接收方太忙，来不及取走滑动窗口里的数据，就会导致发送方可发送的数据越来越小，如果每次发送的数据太小，带宽没有占用，实际上是很亏的，这种现象就叫Silly Window Syndrome。\n解决方法有多种，一种是如果这个问题是接收端引起的，当收到的数据导致滑动窗口的大小小于某个值，就直接ack(0)回发送端，把滑动窗口关闭，等接收方的滑动窗口大于某个值是，才把滑动窗口打开，让发送端发数据过来；另一种是如果这个问题是发送端引起的，就延时处理，攒多点数据一块发。\n  拥塞控制  产生原因：对资源的需求超过了可用的资源，网络吞吐量下降，如果网络出现拥塞，数据包将会丢失或延迟到达，发送方以为发送失败又会继续重传，从而导致网络拥塞程度更高。  TCP通过数据包发送和确认的往返时间，丢包率来判断是否拥塞，使用滑动窗口来进行拥塞控制，控制发送方的发送速率，避免包丢失和超时重传。\n  解决方法：\n 慢开始 + 拥塞避免，一开始慢慢的发送，逐渐增大发送速率（线性上升直至网络最佳值），再慢下来依次重复 快重传 + 快恢复，当拥塞发生时，减少超时重传的使用，而是使用快速重传机制  为了实现上面两种机制，TCP使用BBR拥塞算法，来达到高带宽和低延时的平衡\n  流量控制和拥塞控制的区别：\n流量控制是为了让接收方能来得及接收，而拥塞控制是为了降低整个网络的拥塞程度，此时可以把网络链路想象成水管。\n参考：https://coolshell.cn/articles/11564.html、https://coolshell.cn/articles/11609.html\nUDP（User Datagram Protocol 用户数据报协议） 特点  不可靠的、无连接的，尽最大可能交付，只负责发送数据，无状态服务 没有拥塞控制，流量控制 面向报文（对于应用程序传下来的报文不合并也不拆分，只是添加 UDP 首部），一个一个地发，一个一个地收 头部只有8字节 支持一对一、一对多、多对一和多对多的交互通信。  应用场景  针对网络资源少，对丢包不敏感的应用，比如应用层的DHCP，在获取IP地址和子网掩码的使用 需要广播的应用，比如DHCP、VXLAN 需要处理速度快，时延低，容忍丢包、网络拥塞的应用，比如直播、视频，允许丢包，虽然丢包会导致丢帧，但影响不会很大；实时游戏、物联网终端的数据收集，其实大多数会基于UDP做一定的改进，减少UDP劣势的影响  包头  UDP包头 \nUDP包头比较简单，两端通信时，通过网络层里的IP，将数据包发送给对应的机器，IP头里有个8位协议，表明该数据是UDP协议的，解析到传输层，通过UDP包头提供端口号，让目标机器监听该端口号的应用程序进行处理\n使用场景  网络环境较好，如内网应用，或者对于丢包不敏感的应用 需要广播或多播 需要处理速度快，时延低，容忍丢包和网络拥塞  UDP如何实现TCP 建立连接，是为了在客户端和服务端维护连接，而建立一定的数据结构来维护双方交互的状态，通过这样的数据结构来保证面向连接的特性\nUDP属于传输层，协议已经定死了，要实现TCP的功能只能在应用层实现，模拟TCP有的那些功能：确认机制、重传机制、窗口确认机制、流量控制、拥塞控制等那些功能。\nUDP实现可靠性，可以简单理解成，将TCP的三次握手发送数据全程用UDP去发送，模拟TCP的包头\n 1、seq/ack机制，确保数据发送到对端 2、数据包 + 编号，确保有序性 3、添加发送和接收缓冲区，主要是用户超时重传。 4、定时任务实现超时重传机制。  发送端发送数据时，生成一个随机seq=x，然后每一片按照数据大小分配seq。数据到达接收端后接收端放入缓存，并发送一个ack=x的包，表示对方已经收到了数据。发送端收到了ack包后，删除缓冲区对应的数据。时间到后，定时任务检查是否需要重传数据\n如何实现UDP的可靠传输\n套接字Socket Socket是对TCP和UDP协议的应用\n 基于TCP协议的Socket   基于TCP的Socket \n 基于UDP协议的Socket   基于UDP的Socket \n服务端如何管理这些连接和资源？\n 父进程使用子进程来管理连接和资源，子进程在完成连接和数据通信后告诉父进程进行回收 线程池 + 连接池，每个线程管理一个socket，连接池实现socket复用 IO多路复用，一个线程维护多个socket，如Java NIO、Netty的网络模型  参考 TCP/IP参考1\nTCP/IP参考2\n极客时间 - 趣谈网络协议\nTCP 的那些事儿\n","date":"2019-09-22T00:00:00Z","permalink":"http://nixum.cc/p/%E7%BD%91%E7%BB%9C/","title":"网络"},{"content":"[TOC]\n类图 类图中的关系其实有多种版本的表示方法，这里仅总结自己常用的画法\n访问作用域   + : public\n  - : private\n  # : protocted\n  关系 1. 依赖（dependency） 依赖关系是五种关系中耦合最小的一种关系。\n依赖在代码中主要体现为类A的某个成员函数的返回值、形参、局部变量或静态方法的调用，则表示类A引用了类B。\nA \u0026mdash;-\u0026gt; B ： A use B （虚线+箭头）\n A use B \n2. 关联（Association） 在程序代码中，具有关联关系的类常常被声明为类的引用类型的成员变量。\n因为 关联 是 依赖 的更详细说明， 关联 是专门描述成员属性的关系，所以依赖中所有涉及成员属性的地方更适合使用：关联、聚合、组合\n单向关联：\nA ——————\u0026gt; B ： A has B （实心线 + 箭头）\n A has B \n3. 聚合（Aggregation） 聚合是关联的一种特殊形式，暗含整体/部分关系，但是对方却不是唯一属于自己的那种关系。 用来表示集体与个体之间的关联关系，例如班级与学生之间存在聚合关系。\nA \u0026lt;\u0026gt;—————— B : A是集体，B是个体 （实线 + 空心菱形）\n A是集体，B是个体 \n4. 组合（Composition） 组合又叫复合，用来表示个体与组成部分之间的关联关系。 在组合关系中整体对象可以控制成员对象的生命周期，一旦整体对象不存在，成员对象也不存在，整体对象和成员对象之间具有同生共死的关系。\nA \u0026lt;#\u0026gt;———— B： A是整体，B是部分 （实线线 + 实心菱形）\n A是整体，B是部分 \n5. 泛化 5.1 继承（Generalization） A ——————|\u0026gt; B : A继承了B （实心线 + 空心三角箭头），A is B\n A继承了B \n5.2. 实现（Implementation） A \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;|\u0026gt; B : A实现了接口B （虚心线 + 空心三角箭头）， A like B\n A实现了接口B \nPS：为了方便，继承和接口实现也都可以用实线加空心三角箭头\n","date":"2019-07-11T00:00:00Z","permalink":"http://nixum.cc/p/uml/","title":"UML"},{"content":"[TOC]\nJVM内存模型  JVM内存模型 \n方法区也叫永久代，持久代，非堆，不算在堆里面\n年轻代也叫新生代\n注意区别于Java内存模型\nJVM内存模型描述的是线程运行时的数据在内存的分布\nJava内存模型是多线程情况下数据的分布\n引用类型  强引用：通过new的方式创建，不会被轻易回收 软引用（SoftReference）：被软引用关联的对象只有在内存不够时才会被回收 弱引用（WeakReference）：被弱引用关联的对象一定会被回收，只能存活至下次垃圾回收发生之前 虚引用（PhantomReference）：比如将对象引用设置为null，该引用指向的对象就会被回收，相当于告知JVM可以回收该对象  软引用、弱引用、虚引用均可以搭配引用队列使用，且虚引用必须搭配引用队列使用。使用引用队列时，这些引用对象被垃圾收集器回收之后会进入引用队列，等待二次回收。引用队列一般用于与GC交互的场景，比如，垃圾回收时进行通知。\n引用计数法 为对象添加一个引用计数器，当对象增加一个引用时，计数器加 1，引用失效时，计数器减 1。引用计数为 0 的对象可被回收。\n比较轻便，效率较高，不需要STW，可以很快进行回收，但维护引用计数也有一定的成本\n但有可能出现循环引用，JVM没有使用该判断算法，可能因为编译的时候并不会检测对象是否存在循环引用？go的话会在编译期检测是否存在循环引用，但是它垃圾回收使用三色标记法，本质是标记清除\n可达性分析 以 GC Roots 为起始点进行搜索，可达的对象都是存活的，不可达的对象可被回收，不可达指的是游离在GC Root外的对象。\nGC Roots包括：\n  java虚拟机栈中引用的对象\n方法执行时，JVM会创建一个相应的栈帧进入java虚拟机栈，栈帧中包括操作数栈、局部变量表、运行时常量池的引用、方法内部产生的对象的引用，当方法执行结束后，栈帧出栈，方法内部产生的对象的引用就不存在了，此时这些对象就是不可达对象，因为无法从GC Roots找到，这些对象将在下次GC时回收。\n比如，方法内部创建一个对象A，并持有另一个对象B，对象B引用也同时被其他线程持有，然后在方法里设置对象A=null或者方法结束后，个人认为对象A会被回收，对象B不会被回收，如果是方法外有一个对象C引用了对象A，设置对象A=null或方法结束后，对象A不会被回收\n  方法区中类静态属性引用的对象、常量引用的对象\n静态属性或者静态变量，是class的属性，不属于任何实例，该属性会作为GC Roots，只要该class存在，该引用指向的对象也会一直存在，只有该class被卸载时，才会被回收。对于常量池里的字面量，当没有其他地方引用这个字面量时，也会被清除。\n  本地方法栈中Native方法引用的对象\n这部分属于其他语言写的方法所使用到的对象，道理跟上面是java虚拟机栈是类似的\n  上面两种更像是判断什么对象该被回收，至于要怎么回收，回收有什么策略，就有下面这几种了。\n复制 标记-清理 标记 - 整理 三色标记   把所有对象放到白色的集合中 从根节点开始遍历对象，遍历到的白色对象从白色集合中放到灰色集合中 遍历灰色集合对象，把灰色对象引用的白色集合的对象放入到灰色集合中，同时把遍历过的灰色集合中的对象放到黑色集合中 循环步骤3，直到灰色集合中没有对象 步骤4结束后，白色集合中的对象为不可达对象，进行回收   参考：深入理解Go-垃圾回收机制\n垃圾收集器 CMS 执行过程   初始标记(STW initial mark)：这个过程从垃圾回收的\u0026quot;根对象\u0026quot;开始，只扫描到能够和\u0026quot;根对象\u0026quot;直接关联的对象，并作标记。所以这个过程虽然暂停了整个JVM，但是很快就完成了。 并发标记(Concurrent marking)：这个阶段紧随初始标记阶段，在初始标记的基础上继续向下追溯标记。并发标记阶段，应用程序的线程和并发标记的线程并发执行，所以用户不会感受到停顿。 并发预清理(Concurrent precleaning)：并发预清理阶段仍然是并发的。在这个阶段，虚拟机查找在执行并发标记阶段新进入老年代的对象(可能会有一些对象从新生代晋升到老年代， 或者有一些对象被分配到老年代)。通过重新扫描，减少下一个阶段\u0026quot;重新标记\u0026quot;的工作，因为下一个阶段会Stop The World。 重新标记(STW remark)：这个阶段会暂停虚拟机，收集器线程扫描在CMS堆中剩余的对象。扫描从\u0026quot;跟对象\u0026quot;开始向下追溯，并处理对象关联。 并发清理(Concurrent sweeping)：清理垃圾对象，这个阶段收集器线程和应用程序线程并发执行。 并发重置(Concurrent reset)：这个阶段，重置CMS收集器的数据结构状态，等待下一次垃圾回收。   G1 执行过程   标记阶段：首先是初始标记(Initial-Mark),这个阶段也是停顿的(stop-the-word)，并且会稍带触发一次yong GC。 并发标记：这个过程在整个堆中进行，并且和应用程序并发运行。并发标记过程可能被yong GC中断。在并发标记阶段，如果发现区域对象中的所有对象都是垃圾，那个这个区域会被立即回收(图中打X)。同时，并发标记过程中，每个区域的对象活性(区域中存活对象的比例)被计算。 再标记：这个阶段是用来补充收集并发标记阶段产新的新垃圾。与之不同的是，G1中采用了更快的算法:SATB。 清理阶段：选择活性低的区域(同时考虑停顿时间)，等待下次yong GC一起收集，对应GC log: [GC pause (mixed)]，这个过程也会有停顿(STW)。 回收/完成：新的yong GC清理被计算好的区域。但是有一些区域还是可能存在垃圾对象，可能是这些区域中对象活性较高，回收不划算，也肯能是为了迎合用户设置的时间，不得不舍弃一些区域的收集。   内存分配和回收策略  1. 对象优先在 Eden 分配 大多数情况下，对象在新生代 Eden 上分配，当 Eden 空间不够时，发起 Minor GC。\n2. 大对象直接进入老年代 大对象是指需要连续内存空间的对象，最典型的大对象是那种很长的字符串以及数组。\n经常出现大对象会提前触发垃圾收集以获取足够的连续空间分配给大对象。\n-XX:PretenureSizeThreshold，大于此值的对象直接在老年代分配，避免在 Eden 和 Survivor 之间的大量内存复制。\n3. 长期存活的对象进入老年代 为对象定义年龄计数器，对象在 Eden 出生并经过 Minor GC 依然存活，将移动到 Survivor 中，年龄就增加 1 岁，增加到一定年龄则移动到老年代中。\n-XX:MaxTenuringThreshold 用来定义年龄的阈值。\n4. 动态对象年龄判定 虚拟机并不是永远要求对象的年龄必须达到 MaxTenuringThreshold 才能晋升老年代，如果在 Survivor 中相同年龄所有对象大小的总和大于 Survivor 空间的一半，则年龄大于或等于该年龄的对象可以直接进入老年代，无需等到 MaxTenuringThreshold 中要求的年龄。\n5. 空间分配担保 在发生 Minor GC 之前，虚拟机先检查老年代最大可用的连续空间是否大于新生代所有对象总空间，如果条件成立的话，那么 Minor GC 可以确认是安全的。\n如果不成立的话虚拟机会查看 HandlePromotionFailure 的值是否允许担保失败，如果允许那么就会继续检查老年代最大可用的连续空间是否大于历次晋升到老年代对象的平均大小，如果大于，将尝试着进行一次 Minor GC；如果小于，或者 HandlePromotionFailure 的值不允许冒险，那么就要进行一次 Full GC。\nFull GC 的触发条件 对于 Minor GC，其触发条件非常简单，当 Eden 空间满时，就将触发一次 Minor GC。而 Full GC 则相对复杂，有以下条件：\n1. 调用 System.gc() 只是建议虚拟机执行 Full GC，但是虚拟机不一定真正去执行。不建议使用这种方式，而是让虚拟机管理内存。\n2. 老年代空间不足 老年代空间不足的常见场景为前文所讲的大对象直接进入老年代、长期存活的对象进入老年代等。\n为了避免以上原因引起的 Full GC，应当尽量不要创建过大的对象以及数组。除此之外，可以通过 -Xmn 虚拟机参数调大新生代的大小，让对象尽量在新生代被回收掉，不进入老年代。还可以通过 -XX:MaxTenuringThreshold 调大对象进入老年代的年龄，让对象在新生代多存活一段时间。\n3. 空间分配担保失败 使用复制算法的 Minor GC 需要老年代的内存空间作担保，如果担保失败会执行一次 Full GC。具体内容请参考上面的第 5 小节。\n4. JDK 1.7 及以前的永久代空间不足 在 JDK 1.7 及以前，HotSpot 虚拟机中的方法区是用永久代实现的，永久代中存放的为一些 Class 的信息、常量、静态变量等数据。\n当系统中要加载的类、反射的类和调用的方法较多时，永久代可能会被占满，在未配置为采用 CMS GC 的情况下也会执行 Full GC。如果经过 Full GC 仍然回收不了，那么虚拟机会抛出 java.lang.OutOfMemoryError。\n为避免以上原因引起的 Full GC，可采用的方法为增大永久代空间或转为使用 CMS GC。\n5. Concurrent Mode Failure 执行 CMS GC 的过程中同时有对象要放入老年代，而此时老年代空间不足（可能是 GC 过程中浮动垃圾过多导致暂时性的空间不足），便会报 Concurrent Mode Failure 错误，并触发 Full GC。\n 为什么有了 对象达到年龄限制后晋升的机制 还要有 动态年龄判定的机制\n  如果MaxTenuringThreshold设置过大，会导致本该晋升到老年代的对象一直停留在Survivor区，直到Survivor溢出，这样对象老化机制就失效了\n  如果MaxTenuringThreshold设置过小，过早晋升的对象不能在年轻代充分回收，大量对象进入老年代，会引起频繁的Major GC\n  关于MinorGC、Major、YoungGC、FullGC的说明\n  MinorGC：清理年轻代，等同于YoungGC，叫法不同而已\n  MajorGC：清理老年代\n  FullGC：清理整个堆空间 - 包括年轻代和老年代\n  调优   JVM调优，一般是代码已经优化到了一定程度了，到了最后阶段才会进行JVM调优\n  对于Minor GC和Major GC频繁的优化，扩大Eden区，虽然可以降低Minor GC次数，但由于扫描的区域变大了，Minor GC时间可能会变长，但这点影比 当对象gc后仍然存活，需要复制到Survivor区带来的影响要小，影响Minor GC次数和时间的因素是每次GC后对象的存活数量，因此对于短期对象较多时，增加Eden区大小，同理，如果对象存活时间比较长、对象较多时，增加老年代大小\n  常用参数 只列举了常见的，参数大致分为三类：\n行为参数：改变JVM基础行为    参数 含义 说明     -XX:+ScavengeBeforeFullGC FullGC前触发一次MinorGC 默认启用   -XX:+UseGCOverheadLimit GC耗时过长，会跑OOM 默认启用   -XX:-UseConcMarkSweepGC 使用CMS低停顿垃圾收集器，减少FullGC暂停时间 默认不启用   -XX:-UseParallelGC 启用并行GC 默认不启用   -XX:-UseParallelOldGC 年轻代和老年代都使用并行垃圾收集器 默认不启用，当-XX:-UseParallelGC启用时该项自动启用   -XX:-UseSerialGC 启用串行垃圾收集器 -Client时启用，默认不启用   -XX:+UseThreadPriorities 启用本地线程优先级 默认启用    性能调优：JVM性能调优参数    参数 含义 说明，有些默认值在不同环境下是不同的     -Xms 整个堆的初始大小 默认值：物理内存的1/64   -Xmx 整个堆的最大值 默认值：物理内存的1/4   -Xmn 年轻代大小 设置该值等同于设置了-XX:NewSize和-XX:MaxNewSize，且两者相等，官方推荐是整个堆的3/8   -XX:NewSize 年轻代大小    -XX:MaxNewSize 年轻代最大值    -Xss 每个线程的栈大小 JDK1.5以后该值默认为1M   -XX:PermSize 永久代大小 默认值：物理内存的1/64   -XX:MaxPermSize 永久代最大值 默认值：物理内存的1/4   -XX:NewRatio 年轻代与老年代的比值 默认值：2，年轻代包括Eden区和两个Survivor区，老年代不包括永久代。比如=4，表示年轻代：老年代=1：4，即年轻代占整个堆的1/5   -XX:SurvivorRatio 年轻代里Eden区与两个Survivor的比值 默认值：8，表示一个Eden区：两个Survivor区的比值是8：2，一个Survivor区占整个年轻代的1/10   -XX:SoftRefLRUPolicyMSPerMB 每兆堆空闲空间中软引用的存活时间 默认值：1s   -XX:MaxTenuringThreshold 对象在年轻代的最大年龄 默认值：15，即对象在年轻代熬过了15次Minor GC，达到阈值后晋升到老年代。=0时，对象初始化直接进入老年代   -XX:PretenureSizeThreshold 对象超过多大直接在老年代中分配 默认值：0   -XX:TLABWasteTargetPercent TLAB(线程本地缓冲区)占Eden区的比例 默认值：1%   -XX:+CollectGen0First FullGC时是否先YGC 默认值：false   -XX:MinHeapFreeRatio GC后堆中空闲量占的最小比例 默认值：40   -XX:MaxHeapFreeRatio GC后堆中空闲量占的最大比例 默认值：70，GC后，如果发现空闲堆内存占到整个预估上限值的70%，则收缩预估上限值   -XX:PreBlockSpin 自旋锁自选次数，-XX:+UseSpinning需要先启用 -XX:+UseSpinning默认启用，自旋次数默认值：10次    调试参数：打开堆栈跟踪、打印、输出JVM参数，显示详细信息    参数 含义     -XX:ErrorFile=日志路径/日志文件名称.log 保存错误日志或者数据到文件中   -XX:HeapDumpPath=堆信息文件路径/文件名称.hprof 指定导出堆信息时的路径或文件名   -XX:-HeapDumpOnOutOfMemoryError 当首次遭遇OOM时导出此时堆中相关信息   -XX:-PrintGC 每次GC时打印相关信息   -XX:-PrintGCDetails 每次GC时打印详细信息   -XX:-PrintGCTimeStamps 打印每次GC的时间戳   -XX:-TraceClassLoading 跟踪类的加载信息   -XX:-TraceClassLoadingPreorder 跟踪被引用到的所有类的加载信息   -XX:-TraceClassResolution 跟踪常量池   -XX:-TraceClassUnloading 跟踪类的卸载信息    调优工具 命令行工具 jps：虚拟机进程状况工具 用来查看机器上的Java进程，如pid，启动时的JVM参数，启动时的主类、jar包全路径名称，类似ps命令\n无参数：显示进程ID和类名称 -q：只输出进程ID -m：输出传入 main 方法的参数，即main方法的String[] args -l：输出完全的包名，应用主类名，jar的完全路径名 -v：输出启动时带的jvm参数 jstat：虚拟机统计信息监视工具 一般用来查看堆内gc情况，比如年轻代、老年代大小、YGC次数，平均耗时等\nhttps://www.jianshu.com/p/213710fb9e40\njmap：Java内存印象工具 用来查看堆内存的使用情况，比如输出内存中的所有对象，可以配合eclipse MAT分析内存泄漏情况\nhttps://www.cnblogs.com/huanglog/p/10302901.html\n官方的：https://docs.oracle.com/javase/7/docs/technotes/tools/share/jstat.html\njhat：虚拟机堆转储快照分析工具 分析由jmap导出来的堆dump文件，作用类似Eclipse MAT，但是没MAT直观\njstack：Java堆栈跟踪工具 查看方法或线程的执行情况，线程的堆栈信息，死锁检测，死锁原因\nhttps://blog.csdn.net/wufaliang003/article/details/80414267\n官方：https://docs.oracle.com/javase/7/docs/technotes/tools/share/jstack.html\njinfo：Java配置信息工具 实时查看和调整JVM各项参数配置，进程运行时也能改JVM的配置\njinfo -sysprops [pid] 查看当前JVM全部系统属性 jinfo -flags [pid] 查看进程所有JVM参数，比jps -v更详细 jinfo -flag [[+代表打开，-代表关闭，都不写代表查看][JVM参数][赋值使用=][JVM参数值]] [pid] 可视化工具 JConsole 监控Java应用程序，可查看概述、内存、线程、类、VM、MBeans、CPU、堆栈内容、死锁检测\nVisualVM 功能比JConsole更加强大，支持插件，还能看到年轻代、老年代的内存变化，以及gc频率、gc的时间\nEclipse MAT 工具进行内存快照的分析，图表的方式展示，可以分析内存泄漏或溢出出现的代码段\n参考 CS-Note\n深入理解 Java 虚拟机 - 周志明\nJVM（三）调优工具\njvm系列(七):jvm调优-工具篇\n从实际案例聊聊Java应用的GC优化\nJVM -XX: 参数介绍\n","date":"2019-06-18T00:00:00Z","permalink":"http://nixum.cc/p/jvm/","title":"JVM"},{"content":"[TOC]\nSpringBoot Spring 和 Spring Boot区别 Spring Boot实现了自动配置，降低了项目搭建的复杂度。它主要是为了解决使用Spring框架需要进行大量的配置太麻烦的问题，所以它并不是用来替代Spring的解决方案，而是和Spring框架紧密结合用于提升Spring开发者体验的工具。同时它集成了大量常用的第三方库配置(例如Jackson, JDBC, Mongo, Redis, Mail等等)，做到零配置即用。内置Tomcat作为Web服务器，不像之前还要把服务部署到Tomcat在进行启动。\nSpringBoot整个启动流程  构建SpringApplication对象，执行其run方法 加载properties/yaml等配置文件 创建ApplicationContext（也可以称为Bean、IOC容器） 将扫描到的Bean或者xml中的bean，先解析成BeanDefinition，注册到ApplicationContext中的BeanFactory中（即自动配置过程，也是IOC容器的refresh方法执行过程） 实例化Bean，进行依赖注入，（AOP也是在此处实现，创建代理实例加入IOC容器）   SpringBoot启动流程 \n参考SpringBoot启动流程解析\nSpringBoot启动流程\nSpringBoot自动配置流程 自动配置流程只是SpringBoot启动中的一个环节，该环节只是在告诉Spring要在哪里找到Bean的声明。\n启动类main方法为入口，main方法所在的类会被**@SpringBootApplication**修饰， 通过main方法里执行**SpringApplication.run(Application.class, args)**进行启动，Spring启动时会解析出@SpringBootApplication注解，进行Bean的加载和注入。\n  @SpringBootApplication里包含了\n  @SpringBootConfiguration：作用类似于**@Configuration**，JavaConfig配置类，相当一个xml文件，配合@Bean注解让IOC容器管理声明的Bean\n  @ComponentScan：配上包路径，用于扫描指定包及其子包下所有类，如扫描@Component、@Server、@Controller等，并注入到IOC容器中\n  @EnableAutoConfiguration：自动配置的核心注解，主要用于找出所有自动配置类。该注解会使用**@Import(EnableAutoConfigurationImportSelector.class**)帮助SpringBoot应用将所有符合条件的@Configuration配置都加载到当前SpringBoot创建并使用的IoC容器。\n    EnableAutoConfigurationImportSelector类里有个SpringFactoriesLoader工厂加载器，通过里面的loadFactoryNames方法，传入工厂类名称和对应的类加载器，加载该类加载器搜索路径下的指定文件spring.factories文件，传入的工厂类为接口，而文件中对应的类则是接口的实现类，或最终作为实现类，得到这些类名集合后，通过反射获取这些类的类对象、构造方法，最终生成实例。\n因此只要在maven中加入了所需依赖，根据spring.factories文件里的key-value，能够在类路径下找到对应的class文件，就会触发自动配置\n  自定义starter 实际上就是编写自动配置类，会使用到一系列配置注解，如@Configuration、@EnableConfigurationProperties、@Component、@Bean、@ConditionOnXX、@AutoConfigureOrder等，让IOC容器加载我们自定义的Bean进去；\n另外就是必须在META-INF文件夹下创建spring.factories，告知Spring在哪找到配置类。\norg.springframework.boot.autoconfigure.EnableAutoConfiguration=[自定义配置类的全限定名称] 自定义Starter可以理解为一个Jar包，该Jar包在Maven或Gradle注册后，服务启动时，IOC容器会去自动加载。\n自定义Starter内也可以使用配置文件，设定默认配置的key-value，当本项目里有配置的key与starter里定义的配置key重复时可以被替换\nContextLoaderListener 【Spring】浅谈ContextLoaderListener及其上下文与DispatcherServlet的区别\n 作为Spring启动入口 实现了ServletContextListener 接口，监听ServletContext，如果 ServletContext 发生变化（如服务器启动时ServletContext 被创建，服务器关闭时 ServletContext 将要被销毁）时，执行监听器里的方法 为IOC容器提供环境，扫描包，将带有注解的Bean加入到容器用于依赖注入，或者加载xml文件，将xml注册的bean加入容器用于依赖注入  常用注解 @Controller与@RestController  @Controller 默认是返回视图，即方法的return返回的是视图层的路径，只有+@ResponseBody才会返回Json格式的数据 @RestController实际上是@Controller + @ResponseBody组合，默认返回json格式的数据  @Autowired与@Resource   @Autowired 注解，修饰类成员变量、方法及构造方法，完成自动装配的工作，默认按 byType 自动注入。只有一个required属性，默认是true，表示必须注入，不能为null\n@Autowired 自动注入时，Spring 容器中匹配的候选 Bean 数目必须有且仅有一个。因为它是按类型注入的，如果有多个同类型的Bean会导致出错，此时可以配合@Qualifier来规避这种情况，通过 @Qualifier(\u0026ldquo;实例名称\u0026rdquo;) 指定注入bean的名称，消除歧义，此时与 @Resource指定name属性作用相同。\n  @Resource 的作用相当于 @Autowired，只不过 @Autowired 按 byType 自动注入，面@Resource 默认按 byName 自动注入，该注解有两个属性，name和type，分别代表通过名称查找bean和通过类型查找bean。\n此外@Resource还有其他属性，如lookup、shareable、mappedName等\n  @Component与@Bean 两者都是用于标记，被标记的实例会被Spring管理\n  @Component只能作用类，配合@ComponentScan注解让Spring启动时进行扫描，当扫描到@Component修饰的类时会进行实例化和依赖注入\n@Component是一个比较通用的语义，@Service、@Repository的作用与@Component相同的，只是语义不同，修饰的类所在的层次不同\n  @Bean只能作用于方法，通过方法来实例化Bean，Bean的名称为方法的名称（如果有前缀get会自动忽略），交由IOC容器管理，通常与@Configuration配合使用，等价于xml文件中的配置，方法名相当于中的id，需要唯一\n@Bean的方式初始化bean会更加灵活，因为可以在方法内部进行逻辑处理，比如利用配置文件 + 工厂模式实例化不同的bean\n  @Configuration与@ConfigurationProperties  @Configuration作用于类，相当于加载bean的xml文件，一般配合@Bean注解使用，让IOC容器管理我们声明的bean @ConfigurationProperties用于读取key-value的那种配置文件，如properties、yaml等，类似于@Value，主要用于配置文件的字段注入，有属性prefix表示前缀，key为属性名称，将配置文件里的配置绑定到类的属性上。  @SpringBootApplication与@ComponentScan与@Import 两者都用于告知Spring在哪里找到bean，只是扫描的路径不同\n  @SpringBootApplication作用与main方法所在的类，用于启动IOC容器，默认会扫描该类所在包及其子包下，进行Bean的实例化和管理\n@SpringBootApplication实际上包含了三个注解，@ComponentScan、@EnableAutoConfiguration、@SpringBootConfiguration，详情见上面SpringBoot启动流程。\n  @ComponentScan(\u0026ldquo;包路径\u0026rdquo;)，用于扫描指定包及其子包下的类，哪些需要交由IOC容器管理，一般用于扫描@SpringBootApplication扫描不到的包。在非SpringBoot项目下，必须使用。\n  @Import，相当于xml中的，主要是导入Configuration类，作用类似@ComponentScan，只不过@ComponentScan是通过扫描找到，范围广，@Import是直接指定某个Config类\n  @ImportResource(\u0026ldquo;classpath*:xml文件\u0026rdquo;)，则是直接导入指定的xml文件\n  IOC和DI 控制反转：实际上就是把开发人员对程序执行流程的控制，反转到由程序自己来执行，代表性的例子就是 模板方法设计模式，实际上是一种设计思想，就像spring把依赖注入给抽成框架，由框架来自动创建对象、管理对象生命周期、注入等，开发者只需要关注类间的关系即可\n依赖注入：实际上就是对类成员初始化，并不在类内部进行，而是在外部初始化后通过构造方法、参数等方式才传递给类，就像spring那几种注入方式：构造器注入、set方法注入、注解注入\nBean的作用域 Spring中的bean默认都是单例的，对于一些公共属性，在多线程下并不安全，spring支持将bean设置为其他作用域\n prototype：多例，使用时才创建，每次获取的bean都不是同一个 request：每次请求创建新bean，request结束，bean销毁 session：每次请求创建新bean，仅在当前HTTP session内有效 globalSession：基于 portlet 的 web 应用，现在很少用了  注入方式   setter注入\n  构造器注入，无法解决循环依赖问题\n  自动装配：xml下使用“autowire”属性，有no、byName、byType、constructor、autodetect方式可选\n注解注入：@Resource默认是使用byName进行装配，@Autowired默认使用byType。\nbyName和byType指的是依赖注入时寻找bean的方式。@Resource和@Autowired都可以修饰属性、setter方法、构造器，此时表示的是以哪种方式进行注入\n  Bean加载流程 初始化IOC容器（工厂入货）\n 读取xml文件 / 扫描包类上的注解 解析成BeanDefinition，创建了Bean的定义类 注册到BeanFactory，此时的工厂里只保存了类创建所需要的各种信息还没有真正的实例化Bean对象  依赖注入（工厂出货）\n 初始化IOC容器 初始化Bean（没有设置Lazy-init） 反射创建Bean实例 注入  详细源码分析，参考Spring: 源码解读Spring IOC\nSpring IOC源码分析\n具体例子，参考Spring IOC核心源码学习\n三级缓存解决循环依赖  第一级缓存：单例缓存池singletonObjects，存放完全初始化完的实例或者AOP实例对象，此时已经完成注入，直接可用了。 第二级缓存：早期提前暴露的对象缓存earlySingletonObjects，用于检测循环引用，与singletonFactories互斥，如果一级缓存获取不到，则在此层获取实例，如果获取不到，且允许去三级缓存获取，则从三级缓存中获取，并remove加入二级缓存。一般用于存放完成了部分依赖注入的Bean。 第三级缓存：singletonFactories单例对象工厂缓存，存放初始化不完全的实例（还有依赖没注入），如果到了三级缓存都获取不到，就会进行初始化，并加入。  当出现循环依赖的对象注入时，会利用这三级缓存来解决问题，但是Spring只能解决Setter方法的注入，无法解决构造器注入，原因是如果通过构造器注入，需要先准备好需要注入的属性\n假如现在有类A，持有属性B，类B，持有属性A\n通过构造器注入：初始化A，此时需要B，那初始化B，此时需要A，但是A因为构造器注入需要先有B，此时无法完成初始化。\n通过Setter方法注入：\n 初始化A，会先依次从三级缓存中获取A实例，获取不到，说明A还未初始化，初始化A产生实例，将实例A加入singletonFactories中。 对A进行依赖注入，发现需要注入B，依次从三级缓存里获取B实例，到了第三层都获取不到，说明还未初始化，初始化B产生实例，加入singletonFactories，对B进行依赖注入，发现需要注入A，依次从三级缓存里获取A实例，在singletonFactories获取到还未初始化完全的实例A，从singletonFactories中remove，加入到earlySingletonObjects，注入到B中，此时B完全初始化完成，从earlySingletonObjects中remove，将B加入到singletonObjects中。 回到对A进行依赖注入部分，由于B刚刚初始化完成加入了singletonObjects，所以A获取到B，进行注入，A初始化完全，加入singletonObjects中  总结：Spring在实例化一个bean的时候，先实例化该bean，然后递归的实例化其所依赖的所有bean，直到某个bean没有依赖其他bean，此时就会将该实例返回，然后反递归的将获取到的bean设置为各个上层bean的属性中。\n实际上，看起来earlySingletonObjects会有点多余，这一层的缓存主要是扩展时使用。\nBean的生命周期   Spring 容器可以管理 singleton 作用域下 bean 的生命周期，在此作用域下，Spring 能够精确地知道bean何时被创建，何时初始化完成，以及何时被销毁；prototype 作用域的bean，Spring只负责创建，之后就不再管理，只能由开发人员通过代码控制\n  关于xxxAware类型的接口，Aware之前的名字表示IOC容器可以获得什么资源，Aware方法都是在初始化阶段前被调用。\n  生命周期执行过程，总结为：实例化 -\u0026gt; 属性赋值 -\u0026gt; 初始化 -\u0026gt; 销毁\n实例化\n Bean容器找到配置文件中Spring Bean的定义。 Bean容器利用反射创建一个Bean的实例（对scope为singleton且非懒加载的bean实例化，此处会触发AOPCreator解析@Aspect类）  属性赋值 - 依赖注入\n 如果涉及到一些属性值 利用set方法设置一些属性值。 如果Bean实现了BeanNameAware接口，调用setBeanName()方法，传入Bean的名字，相当于xml中中的id或name。 如果Bean实现了BeanClassLoaderAware接口，调用setBeanClassLoader()方法，传入ClassLoader对象的实例。 如果Bean实现了BeanFactoryAware接口，调用setBeanFactory()方法，传入beanFactory对象的实例，可以通过beanFactory获取其他Bean。 与上面的类似，如果实现了其他*Aware接口，就调用相应的方法，比如实现了ApplicationContextAware接口，就可以获取上下文，作用同BeanFactory，只是能获取到其他数据。 如果Bean实现了BeanPostProcessor接口，执行postProcessBeforeInitialization()方法（需手动实现该方法，利用该方法实现AOP，在此处创建代理类）  初始化\n 如果Bean实现了InitializingBean接口，执行afterPropertiesSet()方法。作用同xml配置中的init-method配置一样，用于指定初始化方法，但是先于init-method方法执行。 如果Bean在xml配置文件中的init-method属性，执行指定的方法进行初始化。 如果Bean实现了BeanPostProcessor接口，执行postProcessAfterInitialization()方法（需手动实现该方法，利用该方法实现AOP，在此处创建代理类） 此时bean已经准备就绪，可以被应用程序使用了，他们将一直驻留在应用上下文中，直到该应用上下文被销毁  销毁\n 当要销毁Bean的时候，如果Bean实现了DisposableBean接口，执行destroy()方法。 当要销毁Bean的时候，如果Bean在xml配置文件中的的destroy-method属性，执行指定的方法进行销毁。    参考：Spring Bean生命周期\nSpring Bean生命周期：这篇写得不错\nApplicationContext与BeanFactory的区别  ApplicationContext与BeanFactory类似，BeanFactory只提供最基本的Bean对象创建与获取，ApplicationContext指的是上下文，包含BeanFactory的功能（实现了BeanFactory接口），同时也提供了其他额外的功能，比如事件机制、监听、拦截器、资源访问等。 BeanFactory采用延迟加载来注入Bean，只有在使用到某个Bean的时候才会实例化，ApplicationContext则在容器启动时，就实例化Bean，常驻在容器内，也可以为Bean配置Lazy-init=true来让Bean延迟实例化。  IOC模拟 Spring——原理解析-利用反射和注解模拟IoC的自动装配\n根据 Spring 源码写一个带有三级缓存的 IOC\nAOP  切面Aspect：指代理类，声明切点和通知 切点PointCut：指要把切面的通知在被代理类的方法的位置，即表达式execution（值的格式为 [方法的访问修饰符] [被代理类的全限定名称]和其方法名(方法参数)]），切点可以有多个，配合通知使用 通知Advice：代理类的增强方法，通知分为前置通知、环绕通知、后置通知、后置返回通知、后置异常通知 连接点JoinPoint：被代理类的方法  基本原理 AOP基于代理模式，代理分为三种代理：静态代理，JDK动态代理，CGLib代理\n静态代理 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  // 接口 public interface A { public void method();} // 代理类 public class BProxy implements A { public B b; public BProxy() {} public BProxy(B b) { this.b = b; } public void method(){ // 为被代理类进行一系列前置操作  b.method(); // 为被代理类进行一系列后置操作  } } // 被代理类 public class B implements A { public void method() { // 具体的处理逻辑  } } public static void main(String[] args) { B b = new B(); BProxy bProxy = new BProxy(b); bProxy.method(); }   好处：在不修改目标对象的功能的前提下，增添新方法 缺点：每个对象都需要有代理对象，导致有很多代理类；接口增加方法，所有的实现类都要改\nJDK动态代理 动态代理是为了解决上述问题，通过反射 + 多态的方式，动态为一个一个类设置代理，只有知道那个类的接口才可以。利用 JDK java.lang.reflect包里的InvocationHandler接口，代理方法具体逻辑写在invoke方法里\n1 2 3 4 5 6  代理对象的执行方法！ public interface InvocationHandler { // proxy：动态产生的代理对象，method：被代理类要执行的方法，args：方法所需参数  public Object invoke(Object proxy, Method method, Object[] args) throws Throwable; }   还有java.lang.reflect包中的Proxy类的newProxyInstance方法,，他是静态的，作用是为被代理类构建代理类，并返回，在这个动态生成的代理类中已经织入了InvocationHandler，而它又持有被代理类，对代理对象的所有接口方法调用都会转发到InvocationHandler.invoke()方法\n1 2 3 4 5  该方法将会为被代理类生成代理类，代理类执行与被代理类的接口时，会执行invocationHandler的方法！ public static Object newProxyInstance(ClassLoader loader,\t// 被代理类的类加载器，如上面B类的类加载器  Class\u0026lt;?\u0026gt;[] interfaces,// 被代理类的接口数组  InvocationHandler h)\t// 上面实现了InvocationHandler接口的实例  throws IllegalArgumentException   具体例子\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  public class ProxyFactory { // 被代理类 \tprivate Object target; public ProxyFactory(Object target){ this.target = target; } // 匿名内部类的方式重写invoke方法 \tpublic Object getProxyInstance(){ // 返回代理类 \treturn Proxy.newProxyInstance(target.getClass().getClassLoader(), target.getClass().getInterfaces(), new InvocationHandler() { @Override //当代理者(接口)执行接口里的方法的时候就会调用此方法 \tpublic Object invoke(Object proxy, Method method, Object[] args) throws Throwable { System.out.println(\u0026#34;为被代理类进行一系列前置操作\u0026#34;); //执行实现了该接口的类中的方法，被代理类的方法 \tObject returnValue = method.invoke(target, args); System.out.println(\u0026#34;为被代理类进行一系列后置操作\u0026#34;); return returnValue; //返回的是调用方法后的结果 \t} }); } } public static void main(String[] args) { A b = new B();\t// B b = new B();也是可以的  A bProxy = (A) new ProxyFactory(b).getProxyInstance();\t// 必须转接口类型  // 调用该接口里的方法都会被代理  bProxy.method(); }   好处：只需写一次该接口的代理类就可以为以后许多实现了此接口的被代理类进行代理，一次编写，处处使用 缺点：只针对一个接口实现的代理，只能针对接口写代理类\nCGLIB代理 CGLIB代理就可以直接代理普通类，不需要接口了\n原理：直接读取被代理类的字节码，通过继承的方式实现，在内存中构建被代理类的子类对象从而实现对目标对象的功能扩展\n具体例子：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  public class ProxyFactory implements MethodInterceptor{ private Object target; public ProxyFactory(Object target){ this.target = target; } //给目标对象创建代理对象 \tpublic Object getProxyInstance(){ Enhancer en = new Enhancer(); //设置父类 \ten.setSuperclass(target.getClass()); //设置回调函数 \ten.setCallback(this); //创建子类代理 \treturn en.create(); } @Override //object 为CGLib动态生成的代理实例 \t//Method 为上文实体类所调用的被代理的方法引用 \t//Object[] 方法的参数列表 \t//MethodProxy 为生成的代理类对方法的代理引用 \tpublic Object intercept(Object obj, Method method, Object[] args, MethodProxy proxy) throws Throwable{ System.out.println(\u0026#34;为被代理类进行一系列前置操作\u0026#34;); Object returnValue = method.invoke(target,args); //或者写成 Object returnValue1 = proxy.invokeSuper(obj, args); \tSystem.out.println(\u0026#34;为被代理类进行一系列后置操作\u0026#34;); return returnValue; } } public static void main(String[] args) { B b = new B(); // B类的所有非final方法，包括它的父类都会被代理  B bProxy = (B) new CGlibProxy(b).getProxyInstance(); bProxy.method(); }   SpringBoot中的实现   SpringBoot中提供@EnableAspectJAutoProxy开启对AOP的支持，其中属性proxyTargetClass=true时使用cglib，为false使用JDK的动态代理，默认为false\n  @EnableAspectJAutoProxy注解主要是使用AspectJAutoProxyRegistrar类将AOP处理工具注册到Bean中。\n  在这个AOP处理工具中有一个AnnotationAwareAspectJAutoProxyCreator类，该类\n 实现了一系列Aware接口，使用BeanFactory：使得Bean可以管理 实现了order接口：用于设置切面的优先级 继承了ProxyConfig：该类封装了代理的通用逻辑，cglib或JDK动态代理开关配置等    Bean加载完AnnotationAwareAspectJAutoProxyCreator类后，会解析开发者定义的切面类、切点、通知，在BeanFactory中找到被代理类，结合通知进行封装，创建出代理类。由于被代理类可被设置多重代理，在创建代理类时，会根据切面的优先级，不断套在被代理类上，形成拦截器链。\n  执行代理类的方法时，就会调用方法拦截器链，进行方法增强。\n  AOP的实现会在创建Bean实例对象前触发@Aspect切面对象，获得Advisor。生成Bean实例对象之后，才会再次触发对该Bean实例对象做代理增强，增强的Advisor来自之前的解析结果。\nSpringAOP详细介绍\nSpring 源码分析Aop\n事务 Spring事务传播行为  PROPERGATION_MANDATORY：方法必须运行在事务中，如果当前事务不存在，抛异常 PROPAGATION_NESTED：当前事务存在，则该方法运行在嵌套事务中 PROPAGATION_NEVER：方法不能运行事务中，否则抛异常 PROPAGATION_REQUIRED：当前方法必须运行在事务中，如果当前存在事务，该方法运行其中，否则创建新事务 PROPAGATION_REQUIRES_NEW：当前方法必须运行在事务中，如果当前存在事务，则该事务在方法运行期间被挂起 PROPAGATION_SUPPORTS：当前方法不需要运行在事务中，但如果存在事务，也可运行在事务中 PROPAGATION_NOT_SUPPORTED：当前方法不能运行在事务中，如果存在事务，则挂起该方法  ","date":"2019-04-17T00:00:00Z","permalink":"http://nixum.cc/p/spring%E5%92%8Cspringboot/","title":"Spring和SpringBoot"},{"content":"[TOC]\n线程、进程、协程   进程：可以简单理解为一个应用程序，进程是资源分配的基本单位。比如一个进程拥有自己的堆、栈、虚存空间、文件描述符等。\n涉及到用户态和内核态的切换。\n进程间的通信：\n 匿名管道：半双工，数据只能向一个方向流动，双方需要通信时，需要建立起两个管道；且只能用于有亲缘关系的进程；本质是一个内核缓冲区，可以看成是内存中的文件，但不属于某种文件系统，无需显示打开，创建时直接返回文件描述符，读写时需要确定对方的存在，否则将退出；以先进先出的方式存取数据，通信的双方需制定好数据的格式； 有名管道：主要解决匿名管道只能作用与有亲缘关系的进程的问题，通过一个路径名关联，以文件形式存在于文件系统中，即使没有亲缘关系的进程也能通过访问路径实现通信；管道名字存在于文件系统中，内容存在内存中；打开时就得确定对方是否存在，否则将阻塞； 信号：操作系统提供的一种机制，可以在任何时候发给某一进程，而无需指定该进程的状态，如果该进程当前处于未执行状态，该信号就由内核保存起来，直到进程回复执行并传递为止；信号接收可以被阻塞，直到阻塞解除；本质是对中断机制的模拟，异步通信，在用户态和内核态之间交互；能携带的信息较少。 消息队列：存放在内核中的消息链表，只有在内核重启或显示地删除时，才会被真正的删除，与管道不同的是消息队列不需要确定接收进程是否存在；一般是FIFO，但也可以实现成随机查询；对消息格式，缓冲区大小等都能进行控制，比管道灵活； 共享内存：没什么好说的，只是在访问共享内存时要依靠一些同步或互斥机制保证并发访问安全； 信号量：计数器，一般用于多进程对共享内存访问的保护，内核中实现，保证原子操作 套接字：通信机制，可用在本机或者跨网络，由域、端口号、协议类型三个属性确定；域分为AF_INET，即网络，另一个是AF_UNIX，即文件系统    线程：线程是独立调度的基本单位，由CPU进行调度和执行的实体。一个进程中可以有多个线程，线程之间共享进程资源，是进程中的实际运作单位。\n涉及到用户态和内核态的切换。\n  协程：GoLang中的协程\n 在用户态层面，由线程控制，即用户应用层面自己控制，很难像抢占式调度那样强制CPU切换到其他线程/进程，只能是协作式调度，但同时也避免了上下文切换 内存消耗比线程小，比如go开启协程是几kb，java开启一个线程至少1MB 实现原理：在一个运行的线程上，起多个协程，每个协程会加入到调度队列中，线程会从调度队列里取出协程进行运行。队列个数默认取决于CPU的个数，协程间的切换会线程使用go提供的io函数进行控制。当有协程执行较慢时，会先将其挂起，然后唤醒其他线程，将未处理的协程队列转移到该线程，消费队列里的协程，当队列消费完成后，再切回原来的线程，继续执行刚刚挂起的协程。    参考：图解Go协程调度原理，小白都能理解 \nGolang 的 goroutine 是如何实现的？\n进程与线程的区别   拥有资源\n进程是资源分配的基本单位，但是线程不拥有资源，线程可以访问隶属进程的资源。\n  调度\n线程是独立调度的基本单位，在同一进程中，线程的切换不会引起进程切换，从一个进程中的线程切换到另一个进程中的线程时，会引起进程切换。\n  系统开销\n由于创建或撤销进程时，系统都要为之分配或回收资源，如内存空间、I/O 设备等，所付出的开销远大于创建或撤销线程时的开销。类似地，在进行进程切换时，涉及当前执行进程 CPU 环境的保存及新调度进程 CPU 环境的设置，而线程切换时只需保存和设置少量寄存器内容，开销很小。\n  通信方面\n线程间可以通过直接读写同一进程中的数据进行通信，在java中如使用共享变量、wait/notify机制、阻塞队列；但是进程通信需要借助管道、消息队列、共享存储、信号量、信号、套接字socket\n  上下文切换的开销\n当CPU从执行一个线程切换到执行另外一个线程的时候，它需要先存储当前线程的本地的数据，程序指针等，然后载入另一个线程的本地数据，程序指针等，最后才开始执行。这种切换称为“上下文切换”(“context switch”)。CPU会在一个上下文中执行一个线程，然后切换到另外一个上下文中执行另外一个线程。\n调度方式  非抢占式：系统一旦开始执行某一进程，就会让该线程就会一直执行下去，直至完成，或者发生了其他事件导致系统放弃对该进程的执行后，才会去执行另外一个进程。 抢占式：系统执行某一进程，在其执行期间，系统可以立即停止当前进程，转而执行另外一个进程，待处理完后，重新回来继续执行之前停止的进程  调度原理 用户空间线程和内核空间线程之间的映射关系\n N:1模型：多个用户空间线程在1个内核空间线程上运行。优势是上下文切换非常快，因为这些线程都在内核态运行，但是无法利用多核系统的优点。\n1:1模型：1个内核空间线程运行一个用户空间线程。这种充分利用了多核系统的优势但是上下文切换非常慢，因为每一次调度都会在用户态和内核态之间切换。POSIX线程模型(pthread)就是这么做的。\nM:N模型：内核空间开启多个内核线程，一个内核空间线程对应多个用户空间线程。效率非常高，但是管理复杂。\n 线程的状态 1.状态转换  线程状态转换 \n“阻塞”与“等待”的区别： “阻塞”状态是等待着获取到一个排他锁，进入“阻塞”状态都是被动的，离开“阻塞”状态是因为其它线程释放了锁，不阻塞了； “等待”状态是在等待一段时间 或者 唤醒动作的发生，进入“等待”状态是主动的\n2. 状态  New（新建）：通过new创建一个新线程，但还没运行，还有一些其他基础工作要做 Runnable（可运行，就绪）：线程调用start方法，可能处于正在运行也可能处于没有运行，取决于操作系统提供的运行时间 Running（运行） Blocked（阻塞）：线程已经被挂起，等待锁的释放，直到另一个线程走完临界区或发生了相应锁对象wait()操作后，它才有机会去争夺进入临界区的权利  等待阻塞：运行的线程执行wait()方法，JVM会把该线程放入等待池中。 同步阻塞：运行的线程在获取对象的同步锁时，若该同步锁被别的线程占用，则JVM会把该线程放入锁池中。 其他阻塞：运行的线程执行sleep()或join()方法，或者发出了I/O请求时，JVM会把该线程置为阻塞状态。当sleep()状态超时、join()等待线程终止或者超时、或者I/O处理完毕时，线程重新转入就绪状态。   Waiting（无限期等待）： 处于此状态的线程会等待另外一个线程，不会被分配CPU执行时间，直到被其他线程唤醒  没有设置timeout参数的Object.wait() 没有设置timeout参数的Thread.join() LockSupport.park() 以上方法会使线程进入无限等待状态   Timed_waiting（限期等待）：不会被分配CPU执行时间，不过无需等待被其它线程显示的唤醒  Thread.sleep()方法 设置了timeout参数的Object.wait()方法 设置了timeout参数的Thread.join()方法 LockSupport.parkNanos()方法 LockSupport.parkUntil()方法   TERMINATED（结束，死亡）：已终止线程的线程状态，线程已经结束执行，run()方法走完了，线程就处于这种状态或者出现没有捕获异常终止run方法意外死亡  死锁 死锁产生的条件  互斥条件：一个资源每次只能被一个线程使用； 请求与保持条件：一个线程因请求资源而阻塞时，对已获得的资源保持不放； 不剥夺条件：进程已经获得的资源，在未使用完之前，不能强行剥夺； 循环等待条件：若干线程之间形成一种头尾相接的循环等待资源关系。  避免死锁 一般场景发生在共享资源的使用上：\n 线程1对资源A加锁后，进行业务操作，线程2对资源B加锁后进行业务操作，线程1业务处理需要用到资源B，线程2业务处理需要用到资源A，此时发生死锁 线程1对资源A加锁后进行业务操作，线程2也需要用到资源A，但是线程1一直不释放锁，  互斥条件是保证线程安全的条件，因此不能破环，只能尽量破坏其他造成死锁的条件，比如提前分配各个线程所需资源；设置等待时间或者自旋次数，超时中断；分配好获得锁的顺序；避免逻辑中出现复数个线程相互持有对方线程所需的独占锁的情况；\n比如:\n 在合适的场景使用合适的锁类型，是否允许锁可重入的，共享还是排他 避免多个线程操作多个共享资源，注意锁的申请顺序，比如给资源设置序号，顺序取放 获取锁时设置超时时间 减少共享资源的使用，使用ThreadLocal，消息队列，共享资源在方法内复制(set回去的时候cas)，或者设计一种模式对共享资源的访问 jstack检查线程与状态  死锁检测工具 Jconsole, Jstack, visualVM\nObject和Thread中关于线程的一些方法 Object类中wait()和notify()、notifyAll()   wait()使得线程进入等待状态，同时释放锁，等待其他线程notify()、notifyAll()的唤醒\n因为wait()和notify()、notifyAll()是对象中的方法，如果wait()没有释放锁，其他线程就无法获得锁进入同步代码块中，也就无法执行notify()或者notifyAll()方法唤醒挂起的线程，造成死锁\n  这套方法只能在同步块synchronized中使用，否则会抛IllegalMonitorStateException异常，因为如果没有synchronized，有可能会导致多线程wait时对共享资源的竞争导致问题\n  wait、notify、synchronized都是对同一个对象进行操作\n  wait() 方法可以设置时间，时间到了也会进入就绪状态\n  notify()方法只会随机唤醒某个在等待的线程，notifyAll()方法是唤醒全部，之后进行竞争，排队执行\n  可响应中断\n  会抛InterruptedException异常\n  为什么操作线程的方法wait()和notify()、notifyAll()是Object类中的？\njava提供的锁是对象级别的，等待需要锁，把每个对象看成一个锁，同一个对象可以放入不同的线程中，从而达到不同线程可以等待或唤醒，如果是线程里的方法，当前线程可能会等待多个线程的锁，这样操作比较复杂\nThread中的yield()  静态方法 yield()的作用是让步。它能让当前线程由“运行状态”进入到“就绪状态”，从而让其它具有相同优先级的等待线程获取执行权 该方法只是对线程调度器的一个建议，而且也只是建议具有相同优先级的其它线程可以运行，并不能保证在当前线程调用yield()之后，其它具有相同优先级的线程就一定能获得执行权，也有可能是当前线程又进入到“运行状态”继续运行  Thread中的 suspend() 和resume()，已过期  不是静态方法 suspend()用于挂起线程， resume() 用于唤醒线程，需要配套使用，这两个方法被标为过期，不推荐 suspend() 在导致线程暂停的同时，并不会去释放任何锁资源。其他线程都无法访问被它占用的锁。直到对应的线程执行 resume() 方法后，被挂起的线程才能继续，从而其它被阻塞在这个锁的线程才可以继续执行 如果 resume() 操作出现在 suspend() 之前执行，那么线程将一直处于挂起状态，同时一直占用锁，这就产生了死锁。而且，对于被挂起的线程，它的线程状态居然还是 Runnable 不会抛InterruptedException异常  Thread类中的sleep()方法  静态方法 使当前正在执行的线程进入休眠（阻塞），不会释放锁，单位是毫秒 sleep() 可能会抛出 InterruptedException，因为异常不能跨线程传播回 main() 中，因此必须在本地进行处理。线程中抛出的其它异常也同样需要在本地进行处理 可响应中断 会抛InterruptedException异常  Thread类中的join()方法  不是静态方法 在线程中调用另一个线程的 join() 方法，会将当前线程挂起（阻塞），而不是一直等待，直到目标线程结束 没有释放锁 可响应中断 会抛InterruptedException异常  Condition类中的await()和signal()、signalAll()  Condition类中的await()和signal()、signalAll()用来代替传统Object里的wait()和notify()、notifyAll()方法，作用基本相同 await()可以指定条件，Condition类中的await()和signal()、signalAll()会更加灵活 Condition配合Lock(ReentrantLock)使用，Lock 可以用来获取一个 Condition对象、还有加锁解锁，阻塞队列中就使用了Condition来模拟线程间协作 会抛InterruptedException异常  中断 参考Java并发\u0026ndash;InterruptedException机制\n关键字 synchronized 在进入synchronized代码块时，执行 monitorenter，将计数器 +1，释放锁 monitorexit 时，计数器-1；当一个线程判断到计数器为 0 时，则当前锁空闲，可以占用；反之，当前线程进入等待状态。\nsynchronized是几种锁的封装：自旋锁、锁消除、锁粗化、轻量锁、偏向锁，在加对象锁时，在对象的对象头中的Mark Word记录对象的线程锁状态，根据线程的竞争情况在这几种锁中切换\n 当synchronized(xxx.class)锁住的是类时，多个线程访问不同对象(它们同类),就会锁住代码段，当一个线程执行完这个代码段后才轮到别的线程，可以理解成全局锁 当synchronized(object)锁住的是对象时，多个线程访问不同对象(它们同类),它们相互之间并不影响，只有当多个线程访问同一对象时，才会锁住代码段，等到一个线程执行完之后才轮到别的线程执行 当synchronized(this)锁住的是当前的对象，当synchronized块里的内容执行完之后，释放当前对象的锁。同一时刻若有多个线程访问这个对象，则会被阻塞 synchronized加在方法上，作用同锁住this，即当前对象 synchronized所在的方法被static修饰，则锁住的是整个类 synchronized下不可被中断 synchronized是非公平锁  在用synchronized关键字的时候，尽量缩小代码段的范围，能在代码段上加同步就不要再整个方法上加同步，减小锁的粒度，使代码更大程度的并发，如果锁的代码段太长了，别的线程等得就久一点\nsynchronized和ReentrantLock区别 相同点：\n 都是加锁实现同步，阻塞性同步 可重入 有相同的并发性和内存语义  不同点：\n  synchronized，是关键字，底层靠JVM实现，通过操作系统调度；ReentrantLock是JDK提供的类，源码可查\n  synchronized锁的范围看{}，能对类、对象、方法加锁，由JVM控制锁的添加和释放；\nReentrantLock锁代码块，可以灵活控制加锁解锁的位置，需要手动控制锁的添加和释放，相对来讲锁的灵活，锁的细粒度都比synchronized好些\n  synchronized是非公平锁，ReentrantLock支持公平锁和非公平锁，默认是非公平锁\n  synchronized不可中断，除非抛异常，否则只能等同步的代码执行完；ReentrantLock持锁在长期不释放锁时，正在等待的线程可以选择放弃等待，方法如下：\n lock(), 如果获取了锁立即返回，如果别的线程持有锁，当前线程则一直处于休眠状态，直到获取锁 tryLock(), 如果获取了锁立即返回true，如果别的线程正持有锁，立即返回false； tryLock (long timeout, TimeUnit unit)， 如果获取了锁定立即返回true，如果别的线程正持有锁，会等待参数给定的时间，在等待的过程中，如果获取了锁定，就返回true，如果等待超时，返回false； lockInterruptibly: 如果获取了锁定立即返回，如果没有获取锁定，当前线程处于休眠状态，直到或者锁定，或者当前线程被别的线程中断    synchronized在JDK1.6版本中进行了优化，性能跟ReentrantLock差不多，ReentrantLock仅比synchronized多了一些新功能\nsynchronized和ReentrantLock都提供了最基本的锁功能，ReentrantLock多了一些额外的功能：可以提供一个Condition类，实现分组唤醒需要唤醒的线程；提供一些方法监听当前锁的信息\n  volatile 需要先了解Java的内存模型，简单的说就是 每个线程有自己的工作内存，工作内存存在高速缓存中，而变量存在于主内存中，线程只能操作工作内存中的变量\n线程操作变量的时候，变量会从主内存中load到工作内存中，线程再在工作内存中使用变量，处理完之后再把变量更新到主内存中\n  保证此变量对所有线程的可见性，即当以线程修改这个变量的值，新值对其他线程可知，即如果此变量发送改变，会立即同步到主内存中，volatile只能保证可见性\n由于java里的运算不是原子性，所有volatile变量的运算在并发下一样不安全\n当出现\n运算结构不依赖变量的当前值，或能确保只有单一的线程修改变量的值\n变量不需要与其他的状态变量共同参与不变约束\n此时需要加锁\n  禁止指令重排序优化，普通变量只会保证该方法执行过程中所依赖赋值的结果都能获得正确的结果，不能保证变量赋值操作的顺序与程序代码的执行顺序一致\n  典型用法是 检查某个状态标记以判断是否退出循环\n参考Java多线程学习（三）volatile关键字\n全面理解Java内存模型(JMM)及volatile关键字\n一些锁的概念  java中的锁 \n参考：\njava中的锁\n美团技术团队 - 不可不说的Java“锁”事\n线程池 为了解决无限创建线程产生的问题，采用线程池来管理，减少在创建和销毁线程上所消耗的时间以及系统资源的开销，解决资源不足的问题。如果不使用线程池，有可能造成系统创建大量同类线程而导致消耗完内存或者“过度切换”的问题\n线程池 Executors静态类，工厂方法生成以下线程池\n​\tnewCachedThreadPool：核心数是0，最大数是Integer.MAX_VALUE，使用SynchronousQueue，不存储任务。对于每个任务，如果有空闲线程可用，立即让他执行任务，如果没有可用得空闲线程，则创建新线程，空闲线程超时设置是1分钟，线程池可无限扩展。容易造成堆外内存溢出，一般用于大量短期任务\n​\tnewFixedThreadPool：构建有固定大小的线程池，使用LinkedBlockingQueue。如果提交的任务数多于核心线程数，则把任务放到队列中，等其他任务完成后再运行他，创建的线程不会超时\n​\tnewSingleThreadExecutor：大小为 1 的线程池，使用LinkedBlockingQueue，每次只有一个线程执行任务，从阻塞队列中取任务一个接一个执行\n这三个静态方法 返回实现了ExecutorService接口的ThreadPoolExecutor类的对象\n使用方式：\n1 2 3 4  ExecutorService threadPool = Executors.newCachedThreadPool(); threadPool.execute(线程实例);\t// 无返回值 Future\u0026lt;T\u0026gt; = threadPool.submit(实现了Callable\u0026lt;T\u0026gt;的实例); // 有返回值 threadPool.shutdown();   在阿里巴巴开发手册中不提倡使用Executors创建，而是通过 ThreadPoolExecutor的方式去定制线程池，从而明确线程池的参数\n1 2 3 4 5 6 7 8 9  ExecutorService threadPool = new ThreadPoolExecutor(一系列参数，具体看构造器); // 构造器 int corePoolSize,\t// 线程池中的线程数量 int maximumPoolSize,\t// 线程池中允许的最大数量，当前阻塞队列满了，且继续提交任务，则创建新的线程执行任务，前提是当前线程数小于maximumPoolSize，如果使用无界队列，该参数就没什么用了 long keepAliveTime,\t// 当活跃线程数大于corePoolSize时，多余的空闲线程最大存活时间 TimeUnit unit,\t// keepAliveTime单位 BlockingQueue\u0026lt;Runnable\u0026gt; workQueue,\t// 用来保存等待被执行的任务的阻塞队列，且任务必须实现Runable接口，常见的如ArrayBlockingQueue、LinkedBlockingQuene ThreadFactory threadFactory,\t// 当线程池创建新线程时调用，例如为每个线程实现一个名字 RejectedExecutionHandler handler\t// 线程池的饱和策略，线程池提供4种饱和策略，默认是AbortPolicy     阻塞队列\n当线程池中的线程数量大于等于corePoolSize的时候，把该任务封装成一个Worker对象放入等待队列\n ArrayBlockingQueue：是一个基于数组结构的有界阻塞队列，此队列按FIFO（先进先出）原则对元素进行排序，此时maximumPoolSize就会限制任务数 LinkedBlockingQueue：一个基于链表结构的阻塞队列，此队列按FIFO排序元素，吞吐量通常要高于ArrayBlockingQueue。静态工厂方法Executors.newFixedThreadPool()使用了这个队列。队列无界时，maximumPoolSize也不起作用了 SynchronousQueue：一个不存储元素的阻塞队列。每个插入操作必须等到另一个线程调用移除操作，否则插入操作一直处于阻塞状态，吞吐量通常要高于LinkedBlockingQueue，静态工厂方法newCachedThreadPool使用了这个队列 PriorityBlockingQueue：一个具有优先级的无界阻塞队列    饱和策略\n当运行的线程数量大于等于maximumPoolSize，且阻塞队列满了，才执行饱和策略\n AbortPolicy：无法处理新任务时抛出异常，默认策略 CallerRunsPolicy：只用调用者所在线程来运行任务 DiscardOldestPolicy：丢弃队列里最近的一个任务，并执行当前任务 DiscardPolicy：不处理，丢弃掉。    线程池处理过程 RUNNING：正常状态，接受新任务，处理等待队列中的任务 SHUTDOWN：不接收新任务，但会处理阻塞队列中的任务； STOP ：不接收新任务，也不会处理阻塞队列中的任务，而且会中断正在运行的任务； TIDYING ：所有的任务都销毁，workcount=0，线程池在转换为此状态时会执行terminated() TERMINATED：terminated()方法执行过后变成这个 线程池内部有一个变量（private final AtomicInteger变量ctl，高3位保存运行状态runState，低29位保存有效线程数量workerCount）来表示线程的状态，根据这个变量表达的状态在操作\n 如果当前运行的线程少于corePoolSize，则创建新线程来执行任务（注意，执行这一步骤需要获取全局锁） 如果运行的线程等于或多于corePoolSize，则将任务加入BlockingQueue 如果无法将任务加入BlockingQueue(队列已满)，则创建新的线程来处理任务（注意，执行这一步骤需要获取全局锁） 如果创建新线程将使当前运行的线程超出maximumPoolSize，任务将被拒绝，并调用RejectedExecutionHandler.rejectedExecution()方法  当corePoolSize = maximumPoolSize时，多出的任务都会交给阻塞队列去处理，如果阻塞队列满了，就执行饱和策略，如果队列无界，则任务堆积\n线程池里线程的锁是不可重入锁\n执行任务的时候要对工作线程加锁的理由：工作线程run方法中有个判断，当阻塞队列中没任务时，会阻塞，循环等待任务；调用shutdown方法时，会将线程池状态设置为SHUTDOWN，并且不允许将任务加入到阻塞队列中，中断各个工作线程；如果shutdown方法中没有对工作线程加锁，并在锁内修改状态，中断工作线程，如果此时操作系统分配的时间片给到工作线程的判断中，阻塞队列没有任务，就一直阻塞了，这样线程池就一直关闭不了了。\n加锁操作发生在创建线程、工作线程在获取到任务后、工作线程没任务处理后退出、判断是否结束线程池、关闭线程池；线程池状态(ctl)的改变使用自旋+CAS判断操作\n 线程池的本质是一个生产者-消费者模型，解耦了线程和任务，达到线程复用的目的。任务管理部分充当生产者的角色，当任务提交后，线程池会判断该任务后续的流转：（1）直接申请线程执行该任务；（2）缓冲到队列中等待线程执行；（3）拒绝该任务。线程管理部分是消费者，它们被统一维护在线程池内，根据任务请求进行线程的分配，当线程执行完任务后则会继续获取新的任务去执行，最终当线程获取不到任务的时候，线程就会被回收。\n 线程池的实现原理\n深入理解Java线程池\nJava线程池实现原理及其在美团业务中的实践，这篇文章写得十分详细易理解了\n对线程池复用的简单理解 这里先不谈如何保证线程安全，只说复用。比如corePoolSize = 5，就是内部有5个线程Worker（继承了AQS和Runnable接口），通过ThreadFactory创建，保持在一个HashSet里，任务task就是实现了Runnable接口的类。当任务数如果大于corePoolSize ，则加入到阻塞队列workQueue中；当然如果BlockingQueue存满了，corePoolSize 个线程里的任务还没执行完，会继续创建线程达到maximumPoolSize，多于corePoolSize 的线程没有被复用，当workQueue里没有任务了，并且线程池里也没其他任务了，这个时候就会等待keepAliveTime之后对线程进行回收。复用指的是对于接下来要执行的一系列任务，可以通过几个线程来执行，而不用重复的开启线程结束线程、一个任务对应一个线程。\n线程worker的run方法里通过直接获取任务或者从阻塞队列workQueue里获取任务作为循环条件来执行任务的run方法来达到线程重用，注意不是start方法\n线程池是如何做到当线程数量大于corePoolSize ，多余的空闲线程在等待keepAliveTime之后被回收 在Worker类里的runWorker方法里，当阻塞队列里取不出任务task.take()==null，即task.take()方法里会根据timed属性作为是否回收空闲线程的标志，在取阻塞队列里的任务为null时，会等待keepAliveTime时间，返回null的任务，相应的空闲线程数将减少，runWorker方法的finally块里根据线程数目判断是否结束此方法，以此达到当前线程被回收的目的\n线程池调优  设置最大线程数，防止线程资源耗尽； 使用有界队列，从而增加系统的稳定性和预警能力(饱和策略)； 根据任务的性质设置线程池大小：CPU密集型任务(CPU个数个线程)，IO密集型任务(CPU个数两倍的线程)，混合型任务(拆分)。  JUC包内的一些类 BlockingQueue 1.分类 注：有界指的是，对阻塞队列初始化时必须指明队列大小\n   名称 说明     ArrayBlockingQueue 底层是数组，有界，需要初始化时指明容量大小，支持公平模式，默认非公平模式，读写都是只有一把锁   LinkedBlockingQueue 底层是链表，有界，默认容量是Integer.MAX_VALUE，使用读锁和写锁，因此吞吐量比ArrayBlockingQueue大   PriorityBlockingQueue 底层是数组 + 堆排实现排序，无界，默认容量是11，最大是Integer.MAX_VALUE - 8，默认对元素使用自然顺序排序，也可指定比较器   DelayQueue 无界，支持延时获取，不允许take或poll移除未过期元素，size=过期元素 + 非过期元素   SynchronousQueue 有界，一个线程的插入必须等待另一个线程的删除后才能完成，反之亦然，不能被迭代，容量可理解为只有1，支持公平和非公平模式   LinkedTransferQueue 底层是链表，无界，生产者会一直阻塞直到所添加到队列的元素被某一个消费者所消费，主要用于线程间消息的传递   LinkedBlockingDeque 底层是链表，双向队列，无界    2.常用方法    操作 操作失败时会抛异常 操作后会返回特殊值 操作时会阻塞 超时退出返回特殊值     插入 add(e) offer(e) put(e) offer(e, time, unit)   移除 / 获取 remove() poll() take() poll(time, unit)    3.原理 ArrayBlockingQueue 1 2 3 4 5 6 7 8 9  // 把数组当成环形队列使用 // 在执行插入或获取操作前会上锁，上的是同一把锁，方法执行完解锁，允许中断 final ReentrantLock lock; // 在take方法中，如果数组长度为0，则调用await()方法进行等待，否则就获取第一个元素，调用singal()方法唤醒等待线程生产 private final Condition notEmpty; // 在put方法中，往数组中添加一个元素，更新索引，再调用singal()方法唤醒等待线程消费，如果数组满了，则调用await()方法进行等待 private final Condition notFull;   LinkedBlockingQueue 1 2 3 4 5 6 7 8 9 10 11  // 整体分为读锁和写锁，而不像ArrayBlockingQueue只使用一个锁，原因是LinkedBlockingQueue底层是链表的，只需要关心头尾两个节点就行了，头节点加读锁，尾节点加写锁，使用的是不同的锁，因此读写时是不阻塞的；另外，由于节点是链表实例，在高并发下也会影响GC，而ArrayBlockingQueue是数组，不用包多一层  // take操作时上读锁，队列为空则等待，不为空时则移除并获取，再不为空则唤醒其他消费线程，解开读锁，允许中断 // 然后判断出消费元素之前队列是满的(此时是临界状态，但刚又被消费了一次)，则加写锁，唤醒其他生产线程，解写锁，不允许中断 private final ReentrantLock takeLock = new ReentrantLock(); // 读锁 private final Condition notEmpty = takeLock.newCondition(); // put操作时上写锁，队列满则等待，不满时则插入，插入后还不满则唤醒其他生产线程，解开写锁，允许中断 // 然后判断入队前只有一个元素(此时的该元素是刚刚加的)，则加读锁，唤醒其他消费线程，解读锁，不允许中断 private final ReentrantLock putLock = new ReentrantLock(); // 写锁 private final Condition notFull = putLock.newCondition();   其实ArrayBlockingQueue也可以像LinkedBlockingQueue使用读写锁，但此时的count和index就需要使用AtomicInteger来保证线程安全，加之数组不像链表那样还要构建节点，使用读写锁带来的收益不是特别大，所以没有使用了。\n参考 java阻塞队列详解\nAQS   AbstractQueuedSynchronized的缩写，全名：抽象队列同步器，基本是JUC包中各个同步组件的底层基础了\n  内置一个 volatile int state 记录同步状态；\n还有一个双端队列（遵循FIFO）用于存放资源堵塞的线程，被阻塞的线程加入队尾，队列头结点释放锁时，唤醒后面结点\n  AQS提供了一些同步组件，如下面提到的方法，使我们能制作自定义同步组件，公平锁和非公平锁是通过FairSync类和NonFairSync内部类实现的，独占式和共享式就是实现了这两个类的tryAcquire - tryRelease方法或tryAcquireShared-tryReleaseShared，像ReentrantLock是独占式就只实现了tryAcquire - tryRelease方法\n  AQS内部有两种队列，一种是同步队列，用于锁的获取和释放时使用；一种是等待队列，用于Condition类，每个Condition对应一个队列\n  AQS内控制线程阻塞和唤醒使用的是Unsafe类里native方法，如park、unpark方法\n  同步方式 独占式：如ReentrantLock 获取：\n 调用入口方法acquire 调用模版方法tryAcquire(arg)尝试获取锁，若成功则返回，若失败则走下一步 将当前线程构造成一个Node节点，并利用CAS将其加入到同步队列到尾部，然后该节点对应到线程进入自旋状态 自旋时，首先判断其前驱节点释放为头节点\u0026amp;是否成功获取同步状态，两个条件都成立，则将当前线程的节点设置为头节点，如果不是，则利用LockSupport.park(this)将当前线程挂起，不然它一直循环 ,等待被前驱节点唤醒(释放锁的时候)  释放：\n 调用入口方法release 调用模版方法tryRelease释放同步状态 获取当前节点的下一个节点 利用LockSupport.unpark(currentNode.next.thread)唤醒后继节点，之后重复上面 获取 第4步  共享式：如Semaphore，CountDownLatch 获取：\n 调用入口方法acquireShared 进入tryAcquireShared(arg)模版方法获取同步状态，如果返回值\u0026gt;=0，则说明同步状态(state)有剩余，获取锁成功直接返回 如果tryAcquireShared(arg)返回值\u0026lt;0，说明获取同步状态失败，向队列尾部添加一个共享类型的Node节点，随即该节点进入自旋状态 自旋时，首先检查 (前驱节点是否为头节点 \u0026amp; tryAcquireShared()是否\u0026gt;=0) (即成功获取同步状态) 如果是，则说明当前节点可执行，同时把当前节点设置为头节点，并且唤醒所有后继节点 如果否，则利用LockSupport.unpark(this)挂起当前线程，等待被前驱节点唤醒  释放：\n 调用releaseShared(arg)模版方法释放同步状态 如果释放成，则遍历整个队列，利用LockSupport.unpark(nextNode.thread)唤醒所有后继节点  区别   独占式每次释放锁只唤醒后继结点；共享式每次释放锁会唤醒所有后继结点，使它们同时获取同步状态\n  独占锁的同步状态值为1，同一时刻只能有一个线程成功获取同步状态；共享锁的同步状态\u0026gt;1，取值由上层同步组件确定\n  公平锁和非公平锁是分别重写tryAcquire()和tryRelease()方法实现，是两套方案，独占模式和共享模式都可以有公平锁非公平锁的实现\n公平锁，当线程请求到来时先会判断同步队列是否存在结点，如果存在先执行同步队列中的结点线程，当前线程将封装成node加入同步队列等待（新加入的线程获取锁，如果队列不为空，加入队尾，执行队头的线程）\n非公平锁，当线程请求到来时，不管同步队列是否存在线程结点，直接尝试获取同步状态，获取成功直接访问共享资源（新加入的线程获取锁，直接与队列中的头结点竞争，不成功在加入队列）\n  Condition 一个Lock可以创建多个Condition，每个Condition对应一个等待队列，单向，有头尾两个结点，遵循 FIFO\n当一个线程调用了await()相关的方法，那么该线程将会释放锁，并构建一个Node节点封装当前线程的相关信息加入到等待队列中进行等待，直到被唤醒、中断、超时才从队列中移出\n只有独占式才有Condition、等待队列这些东西，共享式没有\nawait()：将当前线程封装成结点加入到等待队列中，释放同步状态以唤醒同步队列的后继结点，判断当前结点是否在同步队列中，不是就挂起当前线程，此时仍然在自旋中，等待被唤醒\nsingal()：判断当前线程是否持有锁，没有则抛异常，有的话唤醒等待队列的头结点，如果为null则继续唤醒后面的结点，唤醒的意思是移除等待队列中的头结点，加入到同步队列中，如果该结点的前驱结点结束或者Node.SIGNAL状态设置失败，唤醒它，解除await()方法中的自旋，等待获取锁\n参考\n深入剖析基于并发AQS的重入锁(ReetrantLock)及其Condition实现原理\n(JDK)ReetrantLock手撕AQS\n从ReentrantLock的实现看AQS的原理及应用\n原子操作类 多线程下，java 自增自减操作是线程不安全的，因此JUC包才提供了线程安全的原子操作类\n原理 CAS + 自旋保证\nAtomicInteger源码分析——基于CAS的乐观锁实现\nReentrantLock 原理 ReentrantLock源码之一lock方法解析(锁的获取)\n无同步方案 CAS Compare-and-Swap, 是一种算法，CAS 指令需要有 3 个操作数，分别是内存地址 V、旧的预期值 A 和新值 B。当执行操作时，只有当 V 的值等于 A，才将 V 的值更新为 B。\nThreadLocal\u0026lt;T\u0026gt; 是java.lang包中的一个类，使用 ThreadLocal 维护变量时，其为每个使用该变量的线程提供独立的变量副本，把共享数据的可见范围限制在同一个线程之内，所以每一个线程都可以独立的改变自己的副本，而不会影响其他线程对应的副本。\n注意：ThreadLocal 无法解决共享对象的更新问题，ThreadLocal 对象建议使用 static修饰。这个变量是针对一个线程内所有操作共享的，所以设置为静态变量，所有此类实例共享此静态变量 ，也就是说在类第一次被使用时装载，只分配一块存储空间，所有此类的对象(只要是这个线程内定义的)都可以操控这个变量\n原理 每个线程内部都会维护一个 静态的ThreadLocalMap对象，该对象里有一个Entry（K-V 键值对）数组\nEntry的Key是一个ThreadLocal实例，Value是一个线程特有对象。之所以key是一个ThreadLocal实例，是因为在同一线程内可以有多个ThreadLocal，而一个线程只有一个ThreadLocalMap，所以以一个ThreadLocal对应一个要保存的value就比较合适了\nEntry的作用即是：为其属主线程建立起一个ThreadLocal实例与一个线程特有对象之间的对应关系；\nEntry对Key的引用是弱引用；Entry对 Value的引用是强引用；\nThreadLocalMap不同于HashMap，其解决哈希冲突不使用拉链法，而是使用线性探测法，当出现哈希冲突时，由于key是弱引用，还会有不同的逻辑进行插入\n每次对ThreadLocal做get、set操作时，以get方法为例，先通过Thread.currentThread()获得当前线程，在获取该线程对象里的ThreadLocalMap，以当前对象为key(当前线程的ThreadLoacl)在ThreadLocalMap中找到value，强转返回\nThreadLocal本身有探测式清理和启发式清理过期的key，但在一些场景 (尤其是使用线程池) 下，由于 ThreadLocal.ThreadLocalMap 的底层数据结构导致 ThreadLocal 有内存泄漏的情况，应该尽可能在每次使用 ThreadLocal 后手动调用 remove()，以避免出现 ThreadLocal 经典的内存泄漏甚至是造成自身业务混乱的风险，因为ThreadLocal里的key是弱引用，当释放掉对threadlocal对象的强引用后，map里面的value没有被回收，但却永远不会被访问到了\n详细的可参考threadlocalset方法源码详解\n参考 CyC2018/CS-Notes/并发\n过期的suspend()挂起、resume()继续执行线程\nReenTrantLock可重入锁（和synchronized的区别）总结\nReentrantLock锁和Synchronized锁的异同点\nJava程序员开发岗面试知识点解析\nJava并发包基石-AQS详解\nJava并发-AQS及各种Lock锁的原理\nJava并发之AQS详解\nJava并发编程JUC总结\n阿里巴巴Java开发手册1.40\nJava并发编程实战\n","date":"2019-03-02T00:00:00Z","permalink":"http://nixum.cc/p/java%E5%B9%B6%E5%8F%91/","title":"Java并发"},{"content":" git流程 \n常用命令   git init\n将一个普通文件夹变成git仓库，此时文件夹下多出.git文件夹，表示可以使用git管理，此时这个文件夹称为git工作区\n或者\n使用git clone url(github上的仓库链接)将仓库从github上下载下来\n  当对工作区内的文件做出修改后   git add 文件名\n表示将该文件的修改加入到暂存区\n  git add .\n(注意后面有个 . )表示将当前目录下的所有文件的修改都加入到暂存区\n  git commit -m \u0026ldquo;备注信息\u0026rdquo;\n表示将暂存区的修改提交到当前分支，提交之后暂存区清空\n  git push -u origin master\n将分支上的修改更新到github上\n  撤回修改   git log 查看提交记录，获取commit id\n  git reset \u0026ndash; 文件名 或者 commitId\n使用当前分支上的修改覆盖暂存区，用来撤销最后一次 git add files\n  git checkout \u0026ndash; 文件名\n使用暂存区的修改覆盖工作目录，用来撤销本地修改\n  删除   git rm 文件名\n删除暂存区和分支上的文件，同时工作区也不需要这个文件，之后commit保存到分支\n  git rm -r \u0026ndash;cached 文件夹名\n删除暂存区的修改，之后再commit保存到分支，如果不小心提交了不想提交的文件到分支上，此时想删除刚刚不小心提交的文件同时保留工作目录的文件时使用\n  更新   git pull origin master\n更新线上修改到本地分支\n  查看状态   git status\n可以查看本地和分支哪些文件有修改\n  几个容易混淆的命令  git rebase 和 git merge  效果都是合并，merge时可以看到被merge的分支以及其提交记录，不破坏被merge分支的代码提交记录；rebase是直接把两个分支合并，被合并的分支就像在原来的分支上开发一样，不会新增提交记录到目标分支，形成线性提交的历史记录。\n合并代码到公共分支的时候使用git merge\n合并代码到个人/功能分支时使用git rebase\n git pull 和 git fetch  效果都是更新远程仓库到本地，fetch是将远程仓库的内容拉到本地，用户检查只会再决定是否合并；pull是将远程仓库的内容拉到本地后直接合并，pull = fetch + merge\n git reset 和 git revert  效果都是撤销，reset回退版本会丢失回退前的提交信息，revert则不会，revert会重新创建一个新的提交，新的提交内容是指定的commitId对应的内容，原有的提交得到保留。\n其他常用命令  git命令 \n参考 廖雪峰 git教程\n","date":"2019-02-26T00:00:00Z","permalink":"http://nixum.cc/p/git/","title":"git"},{"content":"[TOC]\n以下笔记如没指定版本，都是基于JDK1.8\nCollection  javaCollection类图简版 \nSet HashSet 1.基本   底层是HashMap，因此初始容量，默认负载因子、扩容倍数这些都和HashMap一样\n  由于HashSet只需要key，因此value统一使用静态不可变的Object对象来装，即所有key共享这一个对象\n  1 2  private transient HashMap\u0026lt;E,Object\u0026gt; map; private static final Object PRESENT = new Object();    HashSet允许存入null 不是线程安全的 不保证插入元素的顺序  List ArrayList 1.基本   底层：Object数组\n  默认大小：10 （调用空参构造方法时）\n最大是Integer.MAX_VALUE - 8（2^31 - 1，一些虚拟器需要在数组前加个头标签，所以减去 8 ）\n调用此构造方法时，\n1  public ArrayList(Collection\u0026lt;? extends E\u0026gt; c)   其中要注意的是，里面有这样一句话\n1 2 3  // c.toArray might (incorrectly) not return Object[] (see 6260652) if (elementData.getClass() != Object[].class) elementData = Arrays.copyOf(elementData, size, Object[].class);   如果传入的是 参数c 是由 ListasList = Arrays.asList(\u0026ldquo;aaa\u0026rdquo;,\u0026ldquo;bbb\u0026rdquo;); 这样来的，那么这个asList.toArray()是一个String[].class类，而不是Object[].class，原因是Arrays的asList()方法返回的是内部类Arrays\u0026amp;ArrayList，其toArray()方法与ArrayList的toArray()方法不一样\n  不是线程安全，因此一般在单线程中使用\n  允许存入null对象，size也会计入\n  每次扩容原来的 1.5 倍，如果扩容后仍然不够用，则采用满足够用时的数量\n  2.扩容 因为ArrayList是底层是数组，当add操作时会使用 ensureCapacityInternal() 方法保证容量，根据size与数组的长度进行比较，如果size比数组长度大时，使用 grow() 方法进行扩容\n在 grow(int minCapacity) 方法中的 int newCapacity = oldCapacity + (oldCapacity \u0026raquo; 1); 新容量的大小为原来的1.5倍，之后使用这个新容量，调用 Arrays.copyOf(elementData, newCapacity) 把原数组整个复制到新数组中，可以看到，复制的时候代价是很大的\n如果扩容之后的容量仍然不够，则将容量扩充至当前需要的数量\n比如 Listlist = new ArrayList\u0026lt;\u0026gt;();，此时ArrayList类里的size是0，但是Object数组长度是10\nArrayList类里：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  public boolean add(E e) { ensureCapacityInternal(size + 1); // Increments modCount!!  elementData[size++] = e; return true; } private void ensureCapacityInternal(int minCapacity) { if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) { minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity); } ensureExplicitCapacity(minCapacity); } private void ensureExplicitCapacity(int minCapacity) { // 记录数组修改(增加或删除或调整内部数组的大小)的次数 \tmodCount++; // 判断是否需要扩容  if (minCapacity - elementData.length \u0026gt; 0) grow(minCapacity); } // 扩容 private void grow(int minCapacity) { // overflow-conscious code \tint oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity \u0026gt;\u0026gt; 1);\t// 相当于 old + (old / 2) \tif (newCapacity - minCapacity \u0026lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE \u0026gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: \telementData = Arrays.copyOf(elementData, newCapacity); }   3.删除 删除指定下标的值时，调用 System.arraycopy() 将 index+1 后面的元素都复制到 index 位置上，不需要遍历数组，时间复杂度为 O(N)，可以看出，删除元素的代价也是非常高的。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  public E remove(int index) { rangeCheck(index); modCount++; E oldValue = elementData(index); int numMoved = size - index - 1; if (numMoved \u0026gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work  return oldValue; }   同理的remove(Object o)（但是需要遍历数组）、removeAll()都是差不多的操作\n4.读取和修改 修改：直接修改数组对应下标的值\n读取：直接获取数组对应下标的值\n注意到两个方法都有 checkForComodification(); 的判断，该方法的判断主要是提示并发修改异常\n1 2 3 4 5 6 7 8 9 10 11 12 13  public E set(int index, E e) { rangeCheck(index); checkForComodification(); E oldValue = ArrayList.this.elementData(offset + index); ArrayList.this.elementData[offset + index] = e; return oldValue; } public E get(int index) { rangeCheck(index); checkForComodification(); return ArrayList.this.elementData(offset + index); }   5.Fail-Fast 一种错误检查机制，因为ArrayList不是线程安全的，因此当多线程修改ArrayList对象时，或者在迭代操作中修改ArrayList对象或者序列化操作中，会进行Fail-Fast检查。\n检查的原理就是使用 全局变量 modCount 来检查，modCount 用来记录 ArrayList 内数组修改(增加、删除或调整内部数组的大小)的次数，当比较前后的modCount不一致时，抛出ConcurrentModificationException异常\n单线程中，如果需要在迭代中remove元素，应该使用迭代器迭代，并且使用迭代器提供的remove()（里面也是调用了ArrayList的remove方法，只是有修改modCount的值）方法，而不是ArrayList的remove方法，多线程的情况下只能使用线程安全的集合类了\n6.序列化 ArrayList的底层Object数组被 transient 修饰，该关键字声明数组默认不会被序列化\n之后通过重写writeObject() 和 readObject() 将数组里的元素取出，进行序列化\n之所以这么做是因为ArrayList的自动扩容机制，数组内元素实际数量可能会比数组长度小，如果一整个序列化的话会浪费空间，通过手动序列化的方式，只序列化实际存储的元素，而不是整个数组\n类实现Serializable接口，序列化时会使用 ObjectOutputStream 的 writeObject() 将对象转换为字节流并输出。而 writeObject() 方法在传入的对象存在 writeObject() 的时候会去反射调用该对象的 writeObject() 来实现序列化。\n7.与Vector的区别  Vector实现方式跟ArrayList类似，也是基于数组实现，默认大小也是10，但它是线程安全的 Vecotr扩容是扩容到原来的2倍，ArrayList是1.5倍 Vecotr的线程安全是因为其方法都使用了 synchronized 关键字进行修饰同步，效率比ArrayList差，这也是不推荐使用Vector的原因，保证线程安全可以使用 juc 包里其他类 当需要线程安全的ArrayList时，不使用Vector，而是使用 CopyOnWriteArrayList 类 或者 Collections.synchronizedList(Listlist);  LinkedList 1.基本  底层是链表，且是双向链表 采用链表，因此插入、删除效率高(但需要知道被删除节点)，查找效率低，不支持随机查找，而ArrayList底层是数组，因此支持随机查找，查找效率高，但是插入，删除效率低 LinkedList底层是双向链表的缘故，可以当成队列使用，创建队列Queue或者Deque时，采用LinkedList作为实现 不是线程安全的  CopyOnWriteArrayList 1.基本  线程安全的ArrayList，基本使用同ArrayList 底层是Object数组，但是有 volatile 修饰，还有一把可重入锁 ReentrantLock 保证线程安全 保证线程安全的原理是 读写分离，读的时候使用 全局变量里的Object数组，每次写的时候加锁，在方法里创建一个新数组，将全局变量里的Object数组 通过Arrays.copyOf复制 给方法里的新数组，之后进行写操作，完了再把全局变量的Object数组指向新数组。由于存在复制操作，因此add()、set()、remove()的开销很大 读操作不能读取实时性的数据，因为写操作的数据可能还未同步 适合读多写少的场景  Map  Map类图 \nHashMap 1.基本   底层：一个Node类型的数组，而1.7的是Entry 类型的，不过基本也差不多\n1  transient Node\u0026lt;K,V\u0026gt;[] table;   Node类是HashMap里的一个内部类\n1 2 3 4 5 6  static class Node\u0026lt;K,V\u0026gt; implements Map.Entry\u0026lt;K,V\u0026gt; { final int hash;\t// key的hash码 \tfinal K key; V value; Node\u0026lt;K,V\u0026gt; next;\t// 对应的链表 }     数组中的每个位置称为一个桶，一个桶存放一条链表，同一个桶的HashCode一样\n  默认容量是16，最大容量是2^30，即数组长度\n默认的填充因子是0.75\n桶的个数，即数组的长度总是 2的n次幂\n当桶上链表的结点数大于 8 时，链表转红黑树，小于 6 时，红黑树转链表\n桶的个数，即数组长度少于 64 时，先扩容，大于64时，链表转红黑树，也就是说，它是先检查链表结点数，如果大于8，再检查数组长度，再决定是否转红黑树\n    两个变量：\n1 2 3 4  // 临界值,衡量数组是否需要扩增的一个标准，threshold=容量x填充因子，键值对个数超过这个阈值，扩容2倍 int threshold; // 填充因子,控制数组存放数据的疏密程度 final float loadFactor;      每次扩容为原来的 2 倍 允许key=null，此时将该元素放入Node类型数组下标为0处 不是线程安全的，支持fast-fail机制 不保证插入顺序   基本结构：  1.8之前，数组的每一个元素中存放一条链表：\n 1.8之前的HashMap \n1.8，数组中的每一个元素存放一条链表，当链表的长度超过8（默认）之后，将链表转换成红黑树，以减少搜索时间\n 1.8HashMap \n2.创建过程   构造方法：除留余数法\n  哈希冲突解决方法：拉链法\n  1  Map\u0026lt;String, String\u0026gt; map = new HashMap\u0026lt;\u0026gt;();   首先创建一个Map，调用空参构造方法，此时只是设置了默认的填充因子，Node数组还没有初始化；除了public HashMap(Map\u0026lt;? extends K, ? extends V\u0026gt; m) 这个构造方法外，其他的构造方法基本都是只设置了填充因子和容量，并没有对数组初始化，当第一次调用put方法时，才对数组进行内存分配，完成初始化，延迟加载\n其中一个可以指定容量和填充因子的构造方法中，\n1  public HashMap(int initialCapacity, float loadFactor)   如果传入的容量不是 2的n次幂，HashMap总会保证其数组的容量为2的n次幂，使用下面方法，原因是为了加快取模的运算速度，具体解释看下一节\n1 2 3 4 5 6 7 8 9 10  // 用于保证容量为2的n次，原理是求出一个数的掩码 + 1，即可得到大于该数的最小2的n次 static final int tableSizeFor(int cap) { int n = cap - 1; n |= n \u0026gt;\u0026gt;\u0026gt; 1; n |= n \u0026gt;\u0026gt;\u0026gt; 2; n |= n \u0026gt;\u0026gt;\u0026gt; 4; n |= n \u0026gt;\u0026gt;\u0026gt; 8; n |= n \u0026gt;\u0026gt;\u0026gt; 16; return (n \u0026lt; 0) ? 1 : (n \u0026gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1; }   当第一次调用put方法时，对数组(桶)进行判断，如果为空，则为数组分配内存，完成初始化，之后再根据key的情况，加入到HashMap中\n1  map.put(\u0026#34;key111\u0026#34;, \u0026#34;value111\u0026#34;);   对put方法的解析具体请看集合框架源码学习之HashMap(JDK1.8)\n3.确定键值对所在的桶的下标 当向map中put进 key-value 时，对key采用哈希 + 除留余数法，确定该键值对所在的桶的下标\n首先计算key的hash值，之后对数组的容量取模，得到的余数就是桶的下标\nJDK1.7\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  final int hash(Object k) { int h = hashSeed;\t// hashSeed的值受容量和resize方法而定  if (0 != h \u0026amp;\u0026amp; k instanceof String) { return sun.misc.Hashing.stringHash32((String) k); } h ^= k.hashCode(); h ^= (h \u0026gt;\u0026gt;\u0026gt; 20) ^ (h \u0026gt;\u0026gt;\u0026gt; 12); return h ^ (h \u0026gt;\u0026gt;\u0026gt; 7) ^ (h \u0026gt;\u0026gt;\u0026gt; 4); } static int indexFor(int h, int length) { // assert Integer.bitCount(length) == 1 : \u0026#34;length must be a non-zero power of 2\u0026#34;;  return h \u0026amp; (length-1); }   但是这里并不直接采用取模%操作， 而是使用了位运算，因为位运算的代价比求模运算小的多，因此在进行这种计算时用位运算的话能带来更高的性能\n而取模(%)操作中如果除数是2的幂次则等价于与其除数减一的与(\u0026amp;)操作，这也解释了为什么容量每次都是2的n次幂\n在JDK1.8中，此操作被简化了，也取消了indexFor(int h, int length)方法，确定桶下标直接 hash \u0026amp; (length-1)\n1 2 3 4  static final int hash(Object key) { int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h \u0026gt;\u0026gt;\u0026gt; 16);\t// ^ 按位异或 }   当计算得到该键值对所在桶的下标后，判断该桶是否为空，如果为空，将此键值对插入到桶中\n当容量未达到阈值，且容量未变，此时出现新键值对，其key计算得到的桶的下标所在位置已经有结点了，则通过拉链法来解决冲突\n先判断新的键值对的key的hash值与头结点的key的hash值是否一致，且key是否相等，如果这两项均满足，则说明该新的key与头结点的一样，将value覆盖掉原来的结点，如果不相等，则\n在JDK1.7中，使用头插法加入链表，在多线程中，头插法可能会导致hashMap扩容时造成死循环，因此在1.8修复这个问题了\n在JDK1.8中，使用尾插法加入链表，当然还有一系列判断是否需要将链表转换为红黑树，具体条件看上面已说明\n4.扩容 当加入到HashMap中的元素越来越多时，碰撞的概率也越来越高，链表的长度变长，查找效率降低，此时就需要对数组进行扩容。HashMap根据键值对的数量，来调整数组长度，保证查找效率\n在两种情况下，HashMap会进行扩容\n 当put操作时，数组大小超过阈值 threshold（容量 x 负载因子），JDK1.7、1.8都是  1  final Node\u0026lt;K,V\u0026gt;[] resize()；   扩容时，将扩容到原来的2倍（newThr = oldThr \u0026laquo; 1），之后进行一次重新的hash分配。\n创建新数组，遍历整个旧的hashMap，重新计算每个键值对的hash值以及对应的桶下标，扩展后Node对象的位置要么在原位置，要么移动到原偏移量两倍的位置，之后将旧hashMap中的结点加入到新的数组中\nHashMap 使用了一个特殊的机制，降低重新计算桶下标的操作。\n假设原数组长度 capacity 为 16，扩容之后 new capacity 为 32：\n1 2  capacity : 00010000 new capacity : 00100000   对于一个 Key，\n 它的哈希值如果在第 5 位上为 0，那么取模得到的结果和之前一样； 如果为 1，那么得到的结果为原来的结果 +16。  5.与HashTable的区别  HashMap不是线程安全的，HashTable是线程安全的(因为其方法使用了synchronized修饰来保证同步，效率较差) HashMap允许插入key为null的键值对，而HashTable不允许 如果不指定容量，HashMap默认容量是16，每次扩容为原来的 2 倍；HashTable默认容量是11，每次扩容为原来的 2n+1 倍 jdk1.8后对HashMap链表的改变，HashTable没有  ConcurrentHashMap 1.基本  线程安全的HashMap，使用方法同HashMap 不允许存入key为null的键值对  2.线程安全的底层原理，JDK1.7和JDK1.8的实现不一样   JDK1.7\n  JDK1.7 采用分段锁（segment）机制，每个分段锁维护几个桶，多个线程可以同时访问不同分段锁上的桶，并发度指segment的个数\n  默认的分段锁是16，segment的数量一经指定就不会再扩了；每个segment里面的HashEntry数组的最小容量是2，每次扩容 2 倍\n  也是延迟初始化，当第一次put的时候，执行第一次hash取模定位segment的位置，如果segment没有初始化，因为put可能出现并发操作，则通过CAS赋值初始化，之后执行第二次hash取模定位HashEntry数组的位置，通过继承 ReentrantLock的tryLock() 方法尝试去获取锁，如果获取成功就直接插入相应的位置，如果已经有线程获取该segment的锁，那当前线程会以自旋的方式去继续的调用 tryLock() 方法去获取锁，超过指定次数就挂起，等待唤醒\n  size（统计键值对数量）操作：因为存在并发的缘故，size的可能随时会变，ConcurrentHashMap采用的做法是先采用不加锁的模式，尝试计算size的大小，比较前后两次计算的结果，结果一致就认为当前没有元素加入，计算的结果是准确的，尝试次数是 3 次，如果超过了 3 次，则会对segment加锁，锁住对segment的操作，之后再统计个数\n     1.7concurrentHashMap \n JDK1.8  采用CAS和synchronized来保证并发安全，更接近HashMap synchronized只锁定当前链表或红黑二叉树的首节点，这样只要hash不冲突，就不会产生并发 扩容的时候由于只有一个table数组，将在多线程下各个线程都会帮忙扩容，加快扩容速度 size操作：在扩容和addCount()方法就进行处理，而不像1.7那样要等调用的时候才计算    红黑树的特性: （1）每个节点要么是黑色，要么是红色。 （2）根节点是黑色。 （3）每个叶子节点（NIL）是黑色。（注意：这里叶子节点，是指为空(NIL或NULL)的叶子节点） （4）如果一个节点是红色的，则它的子节点必须是黑色的。 （5）从一个节点到该节点的子孙节点的所有路径上包含相同数目的黑节点。\n是一种二叉平衡树，但是平衡不是非常严格，因此结点的旋转次数少，插入、删除效率高，插入、删除、搜索都是时间复杂度都是O(log2 n)\n插入和删除时，先理解为二叉搜索树的插入和删除，然后再进行变色，优先变色，变色后仍无法达到上面那五个特性才开始旋转，来达到上面的特性，插入时节点一般是红色\n参考：红黑树插入和删除\n使用Stream处理集合 Java 8 中的 Streams API 详解\n参考 Java：集合，Collection接口框架图\nCyC2018/CS-Notes/java容器.md\nc.toArray might not return Object[]?\nJava集合\u0026mdash;ArrayList的实现原理\n集合框架源码学习之HashMap(JDK1.8)\nJDK1.8 HashMap源码分析\n【JUC】JDK1.8源码分析之CopyOnWriteArrayList（六）\n红黑树(一)之 原理和算法详细介绍\n","date":"2019-02-18T00:00:00Z","permalink":"http://nixum.cc/p/java-list-map/","title":"Java List Map"},{"content":"[TOC]\n一、面向对象 面向对象的特征： 抽象(注意与当前目标有关的，选择一部分，暂时不用部分细节，分为过程抽象、数据抽象) 继承：联结类的层次模型、允许和鼓励类的重用，派生类可以从它的基类那里继承方法和实例变量，进行修改和新增使其更适合 封装：封装是把过程和数据包围起来，对数据的访问只能通过已定义的界面，这些对象通过一个受保护的接口访问其他对象 多态：允许不同类的对象对同一消息作出响应，包括参数化多态性和包含多态性，灵活、抽象、行为共享、代码共享，解决程序函数同名问题\n二、基础类型及其包装类型    基本类型 boolean byte char short int float long double     包装类型 Boolean Byte Character Short Integer Float Long Double   位数 1 8 16 16 32 32 64 64   字节数  1 2 2 4 4 8 8      字符集\nunicode是字符集，一种标准，UTF-8、UTF-16、GBK之类的是编码方式，是字符集的具体实现\nUTF-16：定长,固定2字节， UTF-8：变长,中文占3字节,英文占1字节\nchar可以保存一个中文字符\njava中采用unicode编码，无论中文、英文都是占2个字节\njava虚拟机中使用UTF-16编码方式\njava的字节码文件(.class)文件采用的是UTF-8编码，但是在java 运行时会使用UTF-16编码。\n参考Java中的UTF-8、UTF-16编码字符所占字节数\n  自动转换的顺序，\n由高到低：btye, short, char(这三个之间无法自动转换，只能强转) \u0026mdash;\u0026gt; int \u0026mdash;\u0026gt; long \u0026mdash;\u0026gt; float \u0026mdash;\u0026gt; double\n  为什么 占8字节的long 转占 4字节float 不需要强转转化？\n因为底层实现方式不同，浮点数在内存中的32位不是简单地转化为十进制，而是通过公式计算得到，最大值要比long的范围大\n  装箱和拆箱 以 int 和 Integer 为例\n  装箱的时候自动调用的是 Integer 的 valueOf(int) 方法，Integer a = 1; 就会触发调用valueOf(int)方法\n拆箱的时候自动调用的是Integer的 intValue() 方法\nparseInt(\u0026quot;\u0026quot;)方法是将字符串转化为基本类型\n  其中，装箱时，即调用 valueOf() 方法时，会先去缓存池里找，看看该值是否在缓存池的范围中，如果是，多次调用时会取得同一对象的引用，属于同一对象，如果不在缓存池中，则使用new Integer()\n使用new Integer()初始化的，无论传入的数是否在缓存池的范围内，都是重新分配内存初始化的，属于不同对象\n1 2 3 4 5 6 7 8  Integer a = 23; Integer b = 23; System.out.println(a == b); // true  Integer a1 = new Integer(23); Integer b1 = new Integer(23); System.out.println(a1 == b1); // false  Integer c = 128; Integer d = 128; System.out.println(c == d); // false     基本类型缓存池，其他类型没有\nboolean：true、false\nshort ： -128 ~127\nint ： -128 ~127\nchar ： \\u0000 ~ \\u007F\n  equals 和 == 对于 装箱和拆箱\n  == ：比较的是两个包装类型的，同包装类型，比较两者的引用，判断是否指向同一对象；不同包装类型用==比较会出现编译错误\n比较时两者都是基本类型，无论类型是否一样，都是比较数值\n比较时一个是包装类型，一个是基本类型，无论值的范围是否在缓存池内，则将自动拆箱，比较基本类型\n比较时有算术运算，自动拆箱，运算，比较数值\n  equals：看具体类型的equals方法，一般先比较类型，类型不一样直接返回false，类型一样再比较数值\n比较时传入基本类型，会进行装箱，之后进行equals比较\n比较时有算术运算，自动拆箱，运算，之后根据运算完后的类型再装箱（可能会向上转型），之后再进行equals比较\n    运算与转型  从低位类型到高位类型自动转换；从高位类型到低位类型需要强制类型转换 算术运算中，基本就是先转换为高位数据类型，再参加运算，结果也是最高位的数据类型 short、byte、char计算时都会提升为int 采用 +=、*= 等缩略形式的运算符，系统会自动强制将运算结果转换为目标变量的类型 当运算符为自动递增运算符（++）或自动递减运算符（\u0026ndash;）时，如果操作数为 byte，short 或 char类型不发生改变 被final修饰的变量不会自动改变类型，当2个final修饰相操作时，结果会根据左边变量的类型自动转化 三元运算符？，：两边是基本类型会做自动类型提升  1 2 3 4 5 6  byte a = 127, b = 127, d; final byte c = 127; a += b; a -= b; //这种写法是可以的，会变成 a = (byte) (a+b) 而 a = a + b； a = a - b；则会因为没有类型转化而出错，int无法转成byte d = a + c; // 会出错，a在运算时为自动提升为int 如果是final修饰 a、b，那么byte c = a + b 就不会编译错误   三、String 1.基本   底层：private final char value[];\nvalue数组被final修饰，因此当它初始化之后就不能再引用其它数组\nString 内部没有改变 value 数组的方法，因此可以保证 String 不可变\nString类的方法都不是在原来的字符串上进行操作，而是重新生成新的字符数组\n  类本身被 final 修饰，使它不可被继承\n  String类的 “+\u0026quot; 本质是使用StringBuilder的append方法，最终返回new的string\n  JDK9之前底层使用char数组，JDK9底层改为使用byte数组+coder标识符，主要是因为有些字符集并不需要使用到2个字节的char，不需要用到2个字节的char\n  2.不可变的好处   可以缓存 hash 值\n因为 String 的 hash 值经常被使用，例如 String 用做 HashMap 的 key。不可变的特性可以使得 hash 值也不可变，因此只需要进行一次计算。\n  String Pool 的需要\n如果一个 String 对象已经被创建过了，那么就会从 String Pool 中取得引用。只有 String 是不可变的，才可能使用 String Pool。\n  安全\n  String 经常作为参数，String 不可变性可以保证参数不可变。例如在作为网络连接参数的情况下如果 String 是可变的，那么在网络连接过程中，String 被改变，改变 String 对象的那一方以为现在连接的是其它主机，而实际情况却不一定是。\n  不可变得特性使得它天生是线程安全的\n    3.字符串常量池（String Pool）  一般用于缓存字符串字面量和符号引用  String的intern方法可以把相应的字符串缓存起来，保存在永久代中，但是一般永久代空间有限，且只有FullGC才会进行垃圾回收（JDK8后移永久代移到了元空间中，默认大小也增加了），所以需要谨慎使用该方法\n4.StringBuffer和StringBuilder StringBuffer是线程安全的，因为大部分方法都用了synchronized关键字修饰，而StringBuilder没有，所以是非线程安全的，两者在功能上是等价的，性能上，StringBuilder比StringBuffer好。\n3和4具体参考深入理解Java中的String，这篇文章写得相当详细了\n四、equals()、hashCode()、clone() equals()  在不重写的情况下，Object类下的 equals() 方法比较的是两个对象的引用，即判断两个对象是否是同一个对象，此时等价于 == 重写的情况下，看具体类重写后的equals方法，像String类的 equals 方法是先判断是否是String类型，再比较字符的内容  hashCode()   作用是返回对象的int类型的哈希码，一般用于当索引，例如，在HashMap里，加入一对键值对，HashMap会先计算key的哈希值，取模，找到对应桶的下标\n  hashCode()一般会和equals()有联系，例如，HashMap在找到要加入的键值对所在对应的桶，桶内的键值对的哈希码肯定是一样，为了判断该键值对是否重复出现，将使用key的equals进行比较\n为什么有了equals()还要hashCode方法，有了hashCode方法还要equals方法？\n 因为equals方法的实现比较复杂，效率较低，hashCode只需计算hash值就能进行对比，效率高 hashCode方法不一定可靠，不同对象生成的hashCode可能一样，所以需要equals方法  hashCode() 与 equals() 的相关规定\n 如果两个对象相等，则hashcode一定也是相同的 两个对象相等,对两个对象分别调用equals方法都返回true 两个对象有相同的hashcode值，它们也不一定是相等的 因此，equals 方法被覆盖过，则 hashCode 方法也必须被覆盖 hashCode() 的默认行为是对堆上的对象产生独特值。如果没有重写 hashCode()，则该 class 的两个对象无论如何都不会相等（即使这两个对象指向相同的数据）    clone()   clone分为深拷贝和浅拷贝\n  浅拷贝：对象的属性是基本类型的，直接复制一份；属性是引用类型的（如数组、对象），复制引用，也就是拷贝过后的两个对象里引用类型的属性，都是指向同一个对象\nObject类里的clone方法就是浅拷贝\n  深拷贝：对象里的属性无论是基本类型还是引用类型，都是重新复制一份，即会为拷贝对象里的引用属性重新开辟内存空间，不再是和被拷贝对象指向同一个对象\n    Object里的clone()方法被protected 修饰，一般类如果不重写的话是调用不了的，如果要重写的话，需要实现Cloneable接口，不然会抛出CloneNotSupportedException异常，之后可以通过super.clone()调用Object类中的原clone方法\n  关键字 final和static  final关键字主要用在三个地方：变量、方法、类。   修饰变量，如果是基本数据类型的变量，则其数值一旦在初始化之后便不能更改；如果是引用类型的变量，则在对其初始化之后便不能再让其指向另一个对象，final修饰的变量一定要初始化，要不就直接初始化，不然就需要在每个构造方法里初始化，或者在代码块里初始化，代码块里的初始化和构造方法里的初始化只能二选一，否则会造成赋值重复 修饰类，表明这个类不能被继承，final类中的所有成员方法都会被隐式地指定为final方法 修饰方法，表明该方法不能被子类重写   声明为static成员不能被序列化  instanceof  是关键字，也是运算符 用于判断一个对象是否是 一个类的实例，一个类的子类，一个接口的实现类  抽象类和接口   抽象类可以有构造方法（但不能实例化），接口没有\n  抽象类可以有普通成员变量、静态变量或常量，访问类型任意，接口只有静态常量，且默认被public static final修饰\n  抽象类可以包含非抽象的普通方法，接口中的方法必须是抽象的，不能有非抽象方法\n  抽象类中的抽象方法访问类型可以是public、protected和不写，接口中的方法默认是public abstract\n  抽象类中可以包含静态方法(可以调用)，接口不行\n  JDK1.8之前接口中的方法不能有方法体，且不能被default修饰，但是可以不写访问修饰符，\n1.8之后（包含1.8）接口中的方法可以有方法体，表示默认方法体，但需要用default修饰\n  JDK1.8之前接口中的方法不能用static修饰，1.8之后（包含1.8）才可以使用static修饰，才可以通过接口名调用静态方法\n  继承 重写与重载   重写（Override）\n继承中，子类重写父类方法\n 子类方法的访问权限必须大于等于父类方法； 子类方法的返回类型必须是父类方法返回类型或为其子类型。 子类抛出的异常比父类的小或者相等  关于静态方法重写\nstatic方法不能被子类重写，子类如果定义了和父类完全相同的static方法，Son.staticmethod()或new Son().staticmethod()都是调用子类的，如果是Father.staticmethod()或Father f = new Son(); f.staticmethod()调用的是父类的\n  重载（Overload）\n存在于同一个类中，指一个方法与已经存在的方法名称上相同，但是参数类型、个数、顺序至少有一个不同\n注意，返回值不同，其它都相同不算是重载。\n  初始化顺序 （括号内的按出现先后进行初始化）\n 父类（静态变量、静态代码块） 子类（静态变量、静态代码块） 父类（实例变量、普通代码块） 父类（构造函数） 子类（实例变量、普通代码块） 子类（构造函数）  在构造方法中，this()和super()调用构造方法时，只能放在第一行，且不能同时出现\n内部类 基础   内部类是指在一个外部类的内部再定义一个类。内部类作为外部类的一个成员，并且依附于外部类而存在的。\n  内部类可为静态，可用protected和private修饰（而外部类只能使用public和缺省的包访问权限）。\n  内部类主要有以下几类：成员内部类、局部内部类、静态内部类、匿名内部类\n  成员内部类   在外部类里想调用内部类里的方法和变量，只能通过new的形式创建内部类实例才可以使用\n  内部类调用外部类的方法，无论静态非静态，直接调用\n  内部类里的成员可以和外部类的成员的名字相同，直接使用的时候调用的是内部的，内部类调用外部类同名变量需要加上外部类名，如果没有同名变量就可以直接使用，不需要加外部类名\n  在其他地方想要调用有内部类的类的内部类方法，需要实例化一个外部类，再使用外部类的实例变量实例化内部类\n  可以被abstarct修饰，但无法实例化\n  不能有静态成员、方法\n1 2 3  Outer out = new Outer(); Outer.Inner outin = out.new Inner(); outin.inner_f1();     局部内部类  局部内部类调用外部类的方法，直接调用 外部类调用内部类方法，初始化外部类实例，调用有局部内部类的方法 局部内部类 可以看成是 方法里的成员变量，只能在该方法里实例化 局部内部类 对于同名变量的访问方式同成员内部类 方法外的外部类的成员变量可以直接访问，但只能访问内部类所在方法里的final修饰的变量 不能被static还有访问控制符修饰，可以被abstract、final修饰  静态内部类   静态内部类中可以定义静态或者非静态的成员或方法\n  静态内部类只能访问外部类的静态成员，不能访问外部类的非静态的成员\n  外部类方法访问内部类静态成员，直接 内部类名.静态成员变量\n访问内部非静态成员，实例化内部类，在调用\n说白了，静态属于整个类的，不属于某个对象的，可以直接使用，不依赖外部类，可以直接调用，或者实例化外部类里的静态类\n1 2 3 4 5  Outer.Inner outin = new Outer.Inner(); outin.staticInner_f1();\t// 或者直接调用 Outer.Inner.staticInner_f1() outin.inner_f1();\t// 不能直接 Outer.Inner.inner_f1() System.out.print(outin.staticField + outin.field); // 同理只能 Outer.Inner.staticField, 不能Outer.Inner.field；     不可以只实例化外部类，再使用这个实例去实例化内部类或者调用内部类里的成员，这点跟成员内部类是不一样的\n  匿名内部类  匿名内部类不能有构造方法 无法被访问控制符、static修饰 匿名内部类不能定义任何静态成员、方法和类  匿名都没类名了，构造方法，静态成员之类的没办法调用了\n参考深入理解java内部类\n反射和内省  反射：可以在运行时动态获取类信息或者动态调用类方法；JVM运行的时候，读入类的字节码到 JVM 中，对该类的属性、方法、构造方法进行获取和调用  类加载一次之后会在JVM中缓存，但是如果是不同的类加载器去加载同一个class则会多次加载。\n两个类相等需要类本身相等，并且使用同一个类加载器进行加载。这是因为每一个类加载器都拥有一个独立的类名称空间。\n这里的相等，包括类的 Class 对象的 equals() 方法、isAssignableFrom() 方法、isInstance() 方法的返回结果为 true，也包括使用 instanceof 关键字做对象所属关系判定结果为 true。\n反射慢是因为Java是静态语言，如果在JVM运行时才进行加载，进行参数和方法的解析，此时的JVM无法对反射加载的类进行优化，还有就是类加载需要经过验证，判断是否对JVM有害，反射加载验证会比平时在装载期的时间长，但是总的来说影响不大\n 内省：针对JavaBean，只能对Bean的属性进行操作；加载类，得到它的属性，对属性进行get/set，是对反射的一层封装  枚举   使用enum定义的枚举类默认继承了java.lang.Enum，而不是继承Object类\n  枚举类可以实现一个或多个接口\n  使用enum定义、非抽象的枚举类默认使用final修饰，不可以被继承，定义的Enum类默认被final修饰，无法被其他类继承\n  枚举类的所有实例都必须放在第一行展示，不需使用new 关键字，不需显式调用构造器。\n自动添加public static final修饰\n  枚举类的构造器只能是私有的\n  枚举类也能定义属性和方法，可以是静态和非静态的\n  参考 java浅谈枚举类\n具体例子\n反编译之后会发现，SPRING、SUMMER、FALL这些是静态常量(public static final 修饰)，而且是在静态代码块里初始化的，同时还附带有public static Season[] values()方法和public static Season valueOf(String s)方法\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38  public enum Season{ // 调用无参构造器、括号里的变量称为自定义变量，可以有多个，要跟构造方法对应  SPRING(), // 调用有参构造器  SUMMER(\u0026#34;夏天\u0026#34;), // 默认调用无参构造器  FALL； private String name; // 默认是private的  Season() {} private Season(String name) { this.name = name; } // ----------------------------------------------------  // 如果在enum中定义了抽象方法,每个实例都要重写该方法  public abstract String whatSeason(); // 调用无参构造器  SPRING() { // 方法无法调用enum类里的非静态变量，只能调用非静态变量  @Override public String whatSeason(){return \u0026#34;chun\u0026#34;;} }, // 调用有参构造器  SUMMER(\u0026#34;夏天\u0026#34;) { @Override public String whatSeason(){return \u0026#34;xia\u0026#34;;} }, // 默认调用无参构造器  FALL { @Override public String whatSeason(){return \u0026#34;qiu\u0026#34;;} }; }   原理参考深入理解java类型\n泛型   泛型会类型擦除，那它如何保证类型的正确？\njava编译器是通过先检查代码中泛型的类型，然后再进行类型擦除，在进行编译的\n只能在编译期保证类型相同\n  泛型被擦除，统一使用原始类型Object，泛型类型变量最后都会被替换为原始类型，那为什么我们使用的时候不需要强转转换?\n比如ArrayList，它会帮我们进行强转转换，它会做了一个checkcast操作，检查什么类型，之后进行强转\n  类型擦除与多态导致冲突，如何解决\n比如本意是进行重写，实现多态。可是类型擦除后，只能变为了重载。这样，类型擦除就和多态有了冲突，JVM采用桥方法解决此问题\n  泛型擦除的优点\n类型安全，编译器会帮我们检查、消除强制类型转换，提高代码可读性、为未来版本的 JVM 的优化带来可能\n  List\u0026lt;?\u0026gt;、List、List、List\u0026lt;? super T\u0026gt;、List\u0026lt;? extends T\u0026gt;的区别\nList\u0026lt;? super T\u0026gt;：T的父类，包括T，表示范围\nList\u0026lt;? extends T\u0026gt;：T的子类，包括T，表示范围\nList\u0026lt;?\u0026gt; 表示任意类型，如果没明确，就是Object或者任意类，与List、List一样，表示 点\nList 也可表示范围\n  参考 10 道 Java 泛型面试题\n参考java泛型（二）、泛型的内部原理：类型擦除以及类型擦除带来的问题\n注解 序列化和反序列化 序列化：将对象转换为字节序列，用于将保存在JVM内存中的对象持久化，保存到文件或者网络传输\n反序列化：将字节序列还原成对象\n与JSON的比较，两者都可以用于网络传输，JSON更使用web方面、应用方面，易读，需要将JSON解析才能还原成对象，而序列化反序列化是JAVA提供的，由JVM来还原，应用范围会更广一些\n除此之外也有其他协议，如基于XML文本协议(基于SOAP规范)：WebService、Burlap，二进制协议：Hessian，Hessian生成的字节流简凑、跨平台、高性能比JDK的序列化反序列化优秀\n参考 深入剖析Java中的装箱和拆箱 CyC2018/CS-Notes/java基础.md 深入理解Java中的String\n","date":"2019-02-18T00:00:00Z","permalink":"http://nixum.cc/p/java-se/","title":"Java SE"},{"content":"SpringMVC工作原理 SpringMVC工作原理详解\n流程图看链接里的即可\n简单来说各个组件的作用\n 前端控制器DispatcherServlet：请求的入口，可以看成是中央处理器、转发器，负责调度其他组件，接收请求，完成响应 处理器映射器HandlerMapping：根据请求的url查找Handler，找到url对应的controller类，返回一条执行链，其中就包含拦截器和处理器（具体的controller类）；有配置文件方式，实现接口方式，注解方式等方式实现映射 处理器适配器HandlerAdapter：HandlerMapping找到对应的controller类后，再根据url找到对应的执行方法 处理器Handler：具体的处理方法，也就是我们所写具体的Controller类 视图解析器View resolver：根据逻辑View名称，找到对应的View，根据处理器返回的ModelAndView，将数据渲染到View上 视图View：例如jsp，freemarker之类的视图模板  拦截器在什么时候执行？\n拦截器，是属于HandlerMapping级别的，可以有多个HandlerMapping ，每个HandlerMapping可以有自己的拦截器，拦截器可以设置优先级。一个请求交给一个HandlerMapping时，这个HandlerMapping先找有没有处理器来处理这个请求，如何找到了，就执行拦截器，执行完拦截后，交给目标处理器。如果没有找到处理器，那么这个拦截器就不会被执行。\n实现HandlerInterceptor接口或者继承HandlerInterceptor，重写boolean preHandle()、void postHandle()、void afterCompletion()方法\n  preHandle() 方法：该方法会在控制器方法前执行，其返回值表示是否中断后续操作。\n当其返回值为true时，表示继续向下执行；当其返回值为false时，会中断后续的所有操作（包括调用下一个拦截器和控制器类中的方法执行等）。\n  postHandle()方法：该方法会在控制器方法调用之后，且解析视图之前执行。可以通过此方法对请求域中的模型和视图做出进一步的修改。\n  afterCompletion()方法：该方法会在整个请求完成，即视图渲染结束之后执行。可以通过此方法实现一些资源清理、记录日志信息等工作。\n  Spring Security 简单工作流程\n请求(包含用户名，密码之类)——\u0026gt;登陆信息封装成一个Authentication对象——\u0026gt;AuthenticationManager，调用authenticate ()方法处理——\u0026gt;该方法会将对象传递给一系列AuthenticationAdapter（一系列Filter），每一个AuthenticationAdapter会调用它们配置的UserDetailsService处理\n","date":"2019-02-15T00:00:00Z","permalink":"http://nixum.cc/p/springmvc/","title":"SpringMVC"},{"content":"[TOC]\nORM 普通JDBC使用sql的prepareStatement + sql + 占位符 + 参数的方式执行sql语句\nORM其实就是在先编写类与数据库表字段的映射，可以是xml配置，也可以是注解配置，之后再使用JDBC执行sql时，通过对类的反射获得其属性的值和对应的字段名，拼接sql+占位符+属性值，执行sql语句\nMyBatis #{}和${}区别\n#{}：预编译，底层是PrepareStatement ，可防止SQL注入\n${}：参数替换，不能防止SQL注入\ndao接口与mapper的映射  通过动态代理实现 实际上XML在定义Mapper的时候就相当于在编写dao类了，dao接口类相当于编写调用入口 XML中的配置信息会被存放在Configuration类中，SqlSessionFactoryBuilder会读取Configuration类中的信息创建SqlSessionFactory，之后由SqlSessionFactory创建sqlSession 在启动时先扫描并加载所有定义Mapper的XML，解析XML，根据XML中的namespace找到对应的接口，为这些接口生成对应的代理工厂MapperProxyFactory。 sqlSession.getMapper时，会通过传入的类/接口名（即被代理类）找到对应的MapperProxyFactory，生成代理类MapperProxy，并注入sqlSession，MapperProxy实现InvocationHandler接口进行代理，通过MapperMethod执行SQL，MapperMethod使用注入的sqlSession和解析XML中配置的SQL语句得到的参数，调用对应的executor执行SQL  缓存  一级缓存的作用域是同一个SqlSession，在同一个sqlSession中两次执行相同的sql语句，第一次执行完毕会将数据库中查询的数据写到缓存（内存），第二次会从缓存中获取数据将不再从数据库查询，从而提高查询效率。当一个sqlSession结束后该sqlSession中的一级缓存也就不存在了。Mybatis默认开启一级缓存。 二级缓存是mapper级别的缓存，多个SqlSession去操作同一个Mapper的sql语句，多个SqlSession去操作数据库得到数据会存在二级缓存区域，多个SqlSession可以共用二级缓存，二级缓存是跨SqlSession的。不同的sqlSession两次执行相同namespace下的sql语句且向sql中传递参数也相同即最终执行相同的sql语句，第一次执行完毕会将数据库中查询的数据写到缓存（内存），第二次会从缓存中获取数据将不再从数据库查询，从而提高查询效率。Mybatis默认没有开启二级缓存需要在setting全局参数中配置开启二级缓存  参考 MyBatis的通俗理解：SqlSession.getMapper()源码分析\n","date":"2019-02-12T00:00:00Z","permalink":"http://nixum.cc/p/mybatis/","title":"MyBatis"},{"content":"为了更好的阅读体验，可前往 个人bolg、gitbook\n目录 Java  Java SE JUC Java IO JVM  Go  Go SE GO 并发  框架  Spring SpringMVC MyBatis  数据存储  MySQL MongoDB Redis  微服务与云原生  Kubernetes 容器 etcd 与 ZooKeeper 微服务 分布式相关 RPC与异步设计 其他  消息队列  消息队列基本原理  计算机网络  网络  操作系统  操作系统  设计模式  设计模式  工具  git UML图  ","date":"2019-02-12T00:00:00Z","permalink":"http://nixum.cc/p/readme/","title":"README"}]